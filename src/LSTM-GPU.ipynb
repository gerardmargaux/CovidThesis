{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, date, timedelta\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import io\n",
    "from time import sleep\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Reshape, TimeDistributed, LSTM, Lambda, Bidirectional, RepeatVector, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ProgbarLogger, History\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.metrics import get as metric_get\n",
    "import re\n",
    "import talos\n",
    "from copy import deepcopy, copy\n",
    "import time\n",
    "import util\n",
    "import functools \n",
    "import itertools\n",
    "import networkx as nx\n",
    "from typing import List, Iterator, Tuple, Dict, Union\n",
    "import json\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "# Set up GPU:\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)\n",
    "# remove warnings from tensorflow\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.autograph.set_verbosity(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "load hospitalisations and trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics considered\n",
    "list_topics = {\n",
    "    'Fièvre': '/m/0cjf0',\n",
    "    'Mal de gorge': '/m/0b76bty',\n",
    "    #'Dyspnée': '/m/01cdt5',\n",
    "    #'Agueusie': '/m/05sfr2',\n",
    "    #'Anosmie': '/m/0m7pl',\n",
    "    #'Coronavirus': '/m/01cpyy',\n",
    "    #'Virus': '/m/0g9pc',\n",
    "    #'Température corporelle humaine': '/g/1213j0cz',\n",
    "    #'Épidémie': '/m/0hn9s',\n",
    "    'Symptôme': '/m/01b_06',\n",
    "    #'Thermomètre': '/m/07mf1',\n",
    "    #'Grippe espagnole': '/m/01c751',\n",
    "    #'Paracétamol': '/m/0lbt3',\n",
    "    #'Respiration': '/m/02gy9_',\n",
    "    #'Toux': '/m/01b_21'\n",
    "}\n",
    "\n",
    "# hospitalisations features given as input\n",
    "list_hosp_features = [\n",
    "    'NEW_HOSP',\n",
    "    'TOT_HOSP',\n",
    "    #'TOT_HOSP_log',\n",
    "    #'TOT_HOSP_pct',\n",
    "]\n",
    "\n",
    "europe = False  # if True, use european countries. Otherwise, use french regions and belgium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional information: what is the target, should some features remain unscaled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target, should be one of the hosp features\n",
    "target = 'TOT_HOSP'\n",
    "\n",
    "cumsum = False  # if True, the target will be accumulated over each day\n",
    "\n",
    "# features that should not be scaled\n",
    "unscaled = [\n",
    "    #'NEW_HOSP',\n",
    "    #'TOT_HOSP',\n",
    "    #'TOT_HOSP_log',\n",
    "    #'TOT_HOSP_pct',\n",
    "    #'Fièvre',\n",
    "    #'Mal de gorge',\n",
    "    #'Dyspnée',\n",
    "    #'Agueusie',\n",
    "    #'Anosmie',\n",
    "    #'Coronavirus',\n",
    "    #'Virus',\n",
    "    #'Température corporelle humaine',\n",
    "    #'Épidémie',\n",
    "    #'Symptôme',\n",
    "    #'Thermomètre',\n",
    "    #'Grippe espagnole',\n",
    "    #'Paracétamol',\n",
    "    #'Respiration',\n",
    "    #'Toux',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of prediction: how many days as input sould be used to predict how many days as output? Should we give a prediction on all days or only on the last?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_begin = \"2020-02-01\"\n",
    "n_forecast = 20\n",
    "n_samples = 30\n",
    "predict_one = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_metrics = [metric_get(\"MeanSquaredError\"), metric_get('MeanAbsoluteError'), \n",
    "                      metric_get('RootMeanSquaredError')]\n",
    "\n",
    "url_world = \"../data/hospi/world.csv\"\n",
    "url_pop = \"../data/population.txt\"\n",
    "url_trends = \"../data/trends/model/\"\n",
    "url_hospi_belgium = \"../data/hospi/be-covid-hospi.csv\"\n",
    "url_department_france = \"france_departements.csv\"\n",
    "url_hospi_france_new = \"../data/hospi/fr-covid-hospi.csv\"\n",
    "url_hospi_france_tot = \"../data/hospi/fr-covid-hospi-total.csv\"\n",
    "if europe:\n",
    "    population = util.get_world_population(url_pop)\n",
    "    renaming = {v: k for k, v in util.european_geocodes.items()}\n",
    "    geocodes = {k: v for k, v in util.european_geocodes.items() if population[k] > 1_000_000}\n",
    "    df_hospi = util.hospi_world(url_world, geocodes, renaming, new_hosp=True, date_begin=date_begin)\n",
    "    augment_population = {k: v/1000 for k, v in population.items()}\n",
    "else:\n",
    "    geocodes = util.french_region_and_be\n",
    "    population = pd.read_csv(url_department_france).groupby('regionTrends').agg({'population': 'sum'})\n",
    "    augment_population = {k: pop['population'] / 100_000 for k, pop in population.iterrows()}  # pop per 100.000\n",
    "    df_hospi = util.hospi_french_region_and_be(url_hospi_france_tot, url_hospi_france_new, url_hospi_belgium, \n",
    "                                           url_department_france, util.french_region_and_be, new_hosp=True, \n",
    "                                           tot_hosp=True, date_begin=date_begin)\n",
    "df_trends = util.create_df_trends(url_trends, list_topics, geocodes)  # TODO deal with augmented data\n",
    "for k in df_hospi.keys(): # Rolling average of 7 days \n",
    "    df_hospi[k] = df_hospi[k].rolling(7, center=True).mean().dropna()\n",
    "    df_trends[k] = df_trends[k].rolling(7, center=True).mean().dropna()\n",
    "merged_df = {k: pd.merge(df_hospi[k], df_trends[k], left_index=True, right_index=True).dropna() for k,v in geocodes.items()}\n",
    "    \n",
    "scaler_generator = MinMaxScaler\n",
    "dg = util.DataGenerator(merged_df, n_samples, n_forecast, target, scaler_generator=scaler_generator, scaler_type='batch',\n",
    "                       augment_merge=3, augment_adjacency=util.france_region_adjacency, augment_population=augment_population,\n",
    "                       predict_one=predict_one, cumsum=cumsum, data_columns=list_hosp_features)\n",
    "n_features = dg.n_features\n",
    "target_idx = dg.target_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not predict_one:\n",
    "    weights_loss = np.array([(1/x) for x in range(1, n_forecast+1)])\n",
    "else:\n",
    "    weights_loss = 1\n",
    "\n",
    "def custom_loss_function(y_true, y_pred):\n",
    "    y_true = y_true * weights_loss\n",
    "    y_pred = y_pred * weights_loss\n",
    "    return tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def get_encoder_decoder(batch_input_shape):\n",
    "    model = Sequential()\n",
    "    #model.add(Bidirectional(LSTM(8, return_sequences=True, stateful=False), \n",
    "    #                        input_shape=(n_samples, n_features), merge_mode=\"ave\"))\n",
    "    model.add(LSTM(16, return_sequences=True, stateful=False, batch_input_shape=batch_input_shape, recurrent_dropout=0))\n",
    "    model.add(LSTM(4, return_sequences=False, stateful=False))\n",
    "    model.add(RepeatVector(n_forecast))  # repeat\n",
    "    model.add(LSTM(4, return_sequences=True, stateful=False))  # dec\n",
    "    if not predict_one:\n",
    "        model.add(LSTM(16, return_sequences=True, stateful=False))  # dec\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.add(Reshape((n_forecast,)))\n",
    "    else:\n",
    "        model.add(LSTM(16, return_sequences=False, stateful=False))  # dec\n",
    "        model.add(Dense(1))\n",
    "        model.add(Reshape((1,)))\n",
    "    model.compile(loss=custom_loss_function, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "    \n",
    "get_encoder_decoder((1, n_samples, n_features)).output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation of encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_simple_autoencoder(batch_input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x[:,:,target_idx:target_idx+1], batch_input_shape=batch_input_shape))\n",
    "    model.add(LSTM(32, return_sequences=False, stateful=False))\n",
    "    model.add(Dense(n_forecast))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "get_simple_autoencoder((1, n_samples, n_features)).output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models to beat\n",
    "### MultiStepLastBaseline\n",
    "This model repeats the last value of hospitalisations `n_forecast` time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepLastLayer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    repeat the last hospitalisations given as input n_forecast time\n",
    "    \"\"\"\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        a = inputs[:, -1:, target_idx:target_idx+1]  # target of the last days\n",
    "        # a = tf.where(tf.not_equal(a, 0), tf.zeros_like(a), a)\n",
    "        if not predict_one:\n",
    "            return tf.tile(\n",
    "                a,\n",
    "                [1, n_forecast, 1]   # repeat target n_forecast time\n",
    "            )\n",
    "        else:\n",
    "            return tf.tile(a, [1, 1, 1])\n",
    "        \n",
    "class MultiStepLastBaseline(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    repeat the last hospitalisations given as input n_forecast time\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_input_shape=None, *args, **kwargs):\n",
    "        super(MultiStepLastBaseline, self).__init__(name='')\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((1,)), trainable=False)\n",
    "        self.multi_step = MultiStepLastLayer()\n",
    "        self.reshape = Reshape((n_forecast,))\n",
    "    \n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.multi_step(input_tensor)\n",
    "        return self.reshape(x)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return None\n",
    "    \n",
    "    def set_weights(self, *args, **kwargs):\n",
    "        return None\n",
    "\n",
    "def get_baseline(*args, **kwargs):\n",
    "    model = MultiStepLastBaseline(*args, **kwargs)\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "linear regression of the last `n_sample` days used to predict the next `n_forecast` days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionHospi(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    repeat the last hospitalisations given as input n_forecast time\n",
    "    \"\"\"\n",
    "    def predict(self, inputs, *args, **kwargs):\n",
    "        y = inputs[:, :, target_idx]  # target of the last days\n",
    "        length = len(inputs)\n",
    "        x = np.arange(n_samples).reshape(-1,1)  # dates of the target\n",
    "        if not predict_one:\n",
    "            result = np.zeros((length, n_forecast))\n",
    "            for i in range(length):\n",
    "                regr = LinearRegression().fit(x, y[i])  # linear regression of (days, target)\n",
    "                result[i] = regr.predict(np.arange(n_samples, n_samples+n_forecast).reshape(-1,1))\n",
    "        else:\n",
    "            result = np.zeros((length, 1))\n",
    "            for i in range(length):\n",
    "                regr = LinearRegression().fit(x, y[i])\n",
    "                result[i] = regre.predict([n_samples+n_forecast-1])\n",
    "        return result\n",
    "        \n",
    "def get_custom_linear_regression(*args, **kwargs):\n",
    "    model = LinearRegressionHospi()\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 fully connected layer (Dense model)\n",
    "Using only the target in the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseModel = Sequential()\n",
    "DenseModel.add(Lambda(lambda x: x[:,:,target_idx]))  # select only the target of the previous days\n",
    "if not predict_one:\n",
    "    DenseModel.add(Dense(n_forecast))   # predict the next target based on the previous ones\n",
    "else:\n",
    "    DenseModel.add(Dense(1))\n",
    "DenseModel.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "def get_dense_model(batch_input_shape, use_lambda=True):\n",
    "    model = Sequential()\n",
    "    if use_lambda:\n",
    "        model.add(Lambda(lambda x: x[:,:,target_idx], batch_input_shape=batch_input_shape))  # select only the target of the previous days\n",
    "        model.add(Dense(n_forecast))   # predict the next target based on the previous ones\n",
    "    else:\n",
    "        model.add(Dense(1, batch_input_shape=batch_input_shape))\n",
    "        model.add(Reshape((n_samples,)))\n",
    "        model.add(Dense(n_forecast))\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                          metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_vs_actual(df_y_real, df_y_predicted, prediction_dates, cur_loc, horizon=1, mode=0):\n",
    "    \"\"\"\n",
    "    plot the prediction done on a specific horizon along with the last points of data before\n",
    "    \"\"\"\n",
    "    if not predict_one:\n",
    "        if mode == 0:\n",
    "            horizon_range = range(horizon, horizon+1)\n",
    "        elif mode == 1:\n",
    "            horizon_range = range(1, horizon+1)\n",
    "    else:\n",
    "        horizon_range = [1]\n",
    "\n",
    "    df_real = df_y_real[cur_loc]\n",
    "    df_pred = df_y_predicted[cur_loc]\n",
    "    prediction_dates = prediction_dates[:, horizon]\n",
    "    \n",
    "    for horizon in horizon_range:\n",
    "        fig = plt.figure(figsize=(6,3))\n",
    "        expected = df_real[f\"{target}(t+{horizon})\"].values\n",
    "        plt.plot(prediction_dates, expected, marker=\".\", label=\"True value\")\n",
    "        prediction = df_pred[f\"{target}(t+{horizon})\"].values\n",
    "\n",
    "        if not predict_one:\n",
    "            plot_label = f\"Prediction horizon {horizon}\"\n",
    "        else:\n",
    "            plot_label = f\"Prediction horizon {n_forecast}\"\n",
    "        plt.plot(prediction_dates, prediction, marker='.', label=plot_label)\n",
    "\n",
    "        ax = fig.axes[0]\n",
    "        # set locator\n",
    "        ax.xaxis.set_major_locator(mdates.DayLocator(interval=7))\n",
    "        # set formatter\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y'))\n",
    "        # set font and rotation for date tick labels\n",
    "        plt.gcf().autofmt_xdate()\n",
    "        plt.title(f\"Plot true and predicted values for {cur_loc}\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "def compute_error(Y_expected, Y_actual) -> pd.DataFrame:\n",
    "    MAE = mean_absolute_error(Y_expected, Y_actual, multioutput=\"raw_values\")\n",
    "    MSE = mean_squared_error(Y_expected, Y_actual, multioutput=\"raw_values\")\n",
    "    values = {}\n",
    "    for t in range(n_forecast):\n",
    "        values[f'MAE(t+{t+1})'] = [MAE[t]]\n",
    "        values[f'MSE(t+{t+1})'] = [MSE[t]]\n",
    "    return pd.DataFrame(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple prediction\n",
    "use a percentage of values for training and the remaining values for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X% for training, remaining for test\n",
    "ratio_training = 0.9\n",
    "nb_datapoints = dg.batch_size\n",
    "max_train = int(ratio_training * nb_datapoints)\n",
    "train_idx = np.array(range(max_train))\n",
    "test_idx = np.array(range(max_train, nb_datapoints))\n",
    "X_train = dg.get_x(train_idx, scaled=True)\n",
    "Y_train = dg.get_y(train_idx, scaled=True)\n",
    "\n",
    "#X_test_unscaled = dg.get_x(test_idx, scaled=False)\n",
    "#Y_test_unscaled = dg.get_y(test_idx, scaled=False)\n",
    "\n",
    "X_test = dg.get_x(test_idx, scaled=True, use_previous_scaler=True, geo=dg.loc_init)\n",
    "X_test_unscaled = dg.get_x(test_idx, scaled=False, geo=dg.loc_init)\n",
    "Y_test = dg.get_y(test_idx, scaled=False, geo=dg.loc_init)\n",
    "Y_test_real = dg.remove_padded_y(Y_test, idx=test_idx, geo=dg.loc_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (61576, 30, 16)           1216      \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (61576, 8)                800       \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (61576, 20, 8)            0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (61576, 20, 8)            544       \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (61576, 20, 16)           1600      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (61576, 20, 1)            17        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (61576, 20)               0         \n",
      "=================================================================\n",
      "Total params: 4,177\n",
      "Trainable params: 4,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.0119 - mse: 0.1496 - mae: 0.3292 - root_mean_squared_error: 0.3868\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0117 - mse: 0.1438 - mae: 0.3200 - root_mean_squared_error: 0.3792\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0115 - mse: 0.1381 - mae: 0.3111 - root_mean_squared_error: 0.3716\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0113 - mse: 0.1327 - mae: 0.3023 - root_mean_squared_error: 0.3642\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0111 - mse: 0.1274 - mae: 0.2935 - root_mean_squared_error: 0.3569\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0109 - mse: 0.1221 - mae: 0.2846 - root_mean_squared_error: 0.3495\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0107 - mse: 0.1169 - mae: 0.2757 - root_mean_squared_error: 0.3420\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0105 - mse: 0.1117 - mae: 0.2666 - root_mean_squared_error: 0.3343\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0104 - mse: 0.1065 - mae: 0.2572 - root_mean_squared_error: 0.3264\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0102 - mse: 0.1012 - mae: 0.2475 - root_mean_squared_error: 0.3182\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0100 - mse: 0.0959 - mae: 0.2375 - root_mean_squared_error: 0.3097\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0098 - mse: 0.0905 - mae: 0.2271 - root_mean_squared_error: 0.3008\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0096 - mse: 0.0851 - mae: 0.2167 - root_mean_squared_error: 0.2917\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0094 - mse: 0.0797 - mae: 0.2064 - root_mean_squared_error: 0.2823\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0092 - mse: 0.0745 - mae: 0.1965 - root_mean_squared_error: 0.2730\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0090 - mse: 0.0696 - mae: 0.1877 - root_mean_squared_error: 0.2637\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0088 - mse: 0.0650 - mae: 0.1805 - root_mean_squared_error: 0.2550\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0086 - mse: 0.0612 - mae: 0.1752 - root_mean_squared_error: 0.2473\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0084 - mse: 0.0581 - mae: 0.1722 - root_mean_squared_error: 0.2410\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0082 - mse: 0.0562 - mae: 0.1721 - root_mean_squared_error: 0.2370\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0080 - mse: 0.0556 - mae: 0.1751 - root_mean_squared_error: 0.2357\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0078 - mse: 0.0566 - mae: 0.1817 - root_mean_squared_error: 0.2379\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0076 - mse: 0.0594 - mae: 0.1918 - root_mean_squared_error: 0.2438\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0075 - mse: 0.0641 - mae: 0.2046 - root_mean_squared_error: 0.2531\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0073 - mse: 0.0703 - mae: 0.2189 - root_mean_squared_error: 0.2651\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0072 - mse: 0.0776 - mae: 0.2336 - root_mean_squared_error: 0.2786\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0071 - mse: 0.0854 - mae: 0.2477 - root_mean_squared_error: 0.2922\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0070 - mse: 0.0930 - mae: 0.2605 - root_mean_squared_error: 0.3050\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0069 - mse: 0.0999 - mae: 0.2715 - root_mean_squared_error: 0.3161\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0068 - mse: 0.1056 - mae: 0.2803 - root_mean_squared_error: 0.3250\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0067 - mse: 0.1099 - mae: 0.2868 - root_mean_squared_error: 0.3315\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0067 - mse: 0.1126 - mae: 0.2911 - root_mean_squared_error: 0.3355\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0066 - mse: 0.1138 - mae: 0.2933 - root_mean_squared_error: 0.3373\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0065 - mse: 0.1136 - mae: 0.2936 - root_mean_squared_error: 0.3370\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0064 - mse: 0.1121 - mae: 0.2922 - root_mean_squared_error: 0.3349\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0063 - mse: 0.1097 - mae: 0.2894 - root_mean_squared_error: 0.3312\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0062 - mse: 0.1065 - mae: 0.2854 - root_mean_squared_error: 0.3263\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0061 - mse: 0.1027 - mae: 0.2805 - root_mean_squared_error: 0.3205\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0060 - mse: 0.0986 - mae: 0.2750 - root_mean_squared_error: 0.3140\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0060 - mse: 0.0942 - mae: 0.2689 - root_mean_squared_error: 0.3070\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0059 - mse: 0.0899 - mae: 0.2625 - root_mean_squared_error: 0.2998\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0058 - mse: 0.0856 - mae: 0.2560 - root_mean_squared_error: 0.2925\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0057 - mse: 0.0814 - mae: 0.2495 - root_mean_squared_error: 0.2853\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0056 - mse: 0.0774 - mae: 0.2430 - root_mean_squared_error: 0.2783\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0055 - mse: 0.0737 - mae: 0.2368 - root_mean_squared_error: 0.2715\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0055 - mse: 0.0703 - mae: 0.2307 - root_mean_squared_error: 0.2651\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0054 - mse: 0.0671 - mae: 0.2250 - root_mean_squared_error: 0.2591\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0053 - mse: 0.0643 - mae: 0.2196 - root_mean_squared_error: 0.2535\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0052 - mse: 0.0616 - mae: 0.2145 - root_mean_squared_error: 0.2483\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0052 - mse: 0.0592 - mae: 0.2098 - root_mean_squared_error: 0.2434\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0051 - mse: 0.0571 - mae: 0.2054 - root_mean_squared_error: 0.2389\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0050 - mse: 0.0551 - mae: 0.2013 - root_mean_squared_error: 0.2348\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0049 - mse: 0.0533 - mae: 0.1975 - root_mean_squared_error: 0.2309\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0049 - mse: 0.0517 - mae: 0.1939 - root_mean_squared_error: 0.2273\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0048 - mse: 0.0502 - mae: 0.1906 - root_mean_squared_error: 0.2240\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0047 - mse: 0.0488 - mae: 0.1875 - root_mean_squared_error: 0.2209\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0046 - mse: 0.0475 - mae: 0.1846 - root_mean_squared_error: 0.2180\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0046 - mse: 0.0463 - mae: 0.1818 - root_mean_squared_error: 0.2152\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0045 - mse: 0.0452 - mae: 0.1792 - root_mean_squared_error: 0.2126\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0044 - mse: 0.0442 - mae: 0.1766 - root_mean_squared_error: 0.2101\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0043 - mse: 0.0432 - mae: 0.1742 - root_mean_squared_error: 0.2078\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0043 - mse: 0.0422 - mae: 0.1717 - root_mean_squared_error: 0.2055\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0042 - mse: 0.0414 - mae: 0.1693 - root_mean_squared_error: 0.2034\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0041 - mse: 0.0406 - mae: 0.1670 - root_mean_squared_error: 0.2014\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0040 - mse: 0.0398 - mae: 0.1646 - root_mean_squared_error: 0.1995\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0039 - mse: 0.0391 - mae: 0.1623 - root_mean_squared_error: 0.1978\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0039 - mse: 0.0385 - mae: 0.1600 - root_mean_squared_error: 0.1962\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0038 - mse: 0.0380 - mae: 0.1577 - root_mean_squared_error: 0.1948\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0037 - mse: 0.0374 - mae: 0.1554 - root_mean_squared_error: 0.1935\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0036 - mse: 0.0370 - mae: 0.1532 - root_mean_squared_error: 0.1923\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0036 - mse: 0.0366 - mae: 0.1511 - root_mean_squared_error: 0.1912\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0035 - mse: 0.0362 - mae: 0.1491 - root_mean_squared_error: 0.1902\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0034 - mse: 0.0358 - mae: 0.1471 - root_mean_squared_error: 0.1892\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0034 - mse: 0.0355 - mae: 0.1451 - root_mean_squared_error: 0.1884\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0033 - mse: 0.0352 - mae: 0.1434 - root_mean_squared_error: 0.1877\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0032 - mse: 0.0351 - mae: 0.1420 - root_mean_squared_error: 0.1873\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0032 - mse: 0.0350 - mae: 0.1410 - root_mean_squared_error: 0.1872\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0031 - mse: 0.0351 - mae: 0.1406 - root_mean_squared_error: 0.1874\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0031 - mse: 0.0353 - mae: 0.1406 - root_mean_squared_error: 0.1880\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0030 - mse: 0.0357 - mae: 0.1411 - root_mean_squared_error: 0.1889\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0030 - mse: 0.0361 - mae: 0.1419 - root_mean_squared_error: 0.1901\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0029 - mse: 0.0366 - mae: 0.1428 - root_mean_squared_error: 0.1913\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0028 - mse: 0.0370 - mae: 0.1438 - root_mean_squared_error: 0.1924\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.0028 - mse: 0.0374 - mae: 0.1447 - root_mean_squared_error: 0.1934\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0028 - mse: 0.0377 - mae: 0.1454 - root_mean_squared_error: 0.1943\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0027 - mse: 0.0380 - mae: 0.1459 - root_mean_squared_error: 0.1949\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0027 - mse: 0.0382 - mae: 0.1463 - root_mean_squared_error: 0.1954\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0026 - mse: 0.0383 - mae: 0.1464 - root_mean_squared_error: 0.1957\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0026 - mse: 0.0384 - mae: 0.1464 - root_mean_squared_error: 0.1959\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0025 - mse: 0.0384 - mae: 0.1462 - root_mean_squared_error: 0.1961\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0025 - mse: 0.0384 - mae: 0.1458 - root_mean_squared_error: 0.1961\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0025 - mse: 0.0384 - mae: 0.1453 - root_mean_squared_error: 0.1959\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0024 - mse: 0.0383 - mae: 0.1446 - root_mean_squared_error: 0.1956\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0024 - mse: 0.0381 - mae: 0.1438 - root_mean_squared_error: 0.1951\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0023 - mse: 0.0378 - mae: 0.1428 - root_mean_squared_error: 0.1944\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0023 - mse: 0.0374 - mae: 0.1417 - root_mean_squared_error: 0.1935\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0023 - mse: 0.0371 - mae: 0.1405 - root_mean_squared_error: 0.1925\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0022 - mse: 0.0366 - mae: 0.1392 - root_mean_squared_error: 0.1914\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0022 - mse: 0.0362 - mae: 0.1380 - root_mean_squared_error: 0.1904\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0022 - mse: 0.0358 - mae: 0.1367 - root_mean_squared_error: 0.1893\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0021 - mse: 0.0355 - mae: 0.1355 - root_mean_squared_error: 0.1884\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0021 - mse: 0.0351 - mae: 0.1342 - root_mean_squared_error: 0.1874\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0021 - mse: 0.0348 - mae: 0.1330 - root_mean_squared_error: 0.1865\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0020 - mse: 0.0344 - mae: 0.1318 - root_mean_squared_error: 0.1856\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0020 - mse: 0.0341 - mae: 0.1306 - root_mean_squared_error: 0.1846\n",
      "Epoch 106/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0020 - mse: 0.0337 - mae: 0.1294 - root_mean_squared_error: 0.1836\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0019 - mse: 0.0333 - mae: 0.1281 - root_mean_squared_error: 0.1826\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0019 - mse: 0.0330 - mae: 0.1270 - root_mean_squared_error: 0.1816\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0019 - mse: 0.0326 - mae: 0.1258 - root_mean_squared_error: 0.1806\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0018 - mse: 0.0323 - mae: 0.1247 - root_mean_squared_error: 0.1798\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0018 - mse: 0.0320 - mae: 0.1237 - root_mean_squared_error: 0.1789\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0017 - mse: 0.0318 - mae: 0.1227 - root_mean_squared_error: 0.1782\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0017 - mse: 0.0315 - mae: 0.1217 - root_mean_squared_error: 0.1775\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0017 - mse: 0.0313 - mae: 0.1208 - root_mean_squared_error: 0.1768\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0016 - mse: 0.0310 - mae: 0.1200 - root_mean_squared_error: 0.1762\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0016 - mse: 0.0308 - mae: 0.1191 - root_mean_squared_error: 0.1755\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0016 - mse: 0.0306 - mae: 0.1184 - root_mean_squared_error: 0.1749\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0015 - mse: 0.0304 - mae: 0.1176 - root_mean_squared_error: 0.1744\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0015 - mse: 0.0302 - mae: 0.1169 - root_mean_squared_error: 0.1739\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0015 - mse: 0.0301 - mae: 0.1162 - root_mean_squared_error: 0.1734\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0014 - mse: 0.0299 - mae: 0.1155 - root_mean_squared_error: 0.1730\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0014 - mse: 0.0298 - mae: 0.1148 - root_mean_squared_error: 0.1726\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0013 - mse: 0.0296 - mae: 0.1142 - root_mean_squared_error: 0.1722\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0013 - mse: 0.0295 - mae: 0.1135 - root_mean_squared_error: 0.1718\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0013 - mse: 0.0294 - mae: 0.1128 - root_mean_squared_error: 0.1713\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0012 - mse: 0.0292 - mae: 0.1122 - root_mean_squared_error: 0.1709\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0012 - mse: 0.0291 - mae: 0.1115 - root_mean_squared_error: 0.1706\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0012 - mse: 0.0290 - mae: 0.1108 - root_mean_squared_error: 0.1702\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0011 - mse: 0.0289 - mae: 0.1101 - root_mean_squared_error: 0.1699\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0011 - mse: 0.0288 - mae: 0.1095 - root_mean_squared_error: 0.1696\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0010 - mse: 0.0286 - mae: 0.1089 - root_mean_squared_error: 0.1692\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0010 - mse: 0.0285 - mae: 0.1083 - root_mean_squared_error: 0.1690\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 9.7879e-04 - mse: 0.0285 - mae: 0.1077 - root_mean_squared_error: 0.1688\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 9.4769e-04 - mse: 0.0284 - mae: 0.1072 - root_mean_squared_error: 0.1686\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 9.1841e-04 - mse: 0.0284 - mae: 0.1068 - root_mean_squared_error: 0.1685\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.9117e-04 - mse: 0.0283 - mae: 0.1064 - root_mean_squared_error: 0.1683\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.6609e-04 - mse: 0.0283 - mae: 0.1061 - root_mean_squared_error: 0.1682\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.4323e-04 - mse: 0.0283 - mae: 0.1059 - root_mean_squared_error: 0.1682\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.2258e-04 - mse: 0.0283 - mae: 0.1058 - root_mean_squared_error: 0.1681\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 8.0398e-04 - mse: 0.0282 - mae: 0.1057 - root_mean_squared_error: 0.1680\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.8722e-04 - mse: 0.0282 - mae: 0.1056 - root_mean_squared_error: 0.1678\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.7195e-04 - mse: 0.0281 - mae: 0.1055 - root_mean_squared_error: 0.1676\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.5783e-04 - mse: 0.0280 - mae: 0.1054 - root_mean_squared_error: 0.1672\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.4460e-04 - mse: 0.0278 - mae: 0.1051 - root_mean_squared_error: 0.1667\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.3205e-04 - mse: 0.0276 - mae: 0.1047 - root_mean_squared_error: 0.1661\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 7.2017e-04 - mse: 0.0274 - mae: 0.1043 - root_mean_squared_error: 0.1654\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 7.0899e-04 - mse: 0.0271 - mae: 0.1038 - root_mean_squared_error: 0.1646\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.9860e-04 - mse: 0.0268 - mae: 0.1032 - root_mean_squared_error: 0.1637\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.8903e-04 - mse: 0.0265 - mae: 0.1027 - root_mean_squared_error: 0.1629\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.8026e-04 - mse: 0.0263 - mae: 0.1021 - root_mean_squared_error: 0.1621\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.7217e-04 - mse: 0.0260 - mae: 0.1017 - root_mean_squared_error: 0.1613\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.6458e-04 - mse: 0.0258 - mae: 0.1013 - root_mean_squared_error: 0.1606\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.5730e-04 - mse: 0.0256 - mae: 0.1010 - root_mean_squared_error: 0.1602\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.5016e-04 - mse: 0.0255 - mae: 0.1008 - root_mean_squared_error: 0.1598\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.4308e-04 - mse: 0.0255 - mae: 0.1007 - root_mean_squared_error: 0.1597\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.3609e-04 - mse: 0.0255 - mae: 0.1007 - root_mean_squared_error: 0.1597\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.2931e-04 - mse: 0.0256 - mae: 0.1008 - root_mean_squared_error: 0.1600\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.2291e-04 - mse: 0.0257 - mae: 0.1010 - root_mean_squared_error: 0.1603\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.1702e-04 - mse: 0.0258 - mae: 0.1011 - root_mean_squared_error: 0.1607\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 6.1166e-04 - mse: 0.0259 - mae: 0.1013 - root_mean_squared_error: 0.1611\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 6.0672e-04 - mse: 0.0260 - mae: 0.1015 - root_mean_squared_error: 0.1614\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 6.0202e-04 - mse: 0.0261 - mae: 0.1016 - root_mean_squared_error: 0.1615\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.9739e-04 - mse: 0.0261 - mae: 0.1017 - root_mean_squared_error: 0.1614\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.9275e-04 - mse: 0.0260 - mae: 0.1017 - root_mean_squared_error: 0.1612\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.8814e-04 - mse: 0.0258 - mae: 0.1017 - root_mean_squared_error: 0.1607\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.8364e-04 - mse: 0.0257 - mae: 0.1016 - root_mean_squared_error: 0.1602\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 5.7933e-04 - mse: 0.0255 - mae: 0.1015 - root_mean_squared_error: 0.1597\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.7520e-04 - mse: 0.0253 - mae: 0.1014 - root_mean_squared_error: 0.1591\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.7120e-04 - mse: 0.0252 - mae: 0.1013 - root_mean_squared_error: 0.1586\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.6724e-04 - mse: 0.0250 - mae: 0.1012 - root_mean_squared_error: 0.1583\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.6327e-04 - mse: 0.0250 - mae: 0.1010 - root_mean_squared_error: 0.1580\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.5929e-04 - mse: 0.0249 - mae: 0.1009 - root_mean_squared_error: 0.1578\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 5.5534e-04 - mse: 0.0249 - mae: 0.1008 - root_mean_squared_error: 0.1577\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 5.5147e-04 - mse: 0.0248 - mae: 0.1006 - root_mean_squared_error: 0.1576\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.4770e-04 - mse: 0.0248 - mae: 0.1005 - root_mean_squared_error: 0.1575\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.4399e-04 - mse: 0.0248 - mae: 0.1003 - root_mean_squared_error: 0.1574\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.4033e-04 - mse: 0.0247 - mae: 0.1001 - root_mean_squared_error: 0.1571\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.3674e-04 - mse: 0.0246 - mae: 0.0998 - root_mean_squared_error: 0.1568\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.3325e-04 - mse: 0.0245 - mae: 0.0996 - root_mean_squared_error: 0.1565\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.2990e-04 - mse: 0.0244 - mae: 0.0993 - root_mean_squared_error: 0.1561\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 5.2667e-04 - mse: 0.0242 - mae: 0.0991 - root_mean_squared_error: 0.1557\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.2351e-04 - mse: 0.0241 - mae: 0.0988 - root_mean_squared_error: 0.1554\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.2036e-04 - mse: 0.0241 - mae: 0.0986 - root_mean_squared_error: 0.1551\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 5.1720e-04 - mse: 0.0240 - mae: 0.0984 - root_mean_squared_error: 0.1550\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.1406e-04 - mse: 0.0240 - mae: 0.0982 - root_mean_squared_error: 0.1549\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.1096e-04 - mse: 0.0240 - mae: 0.0982 - root_mean_squared_error: 0.1548\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.0796e-04 - mse: 0.0239 - mae: 0.0977 - root_mean_squared_error: 0.1546\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.0542e-04 - mse: 0.0239 - mae: 0.0982 - root_mean_squared_error: 0.1546\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.0512e-04 - mse: 0.0237 - mae: 0.0967 - root_mean_squared_error: 0.1541\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 5.0704e-04 - mse: 0.0239 - mae: 0.0993 - root_mean_squared_error: 0.1545\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 5.0099e-04 - mse: 0.0236 - mae: 0.0962 - root_mean_squared_error: 0.1535\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.9369e-04 - mse: 0.0235 - mae: 0.0967 - root_mean_squared_error: 0.1533\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.9698e-04 - mse: 0.0236 - mae: 0.0985 - root_mean_squared_error: 0.1536\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.8978e-04 - mse: 0.0233 - mae: 0.0961 - root_mean_squared_error: 0.1527\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 4.8786e-04 - mse: 0.0233 - mae: 0.0959 - root_mean_squared_error: 0.1526\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.8696e-04 - mse: 0.0234 - mae: 0.0979 - root_mean_squared_error: 0.1530\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.8074e-04 - mse: 0.0233 - mae: 0.0967 - root_mean_squared_error: 0.1525\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.8190e-04 - mse: 0.0232 - mae: 0.0956 - root_mean_squared_error: 0.1522\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.7627e-04 - mse: 0.0232 - mae: 0.0968 - root_mean_squared_error: 0.1524\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.7553e-04 - mse: 0.0232 - mae: 0.0973 - root_mean_squared_error: 0.1525\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.7281e-04 - mse: 0.0231 - mae: 0.0956 - root_mean_squared_error: 0.1519\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.6931e-04 - mse: 0.0230 - mae: 0.0959 - root_mean_squared_error: 0.1518\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.6884e-04 - mse: 0.0231 - mae: 0.0973 - root_mean_squared_error: 0.1521\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.6432e-04 - mse: 0.0230 - mae: 0.0961 - root_mean_squared_error: 0.1515\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 4.6377e-04 - mse: 0.0229 - mae: 0.0954 - root_mean_squared_error: 0.1512\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.6049e-04 - mse: 0.0229 - mae: 0.0966 - root_mean_squared_error: 0.1515\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.5828e-04 - mse: 0.0229 - mae: 0.0965 - root_mean_squared_error: 0.1513\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.5685e-04 - mse: 0.0227 - mae: 0.0952 - root_mean_squared_error: 0.1508\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 4.5341e-04 - mse: 0.0228 - mae: 0.0958 - root_mean_squared_error: 0.1509\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.5246e-04 - mse: 0.0228 - mae: 0.0964 - root_mean_squared_error: 0.1510\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.4960e-04 - mse: 0.0226 - mae: 0.0952 - root_mean_squared_error: 0.1504\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.4745e-04 - mse: 0.0226 - mae: 0.0951 - root_mean_squared_error: 0.1503\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.4601e-04 - mse: 0.0226 - mae: 0.0960 - root_mean_squared_error: 0.1504\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.4306e-04 - mse: 0.0225 - mae: 0.0951 - root_mean_squared_error: 0.1500\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 4.4159e-04 - mse: 0.0224 - mae: 0.0946 - root_mean_squared_error: 0.1497\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.3958e-04 - mse: 0.0224 - mae: 0.0955 - root_mean_squared_error: 0.1498\n",
      "Epoch 217/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 99ms/step - loss: 4.3707e-04 - mse: 0.0223 - mae: 0.0949 - root_mean_squared_error: 0.1495\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.3567e-04 - mse: 0.0223 - mae: 0.0943 - root_mean_squared_error: 0.1492\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.3351e-04 - mse: 0.0223 - mae: 0.0950 - root_mean_squared_error: 0.1492\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 4.3130e-04 - mse: 0.0222 - mae: 0.0946 - root_mean_squared_error: 0.1490\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.2983e-04 - mse: 0.0221 - mae: 0.0940 - root_mean_squared_error: 0.1487\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.2776e-04 - mse: 0.0221 - mae: 0.0947 - root_mean_squared_error: 0.1487\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 4.2564e-04 - mse: 0.0220 - mae: 0.0943 - root_mean_squared_error: 0.1485\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.2409e-04 - mse: 0.0220 - mae: 0.0938 - root_mean_squared_error: 0.1482\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.2224e-04 - mse: 0.0220 - mae: 0.0945 - root_mean_squared_error: 0.1482\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.2015e-04 - mse: 0.0219 - mae: 0.0940 - root_mean_squared_error: 0.1479\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.1846e-04 - mse: 0.0218 - mae: 0.0937 - root_mean_squared_error: 0.1477\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.1684e-04 - mse: 0.0218 - mae: 0.0943 - root_mean_squared_error: 0.1477\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.1491e-04 - mse: 0.0217 - mae: 0.0936 - root_mean_squared_error: 0.1474\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 4.1302e-04 - mse: 0.0217 - mae: 0.0937 - root_mean_squared_error: 0.1472\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.1140e-04 - mse: 0.0216 - mae: 0.0939 - root_mean_squared_error: 0.1471\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.0978e-04 - mse: 0.0215 - mae: 0.0932 - root_mean_squared_error: 0.1467\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 4.0796e-04 - mse: 0.0215 - mae: 0.0937 - root_mean_squared_error: 0.1467\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 4.0613e-04 - mse: 0.0214 - mae: 0.0933 - root_mean_squared_error: 0.1464\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.0445e-04 - mse: 0.0214 - mae: 0.0931 - root_mean_squared_error: 0.1462\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.0287e-04 - mse: 0.0214 - mae: 0.0934 - root_mean_squared_error: 0.1461\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 4.0126e-04 - mse: 0.0212 - mae: 0.0927 - root_mean_squared_error: 0.1458\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.9957e-04 - mse: 0.0212 - mae: 0.0932 - root_mean_squared_error: 0.1458\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.9784e-04 - mse: 0.0211 - mae: 0.0926 - root_mean_squared_error: 0.1454\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.9615e-04 - mse: 0.0211 - mae: 0.0928 - root_mean_squared_error: 0.1453\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.9451e-04 - mse: 0.0210 - mae: 0.0926 - root_mean_squared_error: 0.1450\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.9293e-04 - mse: 0.0210 - mae: 0.0923 - root_mean_squared_error: 0.1448\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.9139e-04 - mse: 0.0209 - mae: 0.0926 - root_mean_squared_error: 0.1447\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.8989e-04 - mse: 0.0208 - mae: 0.0920 - root_mean_squared_error: 0.1443\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.8845e-04 - mse: 0.0208 - mae: 0.0926 - root_mean_squared_error: 0.1443\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.8711e-04 - mse: 0.0207 - mae: 0.0916 - root_mean_squared_error: 0.1438\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.8598e-04 - mse: 0.0207 - mae: 0.0927 - root_mean_squared_error: 0.1440\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.8525e-04 - mse: 0.0205 - mae: 0.0910 - root_mean_squared_error: 0.1433\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.8498e-04 - mse: 0.0207 - mae: 0.0932 - root_mean_squared_error: 0.1439\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.8504e-04 - mse: 0.0204 - mae: 0.0904 - root_mean_squared_error: 0.1428\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.8371e-04 - mse: 0.0206 - mae: 0.0934 - root_mean_squared_error: 0.1436\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.8032e-04 - mse: 0.0203 - mae: 0.0904 - root_mean_squared_error: 0.1424\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.7648e-04 - mse: 0.0203 - mae: 0.0918 - root_mean_squared_error: 0.1426\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.7519e-04 - mse: 0.0203 - mae: 0.0918 - root_mean_squared_error: 0.1424\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.7561e-04 - mse: 0.0201 - mae: 0.0902 - root_mean_squared_error: 0.1418\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.7459e-04 - mse: 0.0203 - mae: 0.0924 - root_mean_squared_error: 0.1424\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.7163e-04 - mse: 0.0200 - mae: 0.0903 - root_mean_squared_error: 0.1414\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.6926e-04 - mse: 0.0200 - mae: 0.0908 - root_mean_squared_error: 0.1414\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.6884e-04 - mse: 0.0200 - mae: 0.0915 - root_mean_squared_error: 0.1415\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.6842e-04 - mse: 0.0198 - mae: 0.0897 - root_mean_squared_error: 0.1408\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.6631e-04 - mse: 0.0199 - mae: 0.0914 - root_mean_squared_error: 0.1411\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.6399e-04 - mse: 0.0198 - mae: 0.0903 - root_mean_squared_error: 0.1406\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.6300e-04 - mse: 0.0197 - mae: 0.0899 - root_mean_squared_error: 0.1403\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.6248e-04 - mse: 0.0198 - mae: 0.0911 - root_mean_squared_error: 0.1406\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.6103e-04 - mse: 0.0196 - mae: 0.0895 - root_mean_squared_error: 0.1399\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.5896e-04 - mse: 0.0196 - mae: 0.0904 - root_mean_squared_error: 0.1400\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 3.5759e-04 - mse: 0.0195 - mae: 0.0902 - root_mean_squared_error: 0.1398\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.5689e-04 - mse: 0.0194 - mae: 0.0893 - root_mean_squared_error: 0.1393\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.5581e-04 - mse: 0.0195 - mae: 0.0905 - root_mean_squared_error: 0.1395\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 3.5413e-04 - mse: 0.0193 - mae: 0.0893 - root_mean_squared_error: 0.1390\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.5254e-04 - mse: 0.0193 - mae: 0.0896 - root_mean_squared_error: 0.1389\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 3.5151e-04 - mse: 0.0193 - mae: 0.0898 - root_mean_squared_error: 0.1388\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.5064e-04 - mse: 0.0191 - mae: 0.0889 - root_mean_squared_error: 0.1384\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.4940e-04 - mse: 0.0192 - mae: 0.0899 - root_mean_squared_error: 0.1385\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.4788e-04 - mse: 0.0191 - mae: 0.0889 - root_mean_squared_error: 0.1381\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.4651e-04 - mse: 0.0190 - mae: 0.0891 - root_mean_squared_error: 0.1380\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.4546e-04 - mse: 0.0190 - mae: 0.0893 - root_mean_squared_error: 0.1379\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.4450e-04 - mse: 0.0189 - mae: 0.0885 - root_mean_squared_error: 0.1375\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.4336e-04 - mse: 0.0189 - mae: 0.0893 - root_mean_squared_error: 0.1376\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.4204e-04 - mse: 0.0188 - mae: 0.0884 - root_mean_squared_error: 0.1371\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.4071e-04 - mse: 0.0188 - mae: 0.0888 - root_mean_squared_error: 0.1371\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.3954e-04 - mse: 0.0187 - mae: 0.0886 - root_mean_squared_error: 0.1369\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.3850e-04 - mse: 0.0187 - mae: 0.0882 - root_mean_squared_error: 0.1366\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.3749e-04 - mse: 0.0187 - mae: 0.0888 - root_mean_squared_error: 0.1366\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.3642e-04 - mse: 0.0186 - mae: 0.0879 - root_mean_squared_error: 0.1362\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.3525e-04 - mse: 0.0186 - mae: 0.0886 - root_mean_squared_error: 0.1363\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.3404e-04 - mse: 0.0185 - mae: 0.0878 - root_mean_squared_error: 0.1359\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.3285e-04 - mse: 0.0185 - mae: 0.0882 - root_mean_squared_error: 0.1358\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.3171e-04 - mse: 0.0184 - mae: 0.0879 - root_mean_squared_error: 0.1356\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.3063e-04 - mse: 0.0183 - mae: 0.0877 - root_mean_squared_error: 0.1354\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.2959e-04 - mse: 0.0183 - mae: 0.0879 - root_mean_squared_error: 0.1353\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.2857e-04 - mse: 0.0182 - mae: 0.0873 - root_mean_squared_error: 0.1350\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.2757e-04 - mse: 0.0182 - mae: 0.0879 - root_mean_squared_error: 0.1350\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 3.2660e-04 - mse: 0.0181 - mae: 0.0870 - root_mean_squared_error: 0.1347\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.2565e-04 - mse: 0.0182 - mae: 0.0878 - root_mean_squared_error: 0.1348\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 3.2476e-04 - mse: 0.0180 - mae: 0.0867 - root_mean_squared_error: 0.1343\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.2396e-04 - mse: 0.0181 - mae: 0.0879 - root_mean_squared_error: 0.1345\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.2331e-04 - mse: 0.0179 - mae: 0.0863 - root_mean_squared_error: 0.1339\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.2279e-04 - mse: 0.0180 - mae: 0.0881 - root_mean_squared_error: 0.1343\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 3.2246e-04 - mse: 0.0178 - mae: 0.0858 - root_mean_squared_error: 0.1336\n",
      "computed in 0:00:34.140153 s\n"
     ]
    }
   ],
   "source": [
    "batch_input = len(X_train)\n",
    "encoder_decoder_train = get_encoder_decoder(batch_input_shape=(batch_input, n_samples, n_features))\n",
    "encoder_decoder_train.summary()\n",
    "start = time.time()\n",
    "encoder_decoder_train.fit(X_train, Y_train, epochs=300, verbose=1, batch_size=batch_input)\n",
    "end = time.time() - start\n",
    "print(f\"computed in {str(str(timedelta(seconds=end)))} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE(t+1)</th>\n",
       "      <th>MSE(t+1)</th>\n",
       "      <th>MAE(t+2)</th>\n",
       "      <th>MSE(t+2)</th>\n",
       "      <th>MAE(t+3)</th>\n",
       "      <th>MSE(t+3)</th>\n",
       "      <th>MAE(t+4)</th>\n",
       "      <th>MSE(t+4)</th>\n",
       "      <th>MAE(t+5)</th>\n",
       "      <th>MSE(t+5)</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE(t+16)</th>\n",
       "      <th>MSE(t+16)</th>\n",
       "      <th>MAE(t+17)</th>\n",
       "      <th>MSE(t+17)</th>\n",
       "      <th>MAE(t+18)</th>\n",
       "      <th>MSE(t+18)</th>\n",
       "      <th>MAE(t+19)</th>\n",
       "      <th>MSE(t+19)</th>\n",
       "      <th>MAE(t+20)</th>\n",
       "      <th>MSE(t+20)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.216298</td>\n",
       "      <td>46.30147</td>\n",
       "      <td>4.923568</td>\n",
       "      <td>56.079559</td>\n",
       "      <td>5.755735</td>\n",
       "      <td>80.966191</td>\n",
       "      <td>6.583991</td>\n",
       "      <td>111.297434</td>\n",
       "      <td>7.133885</td>\n",
       "      <td>130.539596</td>\n",
       "      <td>...</td>\n",
       "      <td>8.250472</td>\n",
       "      <td>185.043326</td>\n",
       "      <td>8.40026</td>\n",
       "      <td>195.326674</td>\n",
       "      <td>8.621356</td>\n",
       "      <td>205.724282</td>\n",
       "      <td>8.825004</td>\n",
       "      <td>217.266567</td>\n",
       "      <td>9.080662</td>\n",
       "      <td>234.622359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MAE(t+1)  MSE(t+1)  MAE(t+2)   MSE(t+2)  MAE(t+3)   MSE(t+3)  MAE(t+4)  \\\n",
       "0  4.216298  46.30147  4.923568  56.079559  5.755735  80.966191  6.583991   \n",
       "\n",
       "     MSE(t+4)  MAE(t+5)    MSE(t+5)  ...  MAE(t+16)   MSE(t+16)  MAE(t+17)  \\\n",
       "0  111.297434  7.133885  130.539596  ...   8.250472  185.043326    8.40026   \n",
       "\n",
       "    MSE(t+17)  MAE(t+18)   MSE(t+18)  MAE(t+19)   MSE(t+19)  MAE(t+20)  \\\n",
       "0  195.326674   8.621356  205.724282   8.825004  217.266567   9.080662   \n",
       "\n",
       "    MSE(t+20)  \n",
       "0  234.622359  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input = len(X_test)\n",
    "encoder_decoder_prediction = get_encoder_decoder(batch_input_shape=(batch_input, n_samples, n_features) )\n",
    "encoder_decoder_prediction.set_weights(encoder_decoder_train.get_weights())\n",
    "\n",
    "Y_pred = np.squeeze(encoder_decoder_prediction.predict(X_test, batch_size=batch_input))\n",
    "Y_pred_unscaled = dg.inverse_transform_y(Y_pred, idx=test_idx, geo=dg.loc_init)\n",
    "Y_pred_real = dg.remove_padded_y(Y_pred_unscaled, idx=test_idx, geo=dg.loc_init)\n",
    "\n",
    "compute_error(Y_test_real, Y_pred_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (61576, 32)               4480      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (61576, 20)               660       \n",
      "=================================================================\n",
      "Total params: 5,140\n",
      "Trainable params: 5,140\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1643 - mse: 0.1643 - mae: 0.3445 - root_mean_squared_error: 0.4053\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.1591 - mse: 0.1591 - mae: 0.3375 - root_mean_squared_error: 0.3989\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1544 - mse: 0.1544 - mae: 0.3309 - root_mean_squared_error: 0.3930\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1501 - mse: 0.1501 - mae: 0.3247 - root_mean_squared_error: 0.3874\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1460 - mse: 0.1460 - mae: 0.3188 - root_mean_squared_error: 0.3821\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1422 - mse: 0.1422 - mae: 0.3132 - root_mean_squared_error: 0.3771\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1387 - mse: 0.1387 - mae: 0.3078 - root_mean_squared_error: 0.3724\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1353 - mse: 0.1353 - mae: 0.3027 - root_mean_squared_error: 0.3678\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1321 - mse: 0.1321 - mae: 0.2978 - root_mean_squared_error: 0.3635\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1290 - mse: 0.1290 - mae: 0.2930 - root_mean_squared_error: 0.3592\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1261 - mse: 0.1261 - mae: 0.2884 - root_mean_squared_error: 0.3551\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1232 - mse: 0.1232 - mae: 0.2838 - root_mean_squared_error: 0.3509\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1203 - mse: 0.1203 - mae: 0.2793 - root_mean_squared_error: 0.3469\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1175 - mse: 0.1175 - mae: 0.2748 - root_mean_squared_error: 0.3428\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1147 - mse: 0.1147 - mae: 0.2703 - root_mean_squared_error: 0.3387\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1119 - mse: 0.1119 - mae: 0.2657 - root_mean_squared_error: 0.3346\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1092 - mse: 0.1092 - mae: 0.2611 - root_mean_squared_error: 0.3304\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1063 - mse: 0.1063 - mae: 0.2565 - root_mean_squared_error: 0.3261\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1035 - mse: 0.1035 - mae: 0.2517 - root_mean_squared_error: 0.3217\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1006 - mse: 0.1006 - mae: 0.2469 - root_mean_squared_error: 0.3172\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0977 - mse: 0.0977 - mae: 0.2419 - root_mean_squared_error: 0.3125\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0947 - mse: 0.0947 - mae: 0.2368 - root_mean_squared_error: 0.3078\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0917 - mse: 0.0917 - mae: 0.2316 - root_mean_squared_error: 0.3028\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0886 - mse: 0.0886 - mae: 0.2263 - root_mean_squared_error: 0.2977\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0855 - mse: 0.0855 - mae: 0.2208 - root_mean_squared_error: 0.2924\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0824 - mse: 0.0824 - mae: 0.2153 - root_mean_squared_error: 0.2870\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0792 - mse: 0.0792 - mae: 0.2098 - root_mean_squared_error: 0.2815\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0761 - mse: 0.0761 - mae: 0.2043 - root_mean_squared_error: 0.2759\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0730 - mse: 0.0730 - mae: 0.1988 - root_mean_squared_error: 0.2702\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0700 - mse: 0.0700 - mae: 0.1935 - root_mean_squared_error: 0.2645\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0671 - mse: 0.0671 - mae: 0.1884 - root_mean_squared_error: 0.2590\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0642 - mse: 0.0642 - mae: 0.1836 - root_mean_squared_error: 0.2535\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0616 - mse: 0.0616 - mae: 0.1792 - root_mean_squared_error: 0.2481\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0590 - mse: 0.0590 - mae: 0.1753 - root_mean_squared_error: 0.2429\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0565 - mse: 0.0565 - mae: 0.1716 - root_mean_squared_error: 0.2377\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0541 - mse: 0.0541 - mae: 0.1681 - root_mean_squared_error: 0.2325\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0516 - mse: 0.0516 - mae: 0.1645 - root_mean_squared_error: 0.2273\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0492 - mse: 0.0492 - mae: 0.1605 - root_mean_squared_error: 0.2219\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0469 - mse: 0.0469 - mae: 0.1561 - root_mean_squared_error: 0.2165\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0446 - mse: 0.0446 - mae: 0.1515 - root_mean_squared_error: 0.2113\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0426 - mse: 0.0426 - mae: 0.1468 - root_mean_squared_error: 0.2063\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0407 - mse: 0.0407 - mae: 0.1424 - root_mean_squared_error: 0.2018\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0391 - mse: 0.0391 - mae: 0.1385 - root_mean_squared_error: 0.1977\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0377 - mse: 0.0377 - mae: 0.1352 - root_mean_squared_error: 0.1942\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0365 - mse: 0.0365 - mae: 0.1325 - root_mean_squared_error: 0.1912\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0355 - mse: 0.0355 - mae: 0.1303 - root_mean_squared_error: 0.1884\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0345 - mse: 0.0345 - mae: 0.1287 - root_mean_squared_error: 0.1858\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0337 - mse: 0.0337 - mae: 0.1277 - root_mean_squared_error: 0.1834\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0329 - mse: 0.0329 - mae: 0.1271 - root_mean_squared_error: 0.1813\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0322 - mse: 0.0322 - mae: 0.1270 - root_mean_squared_error: 0.1793\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0316 - mse: 0.0316 - mae: 0.1272 - root_mean_squared_error: 0.1777\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0311 - mse: 0.0311 - mae: 0.1273 - root_mean_squared_error: 0.1763\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0306 - mse: 0.0306 - mae: 0.1272 - root_mean_squared_error: 0.1750\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0302 - mse: 0.0302 - mae: 0.1266 - root_mean_squared_error: 0.1738\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0298 - mse: 0.0298 - mae: 0.1256 - root_mean_squared_error: 0.1725\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0294 - mse: 0.0294 - mae: 0.1244 - root_mean_squared_error: 0.1714\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0291 - mse: 0.0291 - mae: 0.1230 - root_mean_squared_error: 0.1705\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0288 - mse: 0.0288 - mae: 0.1219 - root_mean_squared_error: 0.1696\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0285 - mse: 0.0285 - mae: 0.1210 - root_mean_squared_error: 0.1689\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0283 - mse: 0.0283 - mae: 0.1205 - root_mean_squared_error: 0.1682\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0280 - mse: 0.0280 - mae: 0.1203 - root_mean_squared_error: 0.1675\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0278 - mse: 0.0278 - mae: 0.1204 - root_mean_squared_error: 0.1667\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0276 - mse: 0.0276 - mae: 0.1206 - root_mean_squared_error: 0.1661\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0274 - mse: 0.0274 - mae: 0.1209 - root_mean_squared_error: 0.1655\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0272 - mse: 0.0272 - mae: 0.1209 - root_mean_squared_error: 0.1649\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0270 - mse: 0.0270 - mae: 0.1207 - root_mean_squared_error: 0.1643\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0268 - mse: 0.0268 - mae: 0.1201 - root_mean_squared_error: 0.1637\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0266 - mse: 0.0266 - mae: 0.1192 - root_mean_squared_error: 0.1631\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0264 - mse: 0.0264 - mae: 0.1183 - root_mean_squared_error: 0.1626\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0263 - mse: 0.0263 - mae: 0.1175 - root_mean_squared_error: 0.1620\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0261 - mse: 0.0261 - mae: 0.1170 - root_mean_squared_error: 0.1615\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0259 - mse: 0.0259 - mae: 0.1167 - root_mean_squared_error: 0.1610\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0257 - mse: 0.0257 - mae: 0.1167 - root_mean_squared_error: 0.1605\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0256 - mse: 0.0256 - mae: 0.1169 - root_mean_squared_error: 0.1600\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0254 - mse: 0.0254 - mae: 0.1171 - root_mean_squared_error: 0.1595\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0253 - mse: 0.0253 - mae: 0.1172 - root_mean_squared_error: 0.1591\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0252 - mse: 0.0252 - mae: 0.1170 - root_mean_squared_error: 0.1586\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0250 - mse: 0.0250 - mae: 0.1165 - root_mean_squared_error: 0.1582\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0249 - mse: 0.0249 - mae: 0.1158 - root_mean_squared_error: 0.1577\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0247 - mse: 0.0247 - mae: 0.1152 - root_mean_squared_error: 0.1573\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0246 - mse: 0.0246 - mae: 0.1148 - root_mean_squared_error: 0.1569\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0245 - mse: 0.0245 - mae: 0.1145 - root_mean_squared_error: 0.1565\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0243 - mse: 0.0243 - mae: 0.1145 - root_mean_squared_error: 0.1560\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0242 - mse: 0.0242 - mae: 0.1146 - root_mean_squared_error: 0.1556\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0241 - mse: 0.0241 - mae: 0.1146 - root_mean_squared_error: 0.1552\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0240 - mse: 0.0240 - mae: 0.1144 - root_mean_squared_error: 0.1548\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0239 - mse: 0.0239 - mae: 0.1140 - root_mean_squared_error: 0.1545\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0237 - mse: 0.0237 - mae: 0.1133 - root_mean_squared_error: 0.1541\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0236 - mse: 0.0236 - mae: 0.1127 - root_mean_squared_error: 0.1537\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0235 - mse: 0.0235 - mae: 0.1122 - root_mean_squared_error: 0.1534\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0234 - mse: 0.0234 - mae: 0.1120 - root_mean_squared_error: 0.1530\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0233 - mse: 0.0233 - mae: 0.1120 - root_mean_squared_error: 0.1527\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0232 - mse: 0.0232 - mae: 0.1120 - root_mean_squared_error: 0.1523\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1118 - root_mean_squared_error: 0.1520\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0230 - mse: 0.0230 - mae: 0.1114 - root_mean_squared_error: 0.1517\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0229 - mse: 0.0229 - mae: 0.1109 - root_mean_squared_error: 0.1513\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.1105 - root_mean_squared_error: 0.1510\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.1102 - root_mean_squared_error: 0.1507\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0226 - mse: 0.0226 - mae: 0.1102 - root_mean_squared_error: 0.1504\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0225 - mse: 0.0225 - mae: 0.1101 - root_mean_squared_error: 0.1501\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0224 - mse: 0.0224 - mae: 0.1100 - root_mean_squared_error: 0.1498\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0223 - mse: 0.0223 - mae: 0.1096 - root_mean_squared_error: 0.1495\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0223 - mse: 0.0223 - mae: 0.1091 - root_mean_squared_error: 0.1492\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0222 - mse: 0.0222 - mae: 0.1088 - root_mean_squared_error: 0.1489\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0221 - mse: 0.0221 - mae: 0.1087 - root_mean_squared_error: 0.1486\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0220 - mse: 0.0220 - mae: 0.1087 - root_mean_squared_error: 0.1483\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0219 - mse: 0.0219 - mae: 0.1086 - root_mean_squared_error: 0.1480\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0218 - mse: 0.0218 - mae: 0.1083 - root_mean_squared_error: 0.1478\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0217 - mse: 0.0217 - mae: 0.1079 - root_mean_squared_error: 0.1475\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0217 - mse: 0.0217 - mae: 0.1076 - root_mean_squared_error: 0.1472\n",
      "Epoch 111/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0216 - mse: 0.0216 - mae: 0.1074 - root_mean_squared_error: 0.1469\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0215 - mse: 0.0215 - mae: 0.1073 - root_mean_squared_error: 0.1467\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0214 - mse: 0.0214 - mae: 0.1071 - root_mean_squared_error: 0.1464\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0214 - mse: 0.0214 - mae: 0.1068 - root_mean_squared_error: 0.1461\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0213 - mse: 0.0213 - mae: 0.1065 - root_mean_squared_error: 0.1459\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.1062 - root_mean_squared_error: 0.1456\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0211 - mse: 0.0211 - mae: 0.1060 - root_mean_squared_error: 0.1454\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0211 - mse: 0.0211 - mae: 0.1059 - root_mean_squared_error: 0.1451\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0210 - mse: 0.0210 - mae: 0.1057 - root_mean_squared_error: 0.1448\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0209 - mse: 0.0209 - mae: 0.1054 - root_mean_squared_error: 0.1446\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0208 - mse: 0.0208 - mae: 0.1051 - root_mean_squared_error: 0.1443\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0208 - mse: 0.0208 - mae: 0.1050 - root_mean_squared_error: 0.1441\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0207 - mse: 0.0207 - mae: 0.1048 - root_mean_squared_error: 0.1438\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0206 - mse: 0.0206 - mae: 0.1047 - root_mean_squared_error: 0.1436\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1044 - root_mean_squared_error: 0.1433\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1041 - root_mean_squared_error: 0.1431\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1040 - root_mean_squared_error: 0.1428\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1038 - root_mean_squared_error: 0.1426\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1037 - root_mean_squared_error: 0.1423\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1034 - root_mean_squared_error: 0.1421\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1032 - root_mean_squared_error: 0.1418\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1030 - root_mean_squared_error: 0.1416\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1029 - root_mean_squared_error: 0.1413\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1027 - root_mean_squared_error: 0.1411\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1024 - root_mean_squared_error: 0.1408\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1022 - root_mean_squared_error: 0.1405\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.1020 - root_mean_squared_error: 0.1403\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.1018 - root_mean_squared_error: 0.1400\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.1016 - root_mean_squared_error: 0.1398\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.1014 - root_mean_squared_error: 0.1395\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0194 - mse: 0.0194 - mae: 0.1012 - root_mean_squared_error: 0.1393\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0193 - mse: 0.0193 - mae: 0.1010 - root_mean_squared_error: 0.1390\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.1008 - root_mean_squared_error: 0.1387\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.1006 - root_mean_squared_error: 0.1385\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.1004 - root_mean_squared_error: 0.1382\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.1002 - root_mean_squared_error: 0.1379\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.1000 - root_mean_squared_error: 0.1377\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0998 - root_mean_squared_error: 0.1374\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0996 - root_mean_squared_error: 0.1371\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0994 - root_mean_squared_error: 0.1369\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0992 - root_mean_squared_error: 0.1366\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0990 - root_mean_squared_error: 0.1363\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0185 - mse: 0.0185 - mae: 0.0987 - root_mean_squared_error: 0.1361\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0184 - mse: 0.0184 - mae: 0.0986 - root_mean_squared_error: 0.1358\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0184 - mse: 0.0184 - mae: 0.0984 - root_mean_squared_error: 0.1355\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0183 - mse: 0.0183 - mae: 0.0981 - root_mean_squared_error: 0.1352\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0182 - mse: 0.0182 - mae: 0.0979 - root_mean_squared_error: 0.1350\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0181 - mse: 0.0181 - mae: 0.0977 - root_mean_squared_error: 0.1347\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0181 - mse: 0.0181 - mae: 0.0975 - root_mean_squared_error: 0.1344\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0180 - mse: 0.0180 - mae: 0.0973 - root_mean_squared_error: 0.1342\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0179 - mse: 0.0179 - mae: 0.0971 - root_mean_squared_error: 0.1339\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0179 - mse: 0.0179 - mae: 0.0969 - root_mean_squared_error: 0.1336\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0178 - mse: 0.0178 - mae: 0.0966 - root_mean_squared_error: 0.1334\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0177 - mse: 0.0177 - mae: 0.0964 - root_mean_squared_error: 0.1331\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0176 - mse: 0.0176 - mae: 0.0962 - root_mean_squared_error: 0.1328\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0176 - mse: 0.0176 - mae: 0.0960 - root_mean_squared_error: 0.1326\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0175 - mse: 0.0175 - mae: 0.0958 - root_mean_squared_error: 0.1323\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0174 - mse: 0.0174 - mae: 0.0956 - root_mean_squared_error: 0.1321\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0174 - mse: 0.0174 - mae: 0.0953 - root_mean_squared_error: 0.1318\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0173 - mse: 0.0173 - mae: 0.0951 - root_mean_squared_error: 0.1316\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0172 - mse: 0.0172 - mae: 0.0949 - root_mean_squared_error: 0.1313\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0172 - mse: 0.0172 - mae: 0.0947 - root_mean_squared_error: 0.1311\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0171 - mse: 0.0171 - mae: 0.0945 - root_mean_squared_error: 0.1308\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0171 - mse: 0.0171 - mae: 0.0943 - root_mean_squared_error: 0.1306\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0170 - mse: 0.0170 - mae: 0.0941 - root_mean_squared_error: 0.1304\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0169 - mse: 0.0169 - mae: 0.0939 - root_mean_squared_error: 0.1301\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0169 - mse: 0.0169 - mae: 0.0936 - root_mean_squared_error: 0.1299\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0168 - mse: 0.0168 - mae: 0.0935 - root_mean_squared_error: 0.1297\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0168 - mse: 0.0168 - mae: 0.0932 - root_mean_squared_error: 0.1295\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0167 - mse: 0.0167 - mae: 0.0933 - root_mean_squared_error: 0.1293\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0167 - mse: 0.0167 - mae: 0.0923 - root_mean_squared_error: 0.1291\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0167 - mse: 0.0167 - mae: 0.0941 - root_mean_squared_error: 0.1291\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0167 - mse: 0.0167 - mae: 0.0905 - root_mean_squared_error: 0.1293\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0166 - mse: 0.0166 - mae: 0.0938 - root_mean_squared_error: 0.1287\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0165 - mse: 0.0165 - mae: 0.0929 - root_mean_squared_error: 0.1284\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0165 - mse: 0.0165 - mae: 0.0903 - root_mean_squared_error: 0.1286\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0164 - mse: 0.0164 - mae: 0.0925 - root_mean_squared_error: 0.1281\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0164 - mse: 0.0164 - mae: 0.0929 - root_mean_squared_error: 0.1280\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0164 - mse: 0.0164 - mae: 0.0902 - root_mean_squared_error: 0.1280\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0163 - mse: 0.0163 - mae: 0.0914 - root_mean_squared_error: 0.1275\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0163 - mse: 0.0163 - mae: 0.0928 - root_mean_squared_error: 0.1277\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0162 - mse: 0.0162 - mae: 0.0901 - root_mean_squared_error: 0.1274\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0162 - mse: 0.0162 - mae: 0.0905 - root_mean_squared_error: 0.1271\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0162 - mse: 0.0162 - mae: 0.0924 - root_mean_squared_error: 0.1272\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0161 - mse: 0.0161 - mae: 0.0902 - root_mean_squared_error: 0.1269\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0161 - mse: 0.0161 - mae: 0.0899 - root_mean_squared_error: 0.1268\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0161 - mse: 0.0161 - mae: 0.0918 - root_mean_squared_error: 0.1268\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0160 - mse: 0.0160 - mae: 0.0903 - root_mean_squared_error: 0.1265\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0160 - mse: 0.0160 - mae: 0.0896 - root_mean_squared_error: 0.1264\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0160 - mse: 0.0160 - mae: 0.0912 - root_mean_squared_error: 0.1263\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0159 - mse: 0.0159 - mae: 0.0903 - root_mean_squared_error: 0.1261\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0159 - mse: 0.0159 - mae: 0.0893 - root_mean_squared_error: 0.1261\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0159 - mse: 0.0159 - mae: 0.0906 - root_mean_squared_error: 0.1259\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0158 - mse: 0.0158 - mae: 0.0901 - root_mean_squared_error: 0.1258\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0158 - mse: 0.0158 - mae: 0.0891 - root_mean_squared_error: 0.1257\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0158 - mse: 0.0158 - mae: 0.0901 - root_mean_squared_error: 0.1255\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0157 - mse: 0.0157 - mae: 0.0899 - root_mean_squared_error: 0.1254\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0157 - mse: 0.0157 - mae: 0.0889 - root_mean_squared_error: 0.1254\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0157 - mse: 0.0157 - mae: 0.0898 - root_mean_squared_error: 0.1252\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0157 - mse: 0.0157 - mae: 0.0896 - root_mean_squared_error: 0.1251\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0156 - mse: 0.0156 - mae: 0.0888 - root_mean_squared_error: 0.1250\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0156 - mse: 0.0156 - mae: 0.0895 - root_mean_squared_error: 0.1249\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0156 - mse: 0.0156 - mae: 0.0892 - root_mean_squared_error: 0.1248\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0156 - mse: 0.0156 - mae: 0.0886 - root_mean_squared_error: 0.1247\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0155 - mse: 0.0155 - mae: 0.0893 - root_mean_squared_error: 0.1246\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0155 - mse: 0.0155 - mae: 0.0888 - root_mean_squared_error: 0.1245\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0155 - mse: 0.0155 - mae: 0.0885 - root_mean_squared_error: 0.1244\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0155 - mse: 0.0155 - mae: 0.0891 - root_mean_squared_error: 0.1243\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0154 - mse: 0.0154 - mae: 0.0885 - root_mean_squared_error: 0.1242\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0154 - mse: 0.0154 - mae: 0.0884 - root_mean_squared_error: 0.1241\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0154 - mse: 0.0154 - mae: 0.0888 - root_mean_squared_error: 0.1240\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0154 - mse: 0.0154 - mae: 0.0881 - root_mean_squared_error: 0.1239\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0153 - mse: 0.0153 - mae: 0.0884 - root_mean_squared_error: 0.1238\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0153 - mse: 0.0153 - mae: 0.0884 - root_mean_squared_error: 0.1237\n",
      "Epoch 225/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0153 - mse: 0.0153 - mae: 0.0879 - root_mean_squared_error: 0.1237\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0153 - mse: 0.0153 - mae: 0.0884 - root_mean_squared_error: 0.1236\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0152 - mse: 0.0152 - mae: 0.0879 - root_mean_squared_error: 0.1235\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0152 - mse: 0.0152 - mae: 0.0879 - root_mean_squared_error: 0.1234\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0152 - mse: 0.0152 - mae: 0.0881 - root_mean_squared_error: 0.1233\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0152 - mse: 0.0152 - mae: 0.0876 - root_mean_squared_error: 0.1232\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0152 - mse: 0.0152 - mae: 0.0880 - root_mean_squared_error: 0.1231\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0151 - mse: 0.0151 - mae: 0.0876 - root_mean_squared_error: 0.1230\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0151 - mse: 0.0151 - mae: 0.0876 - root_mean_squared_error: 0.1229\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0151 - mse: 0.0151 - mae: 0.0877 - root_mean_squared_error: 0.1228\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0151 - mse: 0.0151 - mae: 0.0873 - root_mean_squared_error: 0.1228\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0151 - mse: 0.0151 - mae: 0.0877 - root_mean_squared_error: 0.1227\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0150 - mse: 0.0150 - mae: 0.0873 - root_mean_squared_error: 0.1226\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0150 - mse: 0.0150 - mae: 0.0874 - root_mean_squared_error: 0.1225\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0150 - mse: 0.0150 - mae: 0.0872 - root_mean_squared_error: 0.1224\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0150 - mse: 0.0150 - mae: 0.0872 - root_mean_squared_error: 0.1223\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0149 - mse: 0.0149 - mae: 0.0872 - root_mean_squared_error: 0.1223\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0149 - mse: 0.0149 - mae: 0.0870 - root_mean_squared_error: 0.1222\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0149 - mse: 0.0149 - mae: 0.0872 - root_mean_squared_error: 0.1221\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0149 - mse: 0.0149 - mae: 0.0868 - root_mean_squared_error: 0.1220\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0149 - mse: 0.0149 - mae: 0.0872 - root_mean_squared_error: 0.1219\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0149 - mse: 0.0149 - mae: 0.0865 - root_mean_squared_error: 0.1219\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0148 - mse: 0.0148 - mae: 0.0873 - root_mean_squared_error: 0.1218\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0148 - mse: 0.0148 - mae: 0.0862 - root_mean_squared_error: 0.1218\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0148 - mse: 0.0148 - mae: 0.0877 - root_mean_squared_error: 0.1218\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0149 - mse: 0.0149 - mae: 0.0856 - root_mean_squared_error: 0.1219\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0149 - mse: 0.0149 - mae: 0.0883 - root_mean_squared_error: 0.1220\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0149 - mse: 0.0149 - mae: 0.0852 - root_mean_squared_error: 0.1220\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0148 - mse: 0.0148 - mae: 0.0876 - root_mean_squared_error: 0.1215\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0147 - mse: 0.0147 - mae: 0.0863 - root_mean_squared_error: 0.1212\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0147 - mse: 0.0147 - mae: 0.0858 - root_mean_squared_error: 0.1212\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0147 - mse: 0.0147 - mae: 0.0876 - root_mean_squared_error: 0.1214\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0147 - mse: 0.0147 - mae: 0.0854 - root_mean_squared_error: 0.1212\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0146 - mse: 0.0146 - mae: 0.0866 - root_mean_squared_error: 0.1209\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0146 - mse: 0.0146 - mae: 0.0867 - root_mean_squared_error: 0.1209\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0146 - mse: 0.0146 - mae: 0.0853 - root_mean_squared_error: 0.1210\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0146 - mse: 0.0146 - mae: 0.0869 - root_mean_squared_error: 0.1208\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0145 - mse: 0.0145 - mae: 0.0858 - root_mean_squared_error: 0.1206\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0145 - mse: 0.0145 - mae: 0.0855 - root_mean_squared_error: 0.1206\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0145 - mse: 0.0145 - mae: 0.0867 - root_mean_squared_error: 0.1206\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0145 - mse: 0.0145 - mae: 0.0853 - root_mean_squared_error: 0.1205\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0145 - mse: 0.0145 - mae: 0.0859 - root_mean_squared_error: 0.1203\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0145 - mse: 0.0145 - mae: 0.0862 - root_mean_squared_error: 0.1203\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0145 - mse: 0.0145 - mae: 0.0851 - root_mean_squared_error: 0.1203\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0144 - mse: 0.0144 - mae: 0.0862 - root_mean_squared_error: 0.1201\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0144 - mse: 0.0144 - mae: 0.0855 - root_mean_squared_error: 0.1200\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0144 - mse: 0.0144 - mae: 0.0853 - root_mean_squared_error: 0.1199\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0144 - mse: 0.0144 - mae: 0.0860 - root_mean_squared_error: 0.1199\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0144 - mse: 0.0144 - mae: 0.0850 - root_mean_squared_error: 0.1198\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0143 - mse: 0.0143 - mae: 0.0856 - root_mean_squared_error: 0.1197\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0143 - mse: 0.0143 - mae: 0.0854 - root_mean_squared_error: 0.1196\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0143 - mse: 0.0143 - mae: 0.0850 - root_mean_squared_error: 0.1196\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0143 - mse: 0.0143 - mae: 0.0857 - root_mean_squared_error: 0.1195\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0143 - mse: 0.0143 - mae: 0.0848 - root_mean_squared_error: 0.1194\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0142 - mse: 0.0142 - mae: 0.0854 - root_mean_squared_error: 0.1193\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0142 - mse: 0.0142 - mae: 0.0850 - root_mean_squared_error: 0.1192\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0142 - mse: 0.0142 - mae: 0.0848 - root_mean_squared_error: 0.1192\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0142 - mse: 0.0142 - mae: 0.0853 - root_mean_squared_error: 0.1191\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0142 - mse: 0.0142 - mae: 0.0845 - root_mean_squared_error: 0.1190\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0142 - mse: 0.0142 - mae: 0.0853 - root_mean_squared_error: 0.1190\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0141 - mse: 0.0141 - mae: 0.0844 - root_mean_squared_error: 0.1189\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0141 - mse: 0.0141 - mae: 0.0851 - root_mean_squared_error: 0.1188\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0141 - mse: 0.0141 - mae: 0.0844 - root_mean_squared_error: 0.1187\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0141 - mse: 0.0141 - mae: 0.0848 - root_mean_squared_error: 0.1186\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0140 - mse: 0.0140 - mae: 0.0844 - root_mean_squared_error: 0.1185\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0140 - mse: 0.0140 - mae: 0.0847 - root_mean_squared_error: 0.1184\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0140 - mse: 0.0140 - mae: 0.0842 - root_mean_squared_error: 0.1184\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0140 - mse: 0.0140 - mae: 0.0846 - root_mean_squared_error: 0.1183\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0140 - mse: 0.0140 - mae: 0.0840 - root_mean_squared_error: 0.1182\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0140 - mse: 0.0140 - mae: 0.0847 - root_mean_squared_error: 0.1182\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0140 - mse: 0.0140 - mae: 0.0835 - root_mean_squared_error: 0.1183\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0140 - mse: 0.0140 - mae: 0.0856 - root_mean_squared_error: 0.1185\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0142 - mse: 0.0142 - mae: 0.0827 - root_mean_squared_error: 0.1194\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0143 - mse: 0.0143 - mae: 0.0871 - root_mean_squared_error: 0.1197\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0143 - mse: 0.0143 - mae: 0.0825 - root_mean_squared_error: 0.1197\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0139 - mse: 0.0139 - mae: 0.0843 - root_mean_squared_error: 0.1177\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0141 - mse: 0.0141 - mae: 0.0859 - root_mean_squared_error: 0.1186\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0143 - mse: 0.0143 - mae: 0.0824 - root_mean_squared_error: 0.1195\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0138 - mse: 0.0138 - mae: 0.0838 - root_mean_squared_error: 0.1175\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0142 - mse: 0.0142 - mae: 0.0867 - root_mean_squared_error: 0.1192\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0141 - mse: 0.0141 - mae: 0.0823 - root_mean_squared_error: 0.1188\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0139 - mse: 0.0139 - mae: 0.0827 - root_mean_squared_error: 0.1178\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0142 - mse: 0.0142 - mae: 0.0867 - root_mean_squared_error: 0.1191\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0137 - mse: 0.0137 - mae: 0.0831 - root_mean_squared_error: 0.1172\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0140 - mse: 0.0140 - mae: 0.0822 - root_mean_squared_error: 0.1183\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0137 - mse: 0.0137 - mae: 0.0839 - root_mean_squared_error: 0.1171\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0139 - mse: 0.0139 - mae: 0.0853 - root_mean_squared_error: 0.1179\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0137 - mse: 0.0137 - mae: 0.0825 - root_mean_squared_error: 0.1172\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0138 - mse: 0.0138 - mae: 0.0822 - root_mean_squared_error: 0.1175\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0137 - mse: 0.0137 - mae: 0.0841 - root_mean_squared_error: 0.1171\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0137 - mse: 0.0137 - mae: 0.0843 - root_mean_squared_error: 0.1171\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0137 - mse: 0.0137 - mae: 0.0823 - root_mean_squared_error: 0.1170\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0137 - mse: 0.0137 - mae: 0.0822 - root_mean_squared_error: 0.1169\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0136 - mse: 0.0136 - mae: 0.0838 - root_mean_squared_error: 0.1168\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0136 - mse: 0.0136 - mae: 0.0837 - root_mean_squared_error: 0.1167\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0136 - mse: 0.0136 - mae: 0.0822 - root_mean_squared_error: 0.1166\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0136 - mse: 0.0136 - mae: 0.0822 - root_mean_squared_error: 0.1165\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0136 - mse: 0.0136 - mae: 0.0835 - root_mean_squared_error: 0.1165\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0135 - mse: 0.0135 - mae: 0.0832 - root_mean_squared_error: 0.1163\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0135 - mse: 0.0135 - mae: 0.0820 - root_mean_squared_error: 0.1163\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0135 - mse: 0.0135 - mae: 0.0821 - root_mean_squared_error: 0.1161\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0135 - mse: 0.0135 - mae: 0.0832 - root_mean_squared_error: 0.1162\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0134 - mse: 0.0134 - mae: 0.0827 - root_mean_squared_error: 0.1160\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0135 - mse: 0.0135 - mae: 0.0818 - root_mean_squared_error: 0.1160\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0134 - mse: 0.0134 - mae: 0.0821 - root_mean_squared_error: 0.1158\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0134 - mse: 0.0134 - mae: 0.0829 - root_mean_squared_error: 0.1159\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0134 - mse: 0.0134 - mae: 0.0823 - root_mean_squared_error: 0.1157\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0134 - mse: 0.0134 - mae: 0.0817 - root_mean_squared_error: 0.1157\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0133 - mse: 0.0133 - mae: 0.0820 - root_mean_squared_error: 0.1155\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0134 - mse: 0.0134 - mae: 0.0825 - root_mean_squared_error: 0.1156\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0133 - mse: 0.0133 - mae: 0.0819 - root_mean_squared_error: 0.1154\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0133 - mse: 0.0133 - mae: 0.0815 - root_mean_squared_error: 0.1154\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0133 - mse: 0.0133 - mae: 0.0820 - root_mean_squared_error: 0.1153\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0133 - mse: 0.0133 - mae: 0.0822 - root_mean_squared_error: 0.1153\n",
      "Epoch 339/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0133 - mse: 0.0133 - mae: 0.0815 - root_mean_squared_error: 0.1152\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0132 - mse: 0.0132 - mae: 0.0814 - root_mean_squared_error: 0.1151\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0132 - mse: 0.0132 - mae: 0.0819 - root_mean_squared_error: 0.1150\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0132 - mse: 0.0132 - mae: 0.0817 - root_mean_squared_error: 0.1149\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0132 - mse: 0.0132 - mae: 0.0812 - root_mean_squared_error: 0.1149\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0132 - mse: 0.0132 - mae: 0.0814 - root_mean_squared_error: 0.1148\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0132 - mse: 0.0132 - mae: 0.0817 - root_mean_squared_error: 0.1148\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0132 - mse: 0.0132 - mae: 0.0812 - root_mean_squared_error: 0.1147\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0131 - mse: 0.0131 - mae: 0.0811 - root_mean_squared_error: 0.1146\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0131 - mse: 0.0131 - mae: 0.0814 - root_mean_squared_error: 0.1146\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0131 - mse: 0.0131 - mae: 0.0812 - root_mean_squared_error: 0.1145\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0131 - mse: 0.0131 - mae: 0.0809 - root_mean_squared_error: 0.1144\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0131 - mse: 0.0131 - mae: 0.0810 - root_mean_squared_error: 0.1144\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0131 - mse: 0.0131 - mae: 0.0811 - root_mean_squared_error: 0.1143\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0130 - mse: 0.0130 - mae: 0.0808 - root_mean_squared_error: 0.1142\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0130 - mse: 0.0130 - mae: 0.0807 - root_mean_squared_error: 0.1142\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0130 - mse: 0.0130 - mae: 0.0809 - root_mean_squared_error: 0.1141\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0130 - mse: 0.0130 - mae: 0.0807 - root_mean_squared_error: 0.1140\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0130 - mse: 0.0130 - mae: 0.0805 - root_mean_squared_error: 0.1140\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0130 - mse: 0.0130 - mae: 0.0807 - root_mean_squared_error: 0.1139\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0130 - mse: 0.0130 - mae: 0.0806 - root_mean_squared_error: 0.1138\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0129 - mse: 0.0129 - mae: 0.0804 - root_mean_squared_error: 0.1138\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0129 - mse: 0.0129 - mae: 0.0805 - root_mean_squared_error: 0.1137\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0129 - mse: 0.0129 - mae: 0.0805 - root_mean_squared_error: 0.1137\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0129 - mse: 0.0129 - mae: 0.0802 - root_mean_squared_error: 0.1136\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0129 - mse: 0.0129 - mae: 0.0803 - root_mean_squared_error: 0.1135\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0129 - mse: 0.0129 - mae: 0.0803 - root_mean_squared_error: 0.1135\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0129 - mse: 0.0129 - mae: 0.0801 - root_mean_squared_error: 0.1134\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0128 - mse: 0.0128 - mae: 0.0801 - root_mean_squared_error: 0.1133\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0128 - mse: 0.0128 - mae: 0.0801 - root_mean_squared_error: 0.1133\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0128 - mse: 0.0128 - mae: 0.0799 - root_mean_squared_error: 0.1132\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0128 - mse: 0.0128 - mae: 0.0799 - root_mean_squared_error: 0.1132\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0128 - mse: 0.0128 - mae: 0.0800 - root_mean_squared_error: 0.1131\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0128 - mse: 0.0128 - mae: 0.0798 - root_mean_squared_error: 0.1130\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0128 - mse: 0.0128 - mae: 0.0797 - root_mean_squared_error: 0.1130\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0127 - mse: 0.0127 - mae: 0.0798 - root_mean_squared_error: 0.1129\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0127 - mse: 0.0127 - mae: 0.0796 - root_mean_squared_error: 0.1128\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0127 - mse: 0.0127 - mae: 0.0796 - root_mean_squared_error: 0.1128\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0127 - mse: 0.0127 - mae: 0.0796 - root_mean_squared_error: 0.1127\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0127 - mse: 0.0127 - mae: 0.0795 - root_mean_squared_error: 0.1127\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0127 - mse: 0.0127 - mae: 0.0795 - root_mean_squared_error: 0.1126\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0127 - mse: 0.0127 - mae: 0.0794 - root_mean_squared_error: 0.1125\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0793 - root_mean_squared_error: 0.1125\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0793 - root_mean_squared_error: 0.1124\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0793 - root_mean_squared_error: 0.1123\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0791 - root_mean_squared_error: 0.1123\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0792 - root_mean_squared_error: 0.1122\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0791 - root_mean_squared_error: 0.1122\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0790 - root_mean_squared_error: 0.1121\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0790 - root_mean_squared_error: 0.1120\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0125 - mse: 0.0125 - mae: 0.0789 - root_mean_squared_error: 0.1120\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0125 - mse: 0.0125 - mae: 0.0789 - root_mean_squared_error: 0.1119\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0125 - mse: 0.0125 - mae: 0.0789 - root_mean_squared_error: 0.1118\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0125 - mse: 0.0125 - mae: 0.0788 - root_mean_squared_error: 0.1118\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0125 - mse: 0.0125 - mae: 0.0788 - root_mean_squared_error: 0.1117\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0125 - mse: 0.0125 - mae: 0.0787 - root_mean_squared_error: 0.1117\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0125 - mse: 0.0125 - mae: 0.0786 - root_mean_squared_error: 0.1116\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0124 - mse: 0.0124 - mae: 0.0786 - root_mean_squared_error: 0.1115\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0124 - mse: 0.0124 - mae: 0.0785 - root_mean_squared_error: 0.1115\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0124 - mse: 0.0124 - mae: 0.0785 - root_mean_squared_error: 0.1114\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0124 - mse: 0.0124 - mae: 0.0784 - root_mean_squared_error: 0.1113\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0124 - mse: 0.0124 - mae: 0.0784 - root_mean_squared_error: 0.1113\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0124 - mse: 0.0124 - mae: 0.0784 - root_mean_squared_error: 0.1112\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0124 - mse: 0.0124 - mae: 0.0783 - root_mean_squared_error: 0.1111\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0123 - mse: 0.0123 - mae: 0.0783 - root_mean_squared_error: 0.1111\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0123 - mse: 0.0123 - mae: 0.0782 - root_mean_squared_error: 0.1110\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0123 - mse: 0.0123 - mae: 0.0782 - root_mean_squared_error: 0.1110\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0123 - mse: 0.0123 - mae: 0.0781 - root_mean_squared_error: 0.1109\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0123 - mse: 0.0123 - mae: 0.0781 - root_mean_squared_error: 0.1108\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0123 - mse: 0.0123 - mae: 0.0780 - root_mean_squared_error: 0.1108\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0123 - mse: 0.0123 - mae: 0.0780 - root_mean_squared_error: 0.1107\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0122 - mse: 0.0122 - mae: 0.0779 - root_mean_squared_error: 0.1106\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0122 - mse: 0.0122 - mae: 0.0779 - root_mean_squared_error: 0.1106\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0122 - mse: 0.0122 - mae: 0.0779 - root_mean_squared_error: 0.1105\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0122 - mse: 0.0122 - mae: 0.0778 - root_mean_squared_error: 0.1104\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0122 - mse: 0.0122 - mae: 0.0778 - root_mean_squared_error: 0.1104\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0122 - mse: 0.0122 - mae: 0.0777 - root_mean_squared_error: 0.1103\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0122 - mse: 0.0122 - mae: 0.0777 - root_mean_squared_error: 0.1102\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0776 - root_mean_squared_error: 0.1102\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0776 - root_mean_squared_error: 0.1101\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0776 - root_mean_squared_error: 0.1101\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0775 - root_mean_squared_error: 0.1100\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0775 - root_mean_squared_error: 0.1099\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0774 - root_mean_squared_error: 0.1099\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0774 - root_mean_squared_error: 0.1098\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0120 - mse: 0.0120 - mae: 0.0774 - root_mean_squared_error: 0.1097\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0120 - mse: 0.0120 - mae: 0.0772 - root_mean_squared_error: 0.1096\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0120 - mse: 0.0120 - mae: 0.0774 - root_mean_squared_error: 0.1096\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0120 - mse: 0.0120 - mae: 0.0769 - root_mean_squared_error: 0.1096\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0781 - root_mean_squared_error: 0.1098\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0764 - root_mean_squared_error: 0.1122\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0142 - mse: 0.0142 - mae: 0.0859 - root_mean_squared_error: 0.1193\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0178 - mse: 0.0178 - mae: 0.0846 - root_mean_squared_error: 0.1335\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0154 - mse: 0.0154 - mae: 0.0807 - root_mean_squared_error: 0.1240\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0135 - mse: 0.0135 - mae: 0.0835 - root_mean_squared_error: 0.1164\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0151 - mse: 0.0151 - mae: 0.0887 - root_mean_squared_error: 0.1230\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0127 - mse: 0.0127 - mae: 0.0781 - root_mean_squared_error: 0.1128\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0147 - mse: 0.0147 - mae: 0.0798 - root_mean_squared_error: 0.1212\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0142 - mse: 0.0142 - mae: 0.0791 - root_mean_squared_error: 0.1190\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0788 - root_mean_squared_error: 0.1123\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0133 - mse: 0.0133 - mae: 0.0836 - root_mean_squared_error: 0.1152\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0136 - mse: 0.0136 - mae: 0.0844 - root_mean_squared_error: 0.1168\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0797 - root_mean_squared_error: 0.1120\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0778 - root_mean_squared_error: 0.1124\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0131 - mse: 0.0131 - mae: 0.0778 - root_mean_squared_error: 0.1144\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0128 - mse: 0.0128 - mae: 0.0773 - root_mean_squared_error: 0.1133\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0123 - mse: 0.0123 - mae: 0.0779 - root_mean_squared_error: 0.1111\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0125 - mse: 0.0125 - mae: 0.0808 - root_mean_squared_error: 0.1117\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0128 - mse: 0.0128 - mae: 0.0827 - root_mean_squared_error: 0.1131\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0125 - mse: 0.0125 - mae: 0.0806 - root_mean_squared_error: 0.1116\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0122 - mse: 0.0122 - mae: 0.0781 - root_mean_squared_error: 0.1105\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0124 - mse: 0.0124 - mae: 0.0774 - root_mean_squared_error: 0.1115\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0126 - mse: 0.0126 - mae: 0.0772 - root_mean_squared_error: 0.1120\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0123 - mse: 0.0123 - mae: 0.0768 - root_mean_squared_error: 0.1110\n",
      "Epoch 453/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0773 - root_mean_squared_error: 0.1102\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0123 - mse: 0.0123 - mae: 0.0790 - root_mean_squared_error: 0.1107\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0124 - mse: 0.0124 - mae: 0.0799 - root_mean_squared_error: 0.1112\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0122 - mse: 0.0122 - mae: 0.0787 - root_mean_squared_error: 0.1104\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0772 - root_mean_squared_error: 0.1098\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0767 - root_mean_squared_error: 0.1102\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0122 - mse: 0.0122 - mae: 0.0766 - root_mean_squared_error: 0.1104\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0767 - root_mean_squared_error: 0.1100\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0120 - mse: 0.0120 - mae: 0.0771 - root_mean_squared_error: 0.1095\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0120 - mse: 0.0120 - mae: 0.0779 - root_mean_squared_error: 0.1097\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0121 - mse: 0.0121 - mae: 0.0784 - root_mean_squared_error: 0.1099\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0120 - mse: 0.0120 - mae: 0.0778 - root_mean_squared_error: 0.1095\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0119 - mse: 0.0119 - mae: 0.0770 - root_mean_squared_error: 0.1093\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0120 - mse: 0.0120 - mae: 0.0766 - root_mean_squared_error: 0.1094\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0120 - mse: 0.0120 - mae: 0.0765 - root_mean_squared_error: 0.1094\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0119 - mse: 0.0119 - mae: 0.0767 - root_mean_squared_error: 0.1092\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0119 - mse: 0.0119 - mae: 0.0772 - root_mean_squared_error: 0.1090\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0119 - mse: 0.0119 - mae: 0.0776 - root_mean_squared_error: 0.1091\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0119 - mse: 0.0119 - mae: 0.0775 - root_mean_squared_error: 0.1091\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0119 - mse: 0.0119 - mae: 0.0770 - root_mean_squared_error: 0.1089\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0118 - mse: 0.0118 - mae: 0.0764 - root_mean_squared_error: 0.1088\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0119 - mse: 0.0119 - mae: 0.0762 - root_mean_squared_error: 0.1089\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0118 - mse: 0.0118 - mae: 0.0762 - root_mean_squared_error: 0.1088\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0118 - mse: 0.0118 - mae: 0.0765 - root_mean_squared_error: 0.1086\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0118 - mse: 0.0118 - mae: 0.0769 - root_mean_squared_error: 0.1086\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0118 - mse: 0.0118 - mae: 0.0770 - root_mean_squared_error: 0.1086\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0118 - mse: 0.0118 - mae: 0.0767 - root_mean_squared_error: 0.1085\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0118 - mse: 0.0118 - mae: 0.0762 - root_mean_squared_error: 0.1084\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0118 - mse: 0.0118 - mae: 0.0760 - root_mean_squared_error: 0.1084\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0117 - mse: 0.0117 - mae: 0.0760 - root_mean_squared_error: 0.1083\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0117 - mse: 0.0117 - mae: 0.0762 - root_mean_squared_error: 0.1082\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0117 - mse: 0.0117 - mae: 0.0766 - root_mean_squared_error: 0.1082\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0117 - mse: 0.0117 - mae: 0.0766 - root_mean_squared_error: 0.1082\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0117 - mse: 0.0117 - mae: 0.0764 - root_mean_squared_error: 0.1081\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0117 - mse: 0.0117 - mae: 0.0760 - root_mean_squared_error: 0.1080\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0117 - mse: 0.0117 - mae: 0.0758 - root_mean_squared_error: 0.1080\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0117 - mse: 0.0117 - mae: 0.0759 - root_mean_squared_error: 0.1079\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0116 - mse: 0.0116 - mae: 0.0760 - root_mean_squared_error: 0.1079\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0116 - mse: 0.0116 - mae: 0.0762 - root_mean_squared_error: 0.1078\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0116 - mse: 0.0116 - mae: 0.0762 - root_mean_squared_error: 0.1078\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0116 - mse: 0.0116 - mae: 0.0760 - root_mean_squared_error: 0.1077\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0116 - mse: 0.0116 - mae: 0.0758 - root_mean_squared_error: 0.1077\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0116 - mse: 0.0116 - mae: 0.0757 - root_mean_squared_error: 0.1076\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0116 - mse: 0.0116 - mae: 0.0758 - root_mean_squared_error: 0.1075\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0116 - mse: 0.0116 - mae: 0.0759 - root_mean_squared_error: 0.1075\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0115 - mse: 0.0115 - mae: 0.0759 - root_mean_squared_error: 0.1074\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0115 - mse: 0.0115 - mae: 0.0758 - root_mean_squared_error: 0.1074\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0115 - mse: 0.0115 - mae: 0.0756 - root_mean_squared_error: 0.1073\n",
      "computed in 0:00:37.206063 s\n"
     ]
    }
   ],
   "source": [
    "batch_input = len(X_train)\n",
    "simple_autoencoder_train = get_simple_autoencoder(batch_input_shape=(batch_input, n_samples, n_features))\n",
    "simple_autoencoder_train.summary()\n",
    "start = time.time()\n",
    "simple_autoencoder_train.fit(X_train, Y_train, epochs=500, verbose=1, batch_size=batch_input)\n",
    "end = time.time() - start\n",
    "print(f\"computed in {str(str(timedelta(seconds=end)))} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE(t+1)</th>\n",
       "      <th>MSE(t+1)</th>\n",
       "      <th>MAE(t+2)</th>\n",
       "      <th>MSE(t+2)</th>\n",
       "      <th>MAE(t+3)</th>\n",
       "      <th>MSE(t+3)</th>\n",
       "      <th>MAE(t+4)</th>\n",
       "      <th>MSE(t+4)</th>\n",
       "      <th>MAE(t+5)</th>\n",
       "      <th>MSE(t+5)</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE(t+16)</th>\n",
       "      <th>MSE(t+16)</th>\n",
       "      <th>MAE(t+17)</th>\n",
       "      <th>MSE(t+17)</th>\n",
       "      <th>MAE(t+18)</th>\n",
       "      <th>MSE(t+18)</th>\n",
       "      <th>MAE(t+19)</th>\n",
       "      <th>MSE(t+19)</th>\n",
       "      <th>MAE(t+20)</th>\n",
       "      <th>MSE(t+20)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.859568</td>\n",
       "      <td>58.411693</td>\n",
       "      <td>6.075467</td>\n",
       "      <td>98.424237</td>\n",
       "      <td>6.427463</td>\n",
       "      <td>116.993777</td>\n",
       "      <td>7.380563</td>\n",
       "      <td>149.153733</td>\n",
       "      <td>6.929693</td>\n",
       "      <td>116.823133</td>\n",
       "      <td>...</td>\n",
       "      <td>10.041596</td>\n",
       "      <td>295.167095</td>\n",
       "      <td>9.719341</td>\n",
       "      <td>260.226641</td>\n",
       "      <td>9.399061</td>\n",
       "      <td>237.026055</td>\n",
       "      <td>10.680075</td>\n",
       "      <td>333.488576</td>\n",
       "      <td>10.974743</td>\n",
       "      <td>373.631021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MAE(t+1)   MSE(t+1)  MAE(t+2)   MSE(t+2)  MAE(t+3)    MSE(t+3)  MAE(t+4)  \\\n",
       "0  4.859568  58.411693  6.075467  98.424237  6.427463  116.993777  7.380563   \n",
       "\n",
       "     MSE(t+4)  MAE(t+5)    MSE(t+5)  ...  MAE(t+16)   MSE(t+16)  MAE(t+17)  \\\n",
       "0  149.153733  6.929693  116.823133  ...  10.041596  295.167095   9.719341   \n",
       "\n",
       "    MSE(t+17)  MAE(t+18)   MSE(t+18)  MAE(t+19)   MSE(t+19)  MAE(t+20)  \\\n",
       "0  260.226641   9.399061  237.026055  10.680075  333.488576  10.974743   \n",
       "\n",
       "    MSE(t+20)  \n",
       "0  373.631021  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input = len(X_test)\n",
    "simple_autoencoder_pred = get_simple_autoencoder(batch_input_shape=(batch_input, n_samples, n_features) )\n",
    "simple_autoencoder_pred.set_weights(simple_autoencoder_train.get_weights())\n",
    "\n",
    "Y_pred = simple_autoencoder_pred.predict(X_test, batch_size=batch_input)\n",
    "Y_pred_unscaled = dg.inverse_transform_y(Y_pred, idx=test_idx, geo=dg.loc_init)\n",
    "Y_pred_real = dg.remove_padded_y(Y_pred_unscaled, idx=test_idx, geo=dg.loc_init)\n",
    "\n",
    "compute_error(Y_test_real, Y_pred_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE(t+1)</th>\n",
       "      <th>MSE(t+1)</th>\n",
       "      <th>MAE(t+2)</th>\n",
       "      <th>MSE(t+2)</th>\n",
       "      <th>MAE(t+3)</th>\n",
       "      <th>MSE(t+3)</th>\n",
       "      <th>MAE(t+4)</th>\n",
       "      <th>MSE(t+4)</th>\n",
       "      <th>MAE(t+5)</th>\n",
       "      <th>MSE(t+5)</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE(t+16)</th>\n",
       "      <th>MSE(t+16)</th>\n",
       "      <th>MAE(t+17)</th>\n",
       "      <th>MSE(t+17)</th>\n",
       "      <th>MAE(t+18)</th>\n",
       "      <th>MSE(t+18)</th>\n",
       "      <th>MAE(t+19)</th>\n",
       "      <th>MSE(t+19)</th>\n",
       "      <th>MAE(t+20)</th>\n",
       "      <th>MSE(t+20)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.389393</td>\n",
       "      <td>12.022092</td>\n",
       "      <td>3.648033</td>\n",
       "      <td>28.055514</td>\n",
       "      <td>4.727504</td>\n",
       "      <td>49.239597</td>\n",
       "      <td>5.623507</td>\n",
       "      <td>70.83885</td>\n",
       "      <td>6.427616</td>\n",
       "      <td>96.242919</td>\n",
       "      <td>...</td>\n",
       "      <td>9.762542</td>\n",
       "      <td>253.885128</td>\n",
       "      <td>9.96974</td>\n",
       "      <td>267.768117</td>\n",
       "      <td>10.202899</td>\n",
       "      <td>282.481652</td>\n",
       "      <td>10.45039</td>\n",
       "      <td>297.655497</td>\n",
       "      <td>10.727664</td>\n",
       "      <td>316.01504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MAE(t+1)   MSE(t+1)  MAE(t+2)   MSE(t+2)  MAE(t+3)   MSE(t+3)  MAE(t+4)  \\\n",
       "0  2.389393  12.022092  3.648033  28.055514  4.727504  49.239597  5.623507   \n",
       "\n",
       "   MSE(t+4)  MAE(t+5)   MSE(t+5)  ...  MAE(t+16)   MSE(t+16)  MAE(t+17)  \\\n",
       "0  70.83885  6.427616  96.242919  ...   9.762542  253.885128    9.96974   \n",
       "\n",
       "    MSE(t+17)  MAE(t+18)   MSE(t+18)  MAE(t+19)   MSE(t+19)  MAE(t+20)  \\\n",
       "0  267.768117  10.202899  282.481652   10.45039  297.655497  10.727664   \n",
       "\n",
       "   MSE(t+20)  \n",
       "0  316.01504  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model= get_baseline()\n",
    "Y_pred_unscaled = np.squeeze(baseline_model.predict(X_test_unscaled))\n",
    "Y_pred_real = dg.remove_padded_y(Y_pred_unscaled, idx=test_idx, geo=dg.loc_init)\n",
    "compute_error(Y_pred_real, Y_test_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loc = 'BE'\n",
    "df_predicted = dg.remove_padded_y(Y_pred_unscaled, idx=test_idx, return_type='dict_df', geo=dg.loc_init)\n",
    "df_actual = dg.remove_padded_y(Y_test, idx=test_idx, return_type='dict_df', geo=dg.loc_init)\n",
    "plot_prediction_vs_actual(df_actual, df_predicted, dg.get_y_dates(test_idx), 'FR-I', horizon=1)\n",
    "\n",
    "# def plot_prediction_vs_actual(df_y_real, df_y_predicted, prediction_dates, cur_loc, horizon=1, mode=0):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61576, 20)\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (61576, 30)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (61576, 20)               620       \n",
      "=================================================================\n",
      "Total params: 620\n",
      "Trainable params: 620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/400\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.3921 - mse: 0.3921 - mae: 0.5040 - root_mean_squared_error: 0.6262\n",
      "Epoch 2/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3566 - mse: 0.3566 - mae: 0.4770 - root_mean_squared_error: 0.5972\n",
      "Epoch 3/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3333 - mse: 0.3333 - mae: 0.4587 - root_mean_squared_error: 0.5773\n",
      "Epoch 4/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3151 - mse: 0.3151 - mae: 0.4440 - root_mean_squared_error: 0.5613\n",
      "Epoch 5/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2998 - mse: 0.2998 - mae: 0.4315 - root_mean_squared_error: 0.5476\n",
      "Epoch 6/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2866 - mse: 0.2866 - mae: 0.4204 - root_mean_squared_error: 0.5353\n",
      "Epoch 7/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2747 - mse: 0.2747 - mae: 0.4103 - root_mean_squared_error: 0.5241\n",
      "Epoch 8/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2640 - mse: 0.2640 - mae: 0.4011 - root_mean_squared_error: 0.5138\n",
      "Epoch 9/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2541 - mse: 0.2541 - mae: 0.3925 - root_mean_squared_error: 0.5041\n",
      "Epoch 10/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2450 - mse: 0.2450 - mae: 0.3844 - root_mean_squared_error: 0.4950\n",
      "Epoch 11/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2365 - mse: 0.2365 - mae: 0.3768 - root_mean_squared_error: 0.4863\n",
      "Epoch 12/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2285 - mse: 0.2285 - mae: 0.3695 - root_mean_squared_error: 0.4780\n",
      "Epoch 13/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2210 - mse: 0.2210 - mae: 0.3626 - root_mean_squared_error: 0.4701\n",
      "Epoch 14/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2138 - mse: 0.2138 - mae: 0.3559 - root_mean_squared_error: 0.4624\n",
      "Epoch 15/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2071 - mse: 0.2071 - mae: 0.3495 - root_mean_squared_error: 0.4550\n",
      "Epoch 16/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2006 - mse: 0.2006 - mae: 0.3433 - root_mean_squared_error: 0.4479\n",
      "Epoch 17/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1945 - mse: 0.1945 - mae: 0.3373 - root_mean_squared_error: 0.4410\n",
      "Epoch 18/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1886 - mse: 0.1886 - mae: 0.3315 - root_mean_squared_error: 0.4343\n",
      "Epoch 19/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1830 - mse: 0.1830 - mae: 0.3258 - root_mean_squared_error: 0.4277\n",
      "Epoch 20/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1776 - mse: 0.1776 - mae: 0.3203 - root_mean_squared_error: 0.4214\n",
      "Epoch 21/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1724 - mse: 0.1724 - mae: 0.3149 - root_mean_squared_error: 0.4152\n",
      "Epoch 22/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1674 - mse: 0.1674 - mae: 0.3097 - root_mean_squared_error: 0.4092\n",
      "Epoch 23/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1626 - mse: 0.1626 - mae: 0.3045 - root_mean_squared_error: 0.4033\n",
      "Epoch 24/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1580 - mse: 0.1580 - mae: 0.2996 - root_mean_squared_error: 0.3975\n",
      "Epoch 25/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1536 - mse: 0.1536 - mae: 0.2947 - root_mean_squared_error: 0.3919\n",
      "Epoch 26/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1494 - mse: 0.1494 - mae: 0.2899 - root_mean_squared_error: 0.3865\n",
      "Epoch 27/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1453 - mse: 0.1453 - mae: 0.2852 - root_mean_squared_error: 0.3811\n",
      "Epoch 28/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1413 - mse: 0.1413 - mae: 0.2807 - root_mean_squared_error: 0.3759\n",
      "Epoch 29/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1375 - mse: 0.1375 - mae: 0.2762 - root_mean_squared_error: 0.3708\n",
      "Epoch 30/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1339 - mse: 0.1339 - mae: 0.2719 - root_mean_squared_error: 0.3659\n",
      "Epoch 31/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1303 - mse: 0.1303 - mae: 0.2676 - root_mean_squared_error: 0.3610\n",
      "Epoch 32/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1269 - mse: 0.1269 - mae: 0.2635 - root_mean_squared_error: 0.3563\n",
      "Epoch 33/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1237 - mse: 0.1237 - mae: 0.2594 - root_mean_squared_error: 0.3517\n",
      "Epoch 34/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1205 - mse: 0.1205 - mae: 0.2555 - root_mean_squared_error: 0.3472\n",
      "Epoch 35/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1175 - mse: 0.1175 - mae: 0.2516 - root_mean_squared_error: 0.3428\n",
      "Epoch 36/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1146 - mse: 0.1146 - mae: 0.2479 - root_mean_squared_error: 0.3385\n",
      "Epoch 37/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1118 - mse: 0.1118 - mae: 0.2442 - root_mean_squared_error: 0.3343\n",
      "Epoch 38/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1091 - mse: 0.1091 - mae: 0.2407 - root_mean_squared_error: 0.3303\n",
      "Epoch 39/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1065 - mse: 0.1065 - mae: 0.2372 - root_mean_squared_error: 0.3263\n",
      "Epoch 40/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1040 - mse: 0.1040 - mae: 0.2339 - root_mean_squared_error: 0.3225\n",
      "Epoch 41/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1016 - mse: 0.1016 - mae: 0.2306 - root_mean_squared_error: 0.3187\n",
      "Epoch 42/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0993 - mse: 0.0993 - mae: 0.2275 - root_mean_squared_error: 0.3150\n",
      "Epoch 43/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0970 - mse: 0.0970 - mae: 0.2244 - root_mean_squared_error: 0.3115\n",
      "Epoch 44/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0949 - mse: 0.0949 - mae: 0.2215 - root_mean_squared_error: 0.3080\n",
      "Epoch 45/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0928 - mse: 0.0928 - mae: 0.2186 - root_mean_squared_error: 0.3047\n",
      "Epoch 46/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0908 - mse: 0.0908 - mae: 0.2159 - root_mean_squared_error: 0.3014\n",
      "Epoch 47/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0889 - mse: 0.0889 - mae: 0.2132 - root_mean_squared_error: 0.2982\n",
      "Epoch 48/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0871 - mse: 0.0871 - mae: 0.2106 - root_mean_squared_error: 0.2951\n",
      "Epoch 49/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0853 - mse: 0.0853 - mae: 0.2081 - root_mean_squared_error: 0.2921\n",
      "Epoch 50/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0837 - mse: 0.0837 - mae: 0.2057 - root_mean_squared_error: 0.2892\n",
      "Epoch 51/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0820 - mse: 0.0820 - mae: 0.2033 - root_mean_squared_error: 0.2864\n",
      "Epoch 52/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0805 - mse: 0.0805 - mae: 0.2011 - root_mean_squared_error: 0.2836\n",
      "Epoch 53/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0790 - mse: 0.0790 - mae: 0.1989 - root_mean_squared_error: 0.2810\n",
      "Epoch 54/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0775 - mse: 0.0775 - mae: 0.1968 - root_mean_squared_error: 0.2784\n",
      "Epoch 55/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0761 - mse: 0.0761 - mae: 0.1947 - root_mean_squared_error: 0.2759\n",
      "Epoch 56/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0748 - mse: 0.0748 - mae: 0.1927 - root_mean_squared_error: 0.2735\n",
      "Epoch 57/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0735 - mse: 0.0735 - mae: 0.1908 - root_mean_squared_error: 0.2711\n",
      "Epoch 58/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0723 - mse: 0.0723 - mae: 0.1890 - root_mean_squared_error: 0.2688\n",
      "Epoch 59/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0711 - mse: 0.0711 - mae: 0.1872 - root_mean_squared_error: 0.2666\n",
      "Epoch 60/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0700 - mse: 0.0700 - mae: 0.1854 - root_mean_squared_error: 0.2645\n",
      "Epoch 61/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0689 - mse: 0.0689 - mae: 0.1837 - root_mean_squared_error: 0.2624\n",
      "Epoch 62/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0678 - mse: 0.0678 - mae: 0.1821 - root_mean_squared_error: 0.2604\n",
      "Epoch 63/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0668 - mse: 0.0668 - mae: 0.1805 - root_mean_squared_error: 0.2585\n",
      "Epoch 64/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0659 - mse: 0.0659 - mae: 0.1790 - root_mean_squared_error: 0.2566\n",
      "Epoch 65/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0649 - mse: 0.0649 - mae: 0.1775 - root_mean_squared_error: 0.2548\n",
      "Epoch 66/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0641 - mse: 0.0641 - mae: 0.1760 - root_mean_squared_error: 0.2531\n",
      "Epoch 67/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0632 - mse: 0.0632 - mae: 0.1746 - root_mean_squared_error: 0.2514\n",
      "Epoch 68/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0624 - mse: 0.0624 - mae: 0.1732 - root_mean_squared_error: 0.2498\n",
      "Epoch 69/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0616 - mse: 0.0616 - mae: 0.1719 - root_mean_squared_error: 0.2482\n",
      "Epoch 70/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0609 - mse: 0.0609 - mae: 0.1706 - root_mean_squared_error: 0.2467\n",
      "Epoch 71/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0602 - mse: 0.0602 - mae: 0.1693 - root_mean_squared_error: 0.2453\n",
      "Epoch 72/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0595 - mse: 0.0595 - mae: 0.1681 - root_mean_squared_error: 0.2438\n",
      "Epoch 73/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0588 - mse: 0.0588 - mae: 0.1669 - root_mean_squared_error: 0.2425\n",
      "Epoch 74/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0582 - mse: 0.0582 - mae: 0.1657 - root_mean_squared_error: 0.2412\n",
      "Epoch 75/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0575 - mse: 0.0575 - mae: 0.1646 - root_mean_squared_error: 0.2399\n",
      "Epoch 76/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0570 - mse: 0.0570 - mae: 0.1635 - root_mean_squared_error: 0.2387\n",
      "Epoch 77/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0564 - mse: 0.0564 - mae: 0.1625 - root_mean_squared_error: 0.2375\n",
      "Epoch 78/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0558 - mse: 0.0558 - mae: 0.1614 - root_mean_squared_error: 0.2363\n",
      "Epoch 79/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0553 - mse: 0.0553 - mae: 0.1604 - root_mean_squared_error: 0.2352\n",
      "Epoch 80/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0548 - mse: 0.0548 - mae: 0.1594 - root_mean_squared_error: 0.2341\n",
      "Epoch 81/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0543 - mse: 0.0543 - mae: 0.1585 - root_mean_squared_error: 0.2330\n",
      "Epoch 82/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0538 - mse: 0.0538 - mae: 0.1576 - root_mean_squared_error: 0.2320\n",
      "Epoch 83/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0534 - mse: 0.0534 - mae: 0.1567 - root_mean_squared_error: 0.2310\n",
      "Epoch 84/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0529 - mse: 0.0529 - mae: 0.1558 - root_mean_squared_error: 0.2300\n",
      "Epoch 85/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0525 - mse: 0.0525 - mae: 0.1549 - root_mean_squared_error: 0.2291\n",
      "Epoch 86/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0520 - mse: 0.0520 - mae: 0.1541 - root_mean_squared_error: 0.2281\n",
      "Epoch 87/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0516 - mse: 0.0516 - mae: 0.1533 - root_mean_squared_error: 0.2272\n",
      "Epoch 88/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0512 - mse: 0.0512 - mae: 0.1525 - root_mean_squared_error: 0.2263\n",
      "Epoch 89/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0508 - mse: 0.0508 - mae: 0.1517 - root_mean_squared_error: 0.2253\n",
      "Epoch 90/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0504 - mse: 0.0504 - mae: 0.1510 - root_mean_squared_error: 0.2244\n",
      "Epoch 91/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0500 - mse: 0.0500 - mae: 0.1502 - root_mean_squared_error: 0.2236\n",
      "Epoch 92/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0496 - mse: 0.0496 - mae: 0.1495 - root_mean_squared_error: 0.2227\n",
      "Epoch 93/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0492 - mse: 0.0492 - mae: 0.1487 - root_mean_squared_error: 0.2218\n",
      "Epoch 94/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0488 - mse: 0.0488 - mae: 0.1481 - root_mean_squared_error: 0.2209\n",
      "Epoch 95/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0484 - mse: 0.0484 - mae: 0.1473 - root_mean_squared_error: 0.2201\n",
      "Epoch 96/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0480 - mse: 0.0480 - mae: 0.1467 - root_mean_squared_error: 0.2192\n",
      "Epoch 97/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0477 - mse: 0.0477 - mae: 0.1460 - root_mean_squared_error: 0.2183\n",
      "Epoch 98/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0473 - mse: 0.0473 - mae: 0.1453 - root_mean_squared_error: 0.2175\n",
      "Epoch 99/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0469 - mse: 0.0469 - mae: 0.1446 - root_mean_squared_error: 0.2166\n",
      "Epoch 100/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0466 - mse: 0.0466 - mae: 0.1440 - root_mean_squared_error: 0.2158\n",
      "Epoch 101/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0462 - mse: 0.0462 - mae: 0.1433 - root_mean_squared_error: 0.2149\n",
      "Epoch 102/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0458 - mse: 0.0458 - mae: 0.1427 - root_mean_squared_error: 0.2141\n",
      "Epoch 103/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0455 - mse: 0.0455 - mae: 0.1420 - root_mean_squared_error: 0.2132\n",
      "Epoch 104/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0451 - mse: 0.0451 - mae: 0.1414 - root_mean_squared_error: 0.2124\n",
      "Epoch 105/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0448 - mse: 0.0448 - mae: 0.1408 - root_mean_squared_error: 0.2115\n",
      "Epoch 106/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0444 - mse: 0.0444 - mae: 0.1401 - root_mean_squared_error: 0.2107\n",
      "Epoch 107/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0440 - mse: 0.0440 - mae: 0.1395 - root_mean_squared_error: 0.2099\n",
      "Epoch 108/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0437 - mse: 0.0437 - mae: 0.1389 - root_mean_squared_error: 0.2091\n",
      "Epoch 109/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0434 - mse: 0.0434 - mae: 0.1383 - root_mean_squared_error: 0.2082\n",
      "Epoch 110/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0430 - mse: 0.0430 - mae: 0.1377 - root_mean_squared_error: 0.2074\n",
      "Epoch 111/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0427 - mse: 0.0427 - mae: 0.1371 - root_mean_squared_error: 0.2066\n",
      "Epoch 112/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0424 - mse: 0.0424 - mae: 0.1365 - root_mean_squared_error: 0.2058\n",
      "Epoch 113/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0420 - mse: 0.0420 - mae: 0.1359 - root_mean_squared_error: 0.2050\n",
      "Epoch 114/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0417 - mse: 0.0417 - mae: 0.1353 - root_mean_squared_error: 0.2042\n",
      "Epoch 115/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0414 - mae: 0.1347 - root_mean_squared_error: 0.2034\n",
      "Epoch 116/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0411 - mse: 0.0411 - mae: 0.1342 - root_mean_squared_error: 0.2026\n",
      "Epoch 117/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0407 - mse: 0.0407 - mae: 0.1336 - root_mean_squared_error: 0.2019\n",
      "Epoch 118/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0404 - mse: 0.0404 - mae: 0.1330 - root_mean_squared_error: 0.2011\n",
      "Epoch 119/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0401 - mse: 0.0401 - mae: 0.1325 - root_mean_squared_error: 0.2003\n",
      "Epoch 120/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0398 - mse: 0.0398 - mae: 0.1319 - root_mean_squared_error: 0.1996\n",
      "Epoch 121/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0395 - mse: 0.0395 - mae: 0.1314 - root_mean_squared_error: 0.1988\n",
      "Epoch 122/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0392 - mse: 0.0392 - mae: 0.1308 - root_mean_squared_error: 0.1981\n",
      "Epoch 123/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0390 - mse: 0.0390 - mae: 0.1303 - root_mean_squared_error: 0.1974\n",
      "Epoch 124/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0387 - mse: 0.0387 - mae: 0.1298 - root_mean_squared_error: 0.1967\n",
      "Epoch 125/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0384 - mse: 0.0384 - mae: 0.1293 - root_mean_squared_error: 0.1959\n",
      "Epoch 126/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0381 - mse: 0.0381 - mae: 0.1288 - root_mean_squared_error: 0.1952\n",
      "Epoch 127/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0378 - mse: 0.0378 - mae: 0.1283 - root_mean_squared_error: 0.1945\n",
      "Epoch 128/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0376 - mse: 0.0376 - mae: 0.1278 - root_mean_squared_error: 0.1938\n",
      "Epoch 129/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0373 - mse: 0.0373 - mae: 0.1273 - root_mean_squared_error: 0.1932\n",
      "Epoch 130/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0370 - mse: 0.0370 - mae: 0.1268 - root_mean_squared_error: 0.1925\n",
      "Epoch 131/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0368 - mse: 0.0368 - mae: 0.1263 - root_mean_squared_error: 0.1918\n",
      "Epoch 132/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0365 - mse: 0.0365 - mae: 0.1259 - root_mean_squared_error: 0.1912\n",
      "Epoch 133/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0363 - mse: 0.0363 - mae: 0.1254 - root_mean_squared_error: 0.1905\n",
      "Epoch 134/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0361 - mse: 0.0361 - mae: 0.1250 - root_mean_squared_error: 0.1899\n",
      "Epoch 135/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0358 - mse: 0.0358 - mae: 0.1245 - root_mean_squared_error: 0.1892\n",
      "Epoch 136/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0356 - mse: 0.0356 - mae: 0.1241 - root_mean_squared_error: 0.1886\n",
      "Epoch 137/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0354 - mse: 0.0354 - mae: 0.1237 - root_mean_squared_error: 0.1880\n",
      "Epoch 138/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0351 - mse: 0.0351 - mae: 0.1233 - root_mean_squared_error: 0.1874\n",
      "Epoch 139/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0349 - mse: 0.0349 - mae: 0.1229 - root_mean_squared_error: 0.1868\n",
      "Epoch 140/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0347 - mse: 0.0347 - mae: 0.1225 - root_mean_squared_error: 0.1863\n",
      "Epoch 141/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0345 - mse: 0.0345 - mae: 0.1221 - root_mean_squared_error: 0.1857\n",
      "Epoch 142/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0343 - mse: 0.0343 - mae: 0.1217 - root_mean_squared_error: 0.1851\n",
      "Epoch 143/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0341 - mse: 0.0341 - mae: 0.1213 - root_mean_squared_error: 0.1846\n",
      "Epoch 144/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0339 - mse: 0.0339 - mae: 0.1210 - root_mean_squared_error: 0.1840\n",
      "Epoch 145/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0337 - mse: 0.0337 - mae: 0.1206 - root_mean_squared_error: 0.1835\n",
      "Epoch 146/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0335 - mse: 0.0335 - mae: 0.1203 - root_mean_squared_error: 0.1830\n",
      "Epoch 147/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0333 - mse: 0.0333 - mae: 0.1199 - root_mean_squared_error: 0.1824\n",
      "Epoch 148/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0331 - mse: 0.0331 - mae: 0.1196 - root_mean_squared_error: 0.1819\n",
      "Epoch 149/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0329 - mse: 0.0329 - mae: 0.1192 - root_mean_squared_error: 0.1814\n",
      "Epoch 150/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0327 - mse: 0.0327 - mae: 0.1189 - root_mean_squared_error: 0.1809\n",
      "Epoch 151/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0326 - mse: 0.0326 - mae: 0.1186 - root_mean_squared_error: 0.1805\n",
      "Epoch 152/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0324 - mse: 0.0324 - mae: 0.1183 - root_mean_squared_error: 0.1800\n",
      "Epoch 153/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0322 - mse: 0.0322 - mae: 0.1180 - root_mean_squared_error: 0.1795\n",
      "Epoch 154/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0321 - mse: 0.0321 - mae: 0.1177 - root_mean_squared_error: 0.1791\n",
      "Epoch 155/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0319 - mse: 0.0319 - mae: 0.1174 - root_mean_squared_error: 0.1786\n",
      "Epoch 156/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0318 - mse: 0.0318 - mae: 0.1172 - root_mean_squared_error: 0.1782\n",
      "Epoch 157/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0316 - mse: 0.0316 - mae: 0.1169 - root_mean_squared_error: 0.1778\n",
      "Epoch 158/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0315 - mse: 0.0315 - mae: 0.1166 - root_mean_squared_error: 0.1773\n",
      "Epoch 159/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0313 - mse: 0.0313 - mae: 0.1164 - root_mean_squared_error: 0.1769\n",
      "Epoch 160/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0312 - mse: 0.0312 - mae: 0.1161 - root_mean_squared_error: 0.1765\n",
      "Epoch 161/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0310 - mse: 0.0310 - mae: 0.1159 - root_mean_squared_error: 0.1761\n",
      "Epoch 162/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0309 - mse: 0.0309 - mae: 0.1156 - root_mean_squared_error: 0.1757\n",
      "Epoch 163/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0307 - mse: 0.0307 - mae: 0.1154 - root_mean_squared_error: 0.1753\n",
      "Epoch 164/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0306 - mse: 0.0306 - mae: 0.1151 - root_mean_squared_error: 0.1749\n",
      "Epoch 165/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0305 - mse: 0.0305 - mae: 0.1149 - root_mean_squared_error: 0.1745\n",
      "Epoch 166/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0303 - mse: 0.0303 - mae: 0.1147 - root_mean_squared_error: 0.1742\n",
      "Epoch 167/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0302 - mse: 0.0302 - mae: 0.1145 - root_mean_squared_error: 0.1738\n",
      "Epoch 168/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0301 - mse: 0.0301 - mae: 0.1143 - root_mean_squared_error: 0.1735\n",
      "Epoch 169/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0300 - mse: 0.0300 - mae: 0.1141 - root_mean_squared_error: 0.1731\n",
      "Epoch 170/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0299 - mse: 0.0299 - mae: 0.1139 - root_mean_squared_error: 0.1728\n",
      "Epoch 171/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0297 - mse: 0.0297 - mae: 0.1137 - root_mean_squared_error: 0.1725\n",
      "Epoch 172/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0296 - mse: 0.0296 - mae: 0.1135 - root_mean_squared_error: 0.1721\n",
      "Epoch 173/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0295 - mse: 0.0295 - mae: 0.1134 - root_mean_squared_error: 0.1718\n",
      "Epoch 174/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0294 - mse: 0.0294 - mae: 0.1131 - root_mean_squared_error: 0.1715\n",
      "Epoch 175/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0293 - mse: 0.0293 - mae: 0.1130 - root_mean_squared_error: 0.1712\n",
      "Epoch 176/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0292 - mse: 0.0292 - mae: 0.1128 - root_mean_squared_error: 0.1709\n",
      "Epoch 177/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0291 - mse: 0.0291 - mae: 0.1127 - root_mean_squared_error: 0.1706\n",
      "Epoch 178/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0290 - mse: 0.0290 - mae: 0.1124 - root_mean_squared_error: 0.1703\n",
      "Epoch 179/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0289 - mse: 0.0289 - mae: 0.1123 - root_mean_squared_error: 0.1700\n",
      "Epoch 180/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0288 - mse: 0.0288 - mae: 0.1122 - root_mean_squared_error: 0.1697\n",
      "Epoch 181/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0287 - mse: 0.0287 - mae: 0.1121 - root_mean_squared_error: 0.1694\n",
      "Epoch 182/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0286 - mse: 0.0286 - mae: 0.1118 - root_mean_squared_error: 0.1692\n",
      "Epoch 183/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0285 - mse: 0.0285 - mae: 0.1119 - root_mean_squared_error: 0.1689\n",
      "Epoch 184/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0284 - mse: 0.0284 - mae: 0.1116 - root_mean_squared_error: 0.1686\n",
      "Epoch 185/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0284 - mse: 0.0284 - mae: 0.1115 - root_mean_squared_error: 0.1684\n",
      "Epoch 186/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0283 - mse: 0.0283 - mae: 0.1113 - root_mean_squared_error: 0.1681\n",
      "Epoch 187/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0282 - mse: 0.0282 - mae: 0.1113 - root_mean_squared_error: 0.1679\n",
      "Epoch 188/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0281 - mse: 0.0281 - mae: 0.1111 - root_mean_squared_error: 0.1676\n",
      "Epoch 189/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0280 - mse: 0.0280 - mae: 0.1110 - root_mean_squared_error: 0.1674\n",
      "Epoch 190/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0280 - mse: 0.0280 - mae: 0.1109 - root_mean_squared_error: 0.1672\n",
      "Epoch 191/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0279 - mse: 0.0279 - mae: 0.1108 - root_mean_squared_error: 0.1670\n",
      "Epoch 192/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0278 - mse: 0.0278 - mae: 0.1106 - root_mean_squared_error: 0.1667\n",
      "Epoch 193/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0277 - mse: 0.0277 - mae: 0.1106 - root_mean_squared_error: 0.1665\n",
      "Epoch 194/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0276 - mse: 0.0276 - mae: 0.1104 - root_mean_squared_error: 0.1663\n",
      "Epoch 195/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0276 - mse: 0.0276 - mae: 0.1103 - root_mean_squared_error: 0.1661\n",
      "Epoch 196/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0275 - mse: 0.0275 - mae: 0.1102 - root_mean_squared_error: 0.1658\n",
      "Epoch 197/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0274 - mse: 0.0274 - mae: 0.1102 - root_mean_squared_error: 0.1656\n",
      "Epoch 198/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0274 - mse: 0.0274 - mae: 0.1100 - root_mean_squared_error: 0.1654\n",
      "Epoch 199/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0273 - mse: 0.0273 - mae: 0.1100 - root_mean_squared_error: 0.1652\n",
      "Epoch 200/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0272 - mse: 0.0272 - mae: 0.1098 - root_mean_squared_error: 0.1650\n",
      "Epoch 201/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0272 - mse: 0.0272 - mae: 0.1099 - root_mean_squared_error: 0.1649\n",
      "Epoch 202/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0271 - mse: 0.0271 - mae: 0.1096 - root_mean_squared_error: 0.1647\n",
      "Epoch 203/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0271 - mse: 0.0271 - mae: 0.1097 - root_mean_squared_error: 0.1645\n",
      "Epoch 204/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0270 - mse: 0.0270 - mae: 0.1094 - root_mean_squared_error: 0.1643\n",
      "Epoch 205/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0269 - mse: 0.0269 - mae: 0.1095 - root_mean_squared_error: 0.1641\n",
      "Epoch 206/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0269 - mse: 0.0269 - mae: 0.1092 - root_mean_squared_error: 0.1639\n",
      "Epoch 207/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0268 - mse: 0.0268 - mae: 0.1093 - root_mean_squared_error: 0.1637\n",
      "Epoch 208/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0268 - mse: 0.0268 - mae: 0.1091 - root_mean_squared_error: 0.1636\n",
      "Epoch 209/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0267 - mse: 0.0267 - mae: 0.1091 - root_mean_squared_error: 0.1634\n",
      "Epoch 210/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0266 - mse: 0.0266 - mae: 0.1089 - root_mean_squared_error: 0.1632\n",
      "Epoch 211/400\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.0266 - mse: 0.0266 - mae: 0.1089 - root_mean_squared_error: 0.1631\n",
      "Epoch 212/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0265 - mae: 0.1087 - root_mean_squared_error: 0.1629\n",
      "Epoch 213/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0265 - mse: 0.0265 - mae: 0.1088 - root_mean_squared_error: 0.1627\n",
      "Epoch 214/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0264 - mse: 0.0264 - mae: 0.1086 - root_mean_squared_error: 0.1626\n",
      "Epoch 215/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0264 - mse: 0.0264 - mae: 0.1087 - root_mean_squared_error: 0.1624\n",
      "Epoch 216/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0263 - mse: 0.0263 - mae: 0.1084 - root_mean_squared_error: 0.1623\n",
      "Epoch 217/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0263 - mse: 0.0263 - mae: 0.1086 - root_mean_squared_error: 0.1621\n",
      "Epoch 218/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0262 - mse: 0.0262 - mae: 0.1083 - root_mean_squared_error: 0.1620\n",
      "Epoch 219/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0262 - mse: 0.0262 - mae: 0.1084 - root_mean_squared_error: 0.1618\n",
      "Epoch 220/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0261 - mse: 0.0261 - mae: 0.1081 - root_mean_squared_error: 0.1617\n",
      "Epoch 221/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0261 - mse: 0.0261 - mae: 0.1083 - root_mean_squared_error: 0.1616\n",
      "Epoch 222/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0261 - mse: 0.0261 - mae: 0.1080 - root_mean_squared_error: 0.1614\n",
      "Epoch 223/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0260 - mse: 0.0260 - mae: 0.1081 - root_mean_squared_error: 0.1613\n",
      "Epoch 224/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0260 - mse: 0.0260 - mae: 0.1079 - root_mean_squared_error: 0.1611\n",
      "Epoch 225/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0259 - mse: 0.0259 - mae: 0.1080 - root_mean_squared_error: 0.1610\n",
      "Epoch 226/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0259 - mse: 0.0259 - mae: 0.1077 - root_mean_squared_error: 0.1609\n",
      "Epoch 227/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0258 - mse: 0.0258 - mae: 0.1079 - root_mean_squared_error: 0.1607\n",
      "Epoch 228/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0258 - mse: 0.0258 - mae: 0.1076 - root_mean_squared_error: 0.1606\n",
      "Epoch 229/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0258 - mse: 0.0258 - mae: 0.1078 - root_mean_squared_error: 0.1605\n",
      "Epoch 230/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0257 - mse: 0.0257 - mae: 0.1074 - root_mean_squared_error: 0.1604\n",
      "Epoch 231/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0257 - mse: 0.0257 - mae: 0.1077 - root_mean_squared_error: 0.1603\n",
      "Epoch 232/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0256 - mse: 0.0256 - mae: 0.1073 - root_mean_squared_error: 0.1601\n",
      "Epoch 233/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0256 - mse: 0.0256 - mae: 0.1075 - root_mean_squared_error: 0.1600\n",
      "Epoch 234/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0256 - mse: 0.0256 - mae: 0.1072 - root_mean_squared_error: 0.1599\n",
      "Epoch 235/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0255 - mse: 0.0255 - mae: 0.1074 - root_mean_squared_error: 0.1598\n",
      "Epoch 236/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0255 - mse: 0.0255 - mae: 0.1071 - root_mean_squared_error: 0.1597\n",
      "Epoch 237/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0255 - mse: 0.0255 - mae: 0.1073 - root_mean_squared_error: 0.1596\n",
      "Epoch 238/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0254 - mse: 0.0254 - mae: 0.1070 - root_mean_squared_error: 0.1594\n",
      "Epoch 239/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0254 - mse: 0.0254 - mae: 0.1071 - root_mean_squared_error: 0.1593\n",
      "Epoch 240/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0254 - mse: 0.0254 - mae: 0.1069 - root_mean_squared_error: 0.1592\n",
      "Epoch 241/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0253 - mse: 0.0253 - mae: 0.1070 - root_mean_squared_error: 0.1591\n",
      "Epoch 242/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0253 - mse: 0.0253 - mae: 0.1067 - root_mean_squared_error: 0.1590\n",
      "Epoch 243/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0253 - mse: 0.0253 - mae: 0.1069 - root_mean_squared_error: 0.1589\n",
      "Epoch 244/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0252 - mse: 0.0252 - mae: 0.1066 - root_mean_squared_error: 0.1588\n",
      "Epoch 245/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0252 - mse: 0.0252 - mae: 0.1069 - root_mean_squared_error: 0.1587\n",
      "Epoch 246/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0252 - mse: 0.0252 - mae: 0.1065 - root_mean_squared_error: 0.1586\n",
      "Epoch 247/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0251 - mse: 0.0251 - mae: 0.1068 - root_mean_squared_error: 0.1585\n",
      "Epoch 248/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0251 - mse: 0.0251 - mae: 0.1064 - root_mean_squared_error: 0.1584\n",
      "Epoch 249/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0251 - mse: 0.0251 - mae: 0.1066 - root_mean_squared_error: 0.1583\n",
      "Epoch 250/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0250 - mse: 0.0250 - mae: 0.1063 - root_mean_squared_error: 0.1582\n",
      "Epoch 251/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0250 - mse: 0.0250 - mae: 0.1065 - root_mean_squared_error: 0.1581\n",
      "Epoch 252/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0250 - mse: 0.0250 - mae: 0.1062 - root_mean_squared_error: 0.1580\n",
      "Epoch 253/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0249 - mse: 0.0249 - mae: 0.1064 - root_mean_squared_error: 0.1579\n",
      "Epoch 254/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0249 - mse: 0.0249 - mae: 0.1061 - root_mean_squared_error: 0.1579\n",
      "Epoch 255/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0249 - mse: 0.0249 - mae: 0.1063 - root_mean_squared_error: 0.1578\n",
      "Epoch 256/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0249 - mse: 0.0249 - mae: 0.1060 - root_mean_squared_error: 0.1577\n",
      "Epoch 257/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0248 - mse: 0.0248 - mae: 0.1062 - root_mean_squared_error: 0.1576\n",
      "Epoch 258/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0248 - mse: 0.0248 - mae: 0.1059 - root_mean_squared_error: 0.1575\n",
      "Epoch 259/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0248 - mse: 0.0248 - mae: 0.1061 - root_mean_squared_error: 0.1574\n",
      "Epoch 260/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0248 - mse: 0.0248 - mae: 0.1058 - root_mean_squared_error: 0.1573\n",
      "Epoch 261/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0247 - mse: 0.0247 - mae: 0.1060 - root_mean_squared_error: 0.1572\n",
      "Epoch 262/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0247 - mse: 0.0247 - mae: 0.1057 - root_mean_squared_error: 0.1572\n",
      "Epoch 263/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0247 - mse: 0.0247 - mae: 0.1059 - root_mean_squared_error: 0.1571\n",
      "Epoch 264/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0246 - mse: 0.0246 - mae: 0.1056 - root_mean_squared_error: 0.1570\n",
      "Epoch 265/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0246 - mse: 0.0246 - mae: 0.1058 - root_mean_squared_error: 0.1569\n",
      "Epoch 266/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0246 - mse: 0.0246 - mae: 0.1055 - root_mean_squared_error: 0.1568\n",
      "Epoch 267/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0246 - mse: 0.0246 - mae: 0.1057 - root_mean_squared_error: 0.1568\n",
      "Epoch 268/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0245 - mse: 0.0245 - mae: 0.1054 - root_mean_squared_error: 0.1567\n",
      "Epoch 269/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0245 - mse: 0.0245 - mae: 0.1056 - root_mean_squared_error: 0.1566\n",
      "Epoch 270/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0245 - mse: 0.0245 - mae: 0.1053 - root_mean_squared_error: 0.1565\n",
      "Epoch 271/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0245 - mse: 0.0245 - mae: 0.1055 - root_mean_squared_error: 0.1564\n",
      "Epoch 272/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0245 - mse: 0.0245 - mae: 0.1052 - root_mean_squared_error: 0.1564\n",
      "Epoch 273/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0244 - mse: 0.0244 - mae: 0.1054 - root_mean_squared_error: 0.1563\n",
      "Epoch 274/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0244 - mse: 0.0244 - mae: 0.1051 - root_mean_squared_error: 0.1562\n",
      "Epoch 275/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0244 - mse: 0.0244 - mae: 0.1053 - root_mean_squared_error: 0.1561\n",
      "Epoch 276/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0244 - mse: 0.0244 - mae: 0.1050 - root_mean_squared_error: 0.1561\n",
      "Epoch 277/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0243 - mse: 0.0243 - mae: 0.1052 - root_mean_squared_error: 0.1560\n",
      "Epoch 278/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0243 - mse: 0.0243 - mae: 0.1049 - root_mean_squared_error: 0.1559\n",
      "Epoch 279/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0243 - mse: 0.0243 - mae: 0.1052 - root_mean_squared_error: 0.1559\n",
      "Epoch 280/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0243 - mse: 0.0243 - mae: 0.1048 - root_mean_squared_error: 0.1558\n",
      "Epoch 281/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0242 - mse: 0.0242 - mae: 0.1051 - root_mean_squared_error: 0.1557\n",
      "Epoch 282/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0242 - mse: 0.0242 - mae: 0.1047 - root_mean_squared_error: 0.1557\n",
      "Epoch 283/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0242 - mse: 0.0242 - mae: 0.1050 - root_mean_squared_error: 0.1556\n",
      "Epoch 284/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0242 - mse: 0.0242 - mae: 0.1046 - root_mean_squared_error: 0.1555\n",
      "Epoch 285/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0242 - mse: 0.0242 - mae: 0.1049 - root_mean_squared_error: 0.1554\n",
      "Epoch 286/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0241 - mse: 0.0241 - mae: 0.1045 - root_mean_squared_error: 0.1554\n",
      "Epoch 287/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0241 - mse: 0.0241 - mae: 0.1048 - root_mean_squared_error: 0.1553\n",
      "Epoch 288/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0241 - mse: 0.0241 - mae: 0.1044 - root_mean_squared_error: 0.1553\n",
      "Epoch 289/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0241 - mse: 0.0241 - mae: 0.1047 - root_mean_squared_error: 0.1552\n",
      "Epoch 290/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0241 - mse: 0.0241 - mae: 0.1043 - root_mean_squared_error: 0.1551\n",
      "Epoch 291/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0240 - mse: 0.0240 - mae: 0.1046 - root_mean_squared_error: 0.1551\n",
      "Epoch 292/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0240 - mse: 0.0240 - mae: 0.1042 - root_mean_squared_error: 0.1550\n",
      "Epoch 293/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0240 - mse: 0.0240 - mae: 0.1045 - root_mean_squared_error: 0.1549\n",
      "Epoch 294/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0240 - mse: 0.0240 - mae: 0.1042 - root_mean_squared_error: 0.1549\n",
      "Epoch 295/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0240 - mse: 0.0240 - mae: 0.1044 - root_mean_squared_error: 0.1548\n",
      "Epoch 296/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0239 - mse: 0.0239 - mae: 0.1041 - root_mean_squared_error: 0.1547\n",
      "Epoch 297/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0239 - mse: 0.0239 - mae: 0.1043 - root_mean_squared_error: 0.1547\n",
      "Epoch 298/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0239 - mse: 0.0239 - mae: 0.1040 - root_mean_squared_error: 0.1546\n",
      "Epoch 299/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0239 - mse: 0.0239 - mae: 0.1043 - root_mean_squared_error: 0.1546\n",
      "Epoch 300/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0239 - mse: 0.0239 - mae: 0.1039 - root_mean_squared_error: 0.1545\n",
      "Epoch 301/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0239 - mse: 0.0239 - mae: 0.1042 - root_mean_squared_error: 0.1545\n",
      "Epoch 302/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0238 - mse: 0.0238 - mae: 0.1038 - root_mean_squared_error: 0.1544\n",
      "Epoch 303/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0238 - mse: 0.0238 - mae: 0.1041 - root_mean_squared_error: 0.1543\n",
      "Epoch 304/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0238 - mse: 0.0238 - mae: 0.1037 - root_mean_squared_error: 0.1543\n",
      "Epoch 305/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0238 - mse: 0.0238 - mae: 0.1040 - root_mean_squared_error: 0.1542\n",
      "Epoch 306/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0238 - mse: 0.0238 - mae: 0.1036 - root_mean_squared_error: 0.1542\n",
      "Epoch 307/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0237 - mse: 0.0237 - mae: 0.1039 - root_mean_squared_error: 0.1541\n",
      "Epoch 308/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0237 - mse: 0.0237 - mae: 0.1036 - root_mean_squared_error: 0.1541\n",
      "Epoch 309/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0237 - mse: 0.0237 - mae: 0.1038 - root_mean_squared_error: 0.1540\n",
      "Epoch 310/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0237 - mse: 0.0237 - mae: 0.1035 - root_mean_squared_error: 0.1539\n",
      "Epoch 311/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0237 - mse: 0.0237 - mae: 0.1038 - root_mean_squared_error: 0.1539\n",
      "Epoch 312/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0237 - mse: 0.0237 - mae: 0.1034 - root_mean_squared_error: 0.1538\n",
      "Epoch 313/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0236 - mse: 0.0236 - mae: 0.1037 - root_mean_squared_error: 0.1538\n",
      "Epoch 314/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0236 - mse: 0.0236 - mae: 0.1033 - root_mean_squared_error: 0.1537\n",
      "Epoch 315/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0236 - mse: 0.0236 - mae: 0.1036 - root_mean_squared_error: 0.1537\n",
      "Epoch 316/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0236 - mse: 0.0236 - mae: 0.1032 - root_mean_squared_error: 0.1536\n",
      "Epoch 317/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0236 - mse: 0.0236 - mae: 0.1035 - root_mean_squared_error: 0.1536\n",
      "Epoch 318/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0236 - mse: 0.0236 - mae: 0.1032 - root_mean_squared_error: 0.1535\n",
      "Epoch 319/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0236 - mse: 0.0236 - mae: 0.1034 - root_mean_squared_error: 0.1535\n",
      "Epoch 320/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0235 - mse: 0.0235 - mae: 0.1031 - root_mean_squared_error: 0.1534\n",
      "Epoch 321/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0235 - mse: 0.0235 - mae: 0.1034 - root_mean_squared_error: 0.1534\n",
      "Epoch 322/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0235 - mse: 0.0235 - mae: 0.1030 - root_mean_squared_error: 0.1533\n",
      "Epoch 323/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0235 - mse: 0.0235 - mae: 0.1033 - root_mean_squared_error: 0.1533\n",
      "Epoch 324/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0235 - mse: 0.0235 - mae: 0.1029 - root_mean_squared_error: 0.1532\n",
      "Epoch 325/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0235 - mse: 0.0235 - mae: 0.1032 - root_mean_squared_error: 0.1532\n",
      "Epoch 326/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0234 - mse: 0.0234 - mae: 0.1028 - root_mean_squared_error: 0.1531\n",
      "Epoch 327/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0234 - mse: 0.0234 - mae: 0.1031 - root_mean_squared_error: 0.1531\n",
      "Epoch 328/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0234 - mse: 0.0234 - mae: 0.1028 - root_mean_squared_error: 0.1530\n",
      "Epoch 329/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0234 - mse: 0.0234 - mae: 0.1030 - root_mean_squared_error: 0.1530\n",
      "Epoch 330/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0234 - mse: 0.0234 - mae: 0.1027 - root_mean_squared_error: 0.1529\n",
      "Epoch 331/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0234 - mse: 0.0234 - mae: 0.1030 - root_mean_squared_error: 0.1529\n",
      "Epoch 332/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0234 - mse: 0.0234 - mae: 0.1026 - root_mean_squared_error: 0.1528\n",
      "Epoch 333/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0233 - mse: 0.0233 - mae: 0.1029 - root_mean_squared_error: 0.1528\n",
      "Epoch 334/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0233 - mse: 0.0233 - mae: 0.1025 - root_mean_squared_error: 0.1527\n",
      "Epoch 335/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0233 - mse: 0.0233 - mae: 0.1028 - root_mean_squared_error: 0.1527\n",
      "Epoch 336/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0233 - mse: 0.0233 - mae: 0.1025 - root_mean_squared_error: 0.1527\n",
      "Epoch 337/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0233 - mse: 0.0233 - mae: 0.1028 - root_mean_squared_error: 0.1526\n",
      "Epoch 338/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0233 - mse: 0.0233 - mae: 0.1024 - root_mean_squared_error: 0.1526\n",
      "Epoch 339/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0233 - mse: 0.0233 - mae: 0.1027 - root_mean_squared_error: 0.1525\n",
      "Epoch 340/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0232 - mse: 0.0232 - mae: 0.1023 - root_mean_squared_error: 0.1525\n",
      "Epoch 341/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0232 - mse: 0.0232 - mae: 0.1026 - root_mean_squared_error: 0.1524\n",
      "Epoch 342/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0232 - mse: 0.0232 - mae: 0.1022 - root_mean_squared_error: 0.1524\n",
      "Epoch 343/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0232 - mse: 0.0232 - mae: 0.1025 - root_mean_squared_error: 0.1523\n",
      "Epoch 344/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0232 - mse: 0.0232 - mae: 0.1022 - root_mean_squared_error: 0.1523\n",
      "Epoch 345/400\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0232 - mse: 0.0232 - mae: 0.1025 - root_mean_squared_error: 0.1523\n",
      "Epoch 346/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0232 - mse: 0.0232 - mae: 0.1021 - root_mean_squared_error: 0.1522\n",
      "Epoch 347/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0232 - mse: 0.0232 - mae: 0.1024 - root_mean_squared_error: 0.1522\n",
      "Epoch 348/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1020 - root_mean_squared_error: 0.1521\n",
      "Epoch 349/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1023 - root_mean_squared_error: 0.1521\n",
      "Epoch 350/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1020 - root_mean_squared_error: 0.1520\n",
      "Epoch 351/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1023 - root_mean_squared_error: 0.1520\n",
      "Epoch 352/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1019 - root_mean_squared_error: 0.1520\n",
      "Epoch 353/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1022 - root_mean_squared_error: 0.1519\n",
      "Epoch 354/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1018 - root_mean_squared_error: 0.1519\n",
      "Epoch 355/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1021 - root_mean_squared_error: 0.1518\n",
      "Epoch 356/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0230 - mse: 0.0230 - mae: 0.1018 - root_mean_squared_error: 0.1518\n",
      "Epoch 357/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0230 - mse: 0.0230 - mae: 0.1021 - root_mean_squared_error: 0.1518\n",
      "Epoch 358/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0230 - mse: 0.0230 - mae: 0.1017 - root_mean_squared_error: 0.1517\n",
      "Epoch 359/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0230 - mse: 0.0230 - mae: 0.1020 - root_mean_squared_error: 0.1517\n",
      "Epoch 360/400\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0230 - mse: 0.0230 - mae: 0.1016 - root_mean_squared_error: 0.1516\n",
      "Epoch 361/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0230 - mse: 0.0230 - mae: 0.1019 - root_mean_squared_error: 0.1516\n",
      "Epoch 362/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0230 - mse: 0.0230 - mae: 0.1016 - root_mean_squared_error: 0.1516\n",
      "Epoch 363/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0230 - mse: 0.0230 - mae: 0.1019 - root_mean_squared_error: 0.1515\n",
      "Epoch 364/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0230 - mse: 0.0230 - mae: 0.1015 - root_mean_squared_error: 0.1515\n",
      "Epoch 365/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0229 - mse: 0.0229 - mae: 0.1018 - root_mean_squared_error: 0.1515\n",
      "Epoch 366/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0229 - mse: 0.0229 - mae: 0.1014 - root_mean_squared_error: 0.1514\n",
      "Epoch 367/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0229 - mse: 0.0229 - mae: 0.1017 - root_mean_squared_error: 0.1514\n",
      "Epoch 368/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0229 - mse: 0.0229 - mae: 0.1014 - root_mean_squared_error: 0.1513\n",
      "Epoch 369/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0229 - mse: 0.0229 - mae: 0.1017 - root_mean_squared_error: 0.1513\n",
      "Epoch 370/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0229 - mse: 0.0229 - mae: 0.1013 - root_mean_squared_error: 0.1513\n",
      "Epoch 371/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0229 - mse: 0.0229 - mae: 0.1016 - root_mean_squared_error: 0.1512\n",
      "Epoch 372/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0229 - mse: 0.0229 - mae: 0.1013 - root_mean_squared_error: 0.1512\n",
      "Epoch 373/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.1015 - root_mean_squared_error: 0.1512\n",
      "Epoch 374/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.1012 - root_mean_squared_error: 0.1511\n",
      "Epoch 375/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.1015 - root_mean_squared_error: 0.1511\n",
      "Epoch 376/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.1011 - root_mean_squared_error: 0.1511\n",
      "Epoch 377/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.1014 - root_mean_squared_error: 0.1510\n",
      "Epoch 378/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.1011 - root_mean_squared_error: 0.1510\n",
      "Epoch 379/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.1014 - root_mean_squared_error: 0.1509\n",
      "Epoch 380/400\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.1010 - root_mean_squared_error: 0.1509\n",
      "Epoch 381/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.1013 - root_mean_squared_error: 0.1509\n",
      "Epoch 382/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.1010 - root_mean_squared_error: 0.1508\n",
      "Epoch 383/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.1012 - root_mean_squared_error: 0.1508\n",
      "Epoch 384/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.1009 - root_mean_squared_error: 0.1508\n",
      "Epoch 385/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.1012 - root_mean_squared_error: 0.1507\n",
      "Epoch 386/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.1008 - root_mean_squared_error: 0.1507\n",
      "Epoch 387/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.1011 - root_mean_squared_error: 0.1507\n",
      "Epoch 388/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.1008 - root_mean_squared_error: 0.1506\n",
      "Epoch 389/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.1011 - root_mean_squared_error: 0.1506\n",
      "Epoch 390/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.1007 - root_mean_squared_error: 0.1506\n",
      "Epoch 391/400\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.1010 - root_mean_squared_error: 0.1505\n",
      "Epoch 392/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.1007 - root_mean_squared_error: 0.1505\n",
      "Epoch 393/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0226 - mse: 0.0226 - mae: 0.1010 - root_mean_squared_error: 0.1505\n",
      "Epoch 394/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0226 - mse: 0.0226 - mae: 0.1006 - root_mean_squared_error: 0.1504\n",
      "Epoch 395/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0226 - mse: 0.0226 - mae: 0.1009 - root_mean_squared_error: 0.1504\n",
      "Epoch 396/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0226 - mse: 0.0226 - mae: 0.1006 - root_mean_squared_error: 0.1504\n",
      "Epoch 397/400\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0226 - mse: 0.0226 - mae: 0.1008 - root_mean_squared_error: 0.1504\n",
      "Epoch 398/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0226 - mse: 0.0226 - mae: 0.1005 - root_mean_squared_error: 0.1503\n",
      "Epoch 399/400\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0226 - mse: 0.0226 - mae: 0.1008 - root_mean_squared_error: 0.1503\n",
      "Epoch 400/400\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0226 - mse: 0.0226 - mae: 0.1004 - root_mean_squared_error: 0.1503\n",
      "computed in 0:00:03.174850 s\n"
     ]
    }
   ],
   "source": [
    "batch_size = len(X_train)\n",
    "batch_input_shape = (batch_size, n_samples, n_features)\n",
    "dense_model_train = get_dense_model(batch_input_shape)\n",
    "print(dense_model_train.output_shape)\n",
    "dense_model_train.summary()\n",
    "start = time.time()\n",
    "dense_model_train.fit(X_train, Y_train, epochs=400, verbose=1, batch_size=batch_size)\n",
    "end = time.time() - start\n",
    "print(f\"computed in {str(str(timedelta(seconds=end)))} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE(t+1)</th>\n",
       "      <th>MSE(t+1)</th>\n",
       "      <th>MAE(t+2)</th>\n",
       "      <th>MSE(t+2)</th>\n",
       "      <th>MAE(t+3)</th>\n",
       "      <th>MSE(t+3)</th>\n",
       "      <th>MAE(t+4)</th>\n",
       "      <th>MSE(t+4)</th>\n",
       "      <th>MAE(t+5)</th>\n",
       "      <th>MSE(t+5)</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE(t+16)</th>\n",
       "      <th>MSE(t+16)</th>\n",
       "      <th>MAE(t+17)</th>\n",
       "      <th>MSE(t+17)</th>\n",
       "      <th>MAE(t+18)</th>\n",
       "      <th>MSE(t+18)</th>\n",
       "      <th>MAE(t+19)</th>\n",
       "      <th>MSE(t+19)</th>\n",
       "      <th>MAE(t+20)</th>\n",
       "      <th>MSE(t+20)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.183613</td>\n",
       "      <td>37.64569</td>\n",
       "      <td>5.518753</td>\n",
       "      <td>67.801497</td>\n",
       "      <td>6.902325</td>\n",
       "      <td>100.78906</td>\n",
       "      <td>6.93938</td>\n",
       "      <td>110.519651</td>\n",
       "      <td>7.441996</td>\n",
       "      <td>125.528128</td>\n",
       "      <td>...</td>\n",
       "      <td>9.955083</td>\n",
       "      <td>260.233671</td>\n",
       "      <td>10.418484</td>\n",
       "      <td>282.095582</td>\n",
       "      <td>10.592799</td>\n",
       "      <td>306.915463</td>\n",
       "      <td>10.677025</td>\n",
       "      <td>304.047939</td>\n",
       "      <td>11.250903</td>\n",
       "      <td>325.497256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MAE(t+1)  MSE(t+1)  MAE(t+2)   MSE(t+2)  MAE(t+3)   MSE(t+3)  MAE(t+4)  \\\n",
       "0  4.183613  37.64569  5.518753  67.801497  6.902325  100.78906   6.93938   \n",
       "\n",
       "     MSE(t+4)  MAE(t+5)    MSE(t+5)  ...  MAE(t+16)   MSE(t+16)  MAE(t+17)  \\\n",
       "0  110.519651  7.441996  125.528128  ...   9.955083  260.233671  10.418484   \n",
       "\n",
       "    MSE(t+17)  MAE(t+18)   MSE(t+18)  MAE(t+19)   MSE(t+19)  MAE(t+20)  \\\n",
       "0  282.095582  10.592799  306.915463  10.677025  304.047939  11.250903   \n",
       "\n",
       "    MSE(t+20)  \n",
       "0  325.497256  \n",
       "\n",
       "[1 rows x 40 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = len(X_test)\n",
    "dense_model_pred = get_dense_model(batch_input_shape=(batch_size, n_samples, n_features) )\n",
    "dense_model_pred.set_weights(dense_model_train.get_weights())\n",
    "\n",
    "Y_pred = np.squeeze(dense_model_pred.predict(X_test, batch_size=batch_size))\n",
    "Y_pred_unscaled = dg.inverse_transform_y(Y_pred, idx=test_idx, geo=dg.loc_init)\n",
    "Y_pred_real = dg.remove_padded_y(Y_pred_unscaled, idx=test_idx, geo=dg.loc_init)\n",
    "\n",
    "compute_error(Y_pred_real, Y_test_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk forward analysis\n",
    "Several windows are generated and the error is computed over time on each of those windows\n",
    "At each iteration, the previous test set is added to the new training set and another test set is created\n",
    "\n",
    "Exemple:\n",
    "1.  - train:  [0, 1, 2, 3, 4, 5]\n",
    "    - test: [6, 7]\n",
    "2.  - train:  [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "    - test: [8, 9]\n",
    "\n",
    "and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationLogger(ProgbarLogger):\n",
    "    \"\"\"\n",
    "    compute metrics on the validation set, using a different batch size than the training set\n",
    "    at the end of each epoch, the weights of the training model are used to set the validation model\n",
    "    the error is computed based on the unscaled and unpadded values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, validation_model, val_batch_size, X_val, Y_val, val_idx, val_geo):\n",
    "        \"\"\"\n",
    "        :param validation_model: validation model ready to be used\n",
    "        :param X_val: X validation set\n",
    "        :param Y_val: Y validation set, already unscaled and unpadded\n",
    "        :param val_idx: validation idx\n",
    "        :param val_geo: validation geoloc\n",
    "        \"\"\"\n",
    "        super(ValidationLogger, self).__init__(count_mode='steps')\n",
    "        self.validation_model = validation_model\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.X_val = X_val\n",
    "        self.Y_val = Y_val\n",
    "        self.val_idx = val_idx\n",
    "        self.val_geo = val_geo\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        at the end of each epoch, compute the metrics asked on the validation set\n",
    "        \"\"\"\n",
    "        self.validation_model.set_weights(self.model.get_weights())\n",
    "        Y_pred = self.validation_model.predict(self.X_val, batch_size=self.val_batch_size)\n",
    "        Y_pred = dg.inverse_transform_y(Y_pred, geo=self.val_geo, idx=self.val_idx)\n",
    "        Y_pred = dg.remove_padded_y(Y_pred, geo=self.val_geo, idx=self.val_idx)\n",
    "        for metric in validation_metrics:\n",
    "            metric.reset_states()\n",
    "            metric.update_state(Y_pred, self.Y_val)\n",
    "            logs[f\"val_{metric.name}\"] = metric.result().numpy()  # prepend name for validation set\n",
    "        super(ValidationLogger, self).on_epoch_end(epoch, logs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_evaluation_untrainable(model_generator: callable, nb_fit_first: int, nb_validation: int, \n",
    "                                        nb_test: int, verbose: int = 1, *args, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    walk forward evaluation for a model that must not be trained (such as the repeat last model)\n",
    "    \"\"\"\n",
    "    # initial index used\n",
    "    max_len = dg.batch_size\n",
    "    train_idx = np.arange(nb_fit_first)\n",
    "    valid_idx = np.arange(nb_fit_first, nb_fit_first + nb_validation)  # validation set is simply ignored\n",
    "    if nb_fit_first + nb_validation >= max_len:\n",
    "        finished = True  # no test can be done\n",
    "    else:\n",
    "        finished = False  # a test set can be created\n",
    "        test_idx = np.arange(nb_fit_first + nb_validation, min(nb_fit_first + nb_validation + nb_test, max_len))\n",
    "    df_error = pd.DataFrame()\n",
    "    walk = 0\n",
    "    last_iter = False  # True when the last iteration is reached\n",
    "    while not finished:\n",
    "        X_train = dg.get_x(train_idx)  # set the scalers\n",
    "        Y_train = dg.get_y(train_idx)\n",
    "        X_test = dg.get_x(test_idx, geo=dg.loc_init, use_previous_scaler=True)\n",
    "        Y_test_real = dg.get_y(test_idx, geo=dg.loc_init, scaled=False)\n",
    "        Y_test_real = dg.remove_padded_y(Y_test_real, geo=dg.loc_init, idx=test_idx)\n",
    "        \n",
    "        batch_size = len(X_test)\n",
    "        model_pred = model_generator(batch_input_shape=(batch_size, n_samples, n_features))\n",
    "        Y_predicted = model_pred.predict(X_test, batch_size=batch_size)\n",
    "        Y_predicted_real = dg.inverse_transform_y(Y_predicted, geo=dg.loc_init, idx=test_idx)\n",
    "        Y_predicted_real = dg.remove_padded_y(Y_predicted_real, geo=dg.loc_init, idx=test_idx)\n",
    "        \n",
    "        \n",
    "        # compute the error using the unpadded and unscaled data\n",
    "        error = compute_error(Y_test_real, Y_predicted_real)\n",
    "        name = f'walk {walk + 1}'\n",
    "        error['walk'] = name\n",
    "        error['nb_test_datapoints'] = len(Y_test_real)\n",
    "        error['days_train'] = len(train_idx)\n",
    "        error['days_valid'] = len(valid_idx)\n",
    "        error['days_test'] = len(test_idx)\n",
    "        error = error.set_index('walk')\n",
    "        df_error = df_error.append(error)\n",
    "\n",
    "        if verbose != 0:\n",
    "            display(df_error)\n",
    "        if last_iter:\n",
    "            finished = True\n",
    "        # indexes for next fit\n",
    "        train_idx = np.arange(train_idx[-1] + 1 + nb_test)\n",
    "        valid_idx += nb_test\n",
    "        if test_idx[-1] + nb_test >= max_len:  # last iteration, less points can be used for the test set\n",
    "            last_iter = True  # last iteration to be done\n",
    "            test_idx = np.arange(test_idx[-1], max_len)\n",
    "        else:\n",
    "            test_idx += nb_test\n",
    "        walk += 1\n",
    "    df_error.loc['mean'] = df_error.mean()\n",
    "    if verbose != 0:\n",
    "        display(df_error)\n",
    "    return df_error\n",
    "\n",
    "\n",
    "def walk_forward_evaluation(model_generator: callable, nb_fit_first: int, nb_validation: int, nb_test: int, \n",
    "                            epochs: int = 400, plot=False, verbose: int = 1,\n",
    "                            return_history: bool = False, batch_size_fun: callable = None,\n",
    "                            es_stop_val: bool = False, patience : int = 25, *args, **kwargs\n",
    "                           ) -> Union[pd.DataFrame, List[History]]:\n",
    "    \"\"\"\n",
    "    evaluates a model using a walk forward evaluation: multiples fit are done, each followed by at most nb_test\n",
    "    to evaluate the model\n",
    "    :param model_generator: function returning the model to evaluate\n",
    "    :param nb_fit_first: number of datapoints used for the first fit\n",
    "    :param nb_validation: number of datapoints used for each evaluation set\n",
    "    :param nb_test: number of datapoints used for the test set (at most)\n",
    "    :param epochs: number of epochs used on each fit\n",
    "    :param verbose: verbose level. Passed to fit and used to display the error dataframe\n",
    "    :param return_history: if True, returns the list of history of each walk\n",
    "    :param batch_size_fun: function used to compute the batch size based on the X_train tensor\n",
    "        if not specified, default to batch_size = len(train_idx)\n",
    "    :param es_stop_val: whether to do an early stop based on the validation set or not\n",
    "    :param patience: patience to use when using an early stop\n",
    "    \"\"\"\n",
    "    # initial index used\n",
    "    max_len = dg.batch_size\n",
    "    train_idx = np.arange(nb_fit_first)\n",
    "    valid_idx = np.arange(nb_fit_first, nb_fit_first + nb_validation)\n",
    "    if nb_fit_first + nb_validation >= max_len:\n",
    "        finished = True  # no test can be done\n",
    "    else:\n",
    "        finished = False  # a test set can be created\n",
    "        test_idx = np.arange(nb_fit_first + nb_validation, min(nb_fit_first + nb_validation + nb_test, max_len))\n",
    "    df_error = pd.DataFrame()\n",
    "    walk = 0\n",
    "    last_iter = False  # True when the last iteration is reached\n",
    "    all_history = []\n",
    "    \n",
    "    while not finished:\n",
    "        X_train = dg.get_x(train_idx)\n",
    "        Y_train = dg.get_y(train_idx)\n",
    "        if batch_size_fun is None:\n",
    "            batch_size = len(X_train)\n",
    "        else:\n",
    "            batch_size = batch_size_fun(X_train)\n",
    "            \n",
    "        if len(valid_idx) > 0:\n",
    "            X_val = dg.get_x(valid_idx, geo=dg.loc_init, use_previous_scaler=True)\n",
    "            Y_val_real = dg.get_y(valid_idx, geo=dg.loc_init, scaled=False)\n",
    "            Y_val_real = dg.remove_padded_y(Y_val_real, geo=dg.loc_init, idx=valid_idx)\n",
    "            batch_size_val = len(X_val)\n",
    "            model_validation = model_generator(batch_input_shape=(batch_size_val, n_samples, n_features))\n",
    "            val_log = ValidationLogger(model_validation, len(X_val), X_val, Y_val_real, valid_idx, dg.loc_init)\n",
    "            if es_stop_val:\n",
    "                callbacks = [val_log, EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=patience)]\n",
    "            else:\n",
    "                callbacks = [val_log]\n",
    "        else:\n",
    "            callbacks = None\n",
    "            \n",
    "        model_train = model_generator(batch_input_shape=(batch_size, n_samples, n_features))\n",
    "\n",
    "        history = model_train.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=verbose)\n",
    "        \n",
    "        # test only on the unaugmented data without padding\n",
    "        batch_size = len(test_idx)\n",
    "        # construct a new model and use the previous weights to set it\n",
    "        model_pred = model_generator(batch_input_shape=(batch_size, n_samples, n_features))\n",
    "        model_pred.set_weights(model_train.get_weights())\n",
    "        X_test = dg.get_x(test_idx, geo=dg.loc_init, use_previous_scaler=True)\n",
    "        Y_test = dg.get_y(test_idx, geo=dg.loc_init, use_previous_scaler=True)\n",
    "        Y_test_real = dg.get_y(test_idx, geo=dg.loc_init, scaled=False)\n",
    "        Y_test_real = dg.remove_padded_y(Y_test_real, geo=dg.loc_init, idx=test_idx)\n",
    "        Y_predicted = model_pred.predict(X_test, batch_size=batch_size)\n",
    "        Y_predicted_real = dg.inverse_transform_y(Y_predicted, geo=dg.loc_init, idx=test_idx)\n",
    "        Y_predicted_real = dg.remove_padded_y(Y_predicted_real, geo=dg.loc_init, idx=test_idx)        \n",
    "        if return_history:\n",
    "            # add test metrics to the history for this walk, based on the unpadded unscaled data \n",
    "            # except for the loss\n",
    "            for metric in model_train.metrics:\n",
    "                metric.reset_states()\n",
    "                if metric.name == 'loss':\n",
    "                    metric.update_state(Y_predicted_real, Y_test_real)\n",
    "                else:\n",
    "                    # compute metric accross each horizon\n",
    "                    for i in range(n_forecast):\n",
    "                        metric.update_state(Y_predicted_real[:, i], Y_test_real[:, i])\n",
    "                        history.history[f\"test_{metric.name}(t+{i+1})\"] = [metric.result().numpy()]\n",
    "                        metric.reset_states()\n",
    "                    # compute mean of metric on all horizon\n",
    "                    metric.update_state(Y_predicted_real, Y_test_real)\n",
    "                # prepend name for test set\n",
    "                history.history[f\"test_{metric.name}\"] = [metric.result().numpy()]\n",
    "            # add number of unpadded datapoints\n",
    "            history.history['nb_test_datapoints'] = [len(Y_test_real)]\n",
    "            all_history.append(history)\n",
    "        if not return_history or verbose != 0:\n",
    "            # compute the error using the unpadded and unscaled data\n",
    "            error = compute_error(Y_test_real, Y_predicted_real)\n",
    "            name = f'walk {walk + 1}'\n",
    "            error['walk'] = name\n",
    "            error['nb_test_datapoints'] = len(Y_test_real)\n",
    "            error['days_train'] = len(train_idx)\n",
    "            error['days_valid'] = len(valid_idx)\n",
    "            error['days_test'] = len(test_idx)\n",
    "            error['epochs'] = len(history.history['loss'])\n",
    "            error = error.set_index('walk')\n",
    "            df_error = df_error.append(error)\n",
    "        if verbose != 0:\n",
    "            display(df_error)\n",
    "        if last_iter:\n",
    "            finished = True\n",
    "        # indexes for next fit\n",
    "        train_idx = np.arange(train_idx[-1] + 1 + nb_test)\n",
    "        valid_idx += nb_test\n",
    "        if test_idx[-1] + nb_test >= max_len:  # last iteration, less points can be used for the test set\n",
    "            last_iter = True  # last iteration to be done\n",
    "            test_idx = np.arange(test_idx[-1], max_len)\n",
    "        else:\n",
    "            test_idx += nb_test\n",
    "        walk += 1\n",
    "    if not return_history or verbose != 0:\n",
    "        df_error.loc['mean'] = df_error.mean()\n",
    "        if verbose != 0:\n",
    "            display(df_error)\n",
    "        \n",
    "    return df_error if not return_history else all_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_history_dense = walk_forward_evaluation(get_dense_model, 230, 30, 30, epochs=10000, verbose=0, \n",
    "                                       es_stop_val=False, patience=250, return_history=True)\n",
    "\n",
    "patience = 200\n",
    "for i in range(len(list_history_dense)):\n",
    "    val_metric = list_history_dense[i].history['val_mean_absolute_error']\n",
    "    best_found = np.inf\n",
    "    best_found_idx = -1\n",
    "    count = 0\n",
    "    for j in range(len(val_metric)):  # tries to find the minimum of val_mae\n",
    "        if val_metric[j] <= best_found:\n",
    "            best_found = val_metric[j]\n",
    "            best_found_idx = j\n",
    "            count = 0\n",
    "        else:\n",
    "            count += 1\n",
    "            if count == patience:\n",
    "                break\n",
    "    plt.figure()\n",
    "    plt.plot(val_metric)\n",
    "    plt.plot(best_found_idx, best_found, 'o')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_history_simple = walk_forward_evaluation(get_simple_autoencoder, 230, 30, 30, epochs=10000, verbose=0, \n",
    "                                       es_stop_val=False, patience=200, return_history=True)\n",
    "\n",
    "patience = 200\n",
    "for i in range(len(list_history_simple)):\n",
    "    val_metric = list_history_simple[i].history['val_mean_absolute_error']\n",
    "    best_found = np.inf\n",
    "    best_found_idx = -1\n",
    "    count = 0\n",
    "    for j in range(len(val_metric)):  # tries to find the minimum of val_mae\n",
    "        if val_metric[j] <= best_found:\n",
    "            best_found = val_metric[j]\n",
    "            best_found_idx = j\n",
    "            count = 0\n",
    "        else:\n",
    "            count += 1\n",
    "            if count == patience:\n",
    "                break\n",
    "    plt.figure()\n",
    "    plt.plot(val_metric)\n",
    "    plt.plot(best_found_idx, best_found, 'o')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_history = walk_forward_evaluation(get_encoder_decoder, 230, 30, 30, epochs=10000, verbose=1, \n",
    "                                       es_stop_val=False, return_history=True)\n",
    "\n",
    "patience = 1100\n",
    "for i in range(len(list_history)):\n",
    "    val_metric = list_history[i].history['val_mean_absolute_error']\n",
    "    best_found = np.inf\n",
    "    best_found_idx = -1\n",
    "    count = 0\n",
    "    for j in range(len(val_metric)):  # tries to find the minimum of val_mae\n",
    "        if val_metric[j] <= best_found:\n",
    "            best_found = val_metric[j]\n",
    "            best_found_idx = j\n",
    "            count = 0\n",
    "        else:\n",
    "            count += 1\n",
    "            if count == patience:\n",
    "                break\n",
    "    plt.figure()\n",
    "    plt.plot(val_metric)\n",
    "    plt.plot(best_found_idx, best_found, 'o')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE(t+1)</th>\n",
       "      <th>MSE(t+1)</th>\n",
       "      <th>MAE(t+2)</th>\n",
       "      <th>MSE(t+2)</th>\n",
       "      <th>MAE(t+3)</th>\n",
       "      <th>MSE(t+3)</th>\n",
       "      <th>MAE(t+4)</th>\n",
       "      <th>MSE(t+4)</th>\n",
       "      <th>MAE(t+5)</th>\n",
       "      <th>MSE(t+5)</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE(t+18)</th>\n",
       "      <th>MSE(t+18)</th>\n",
       "      <th>MAE(t+19)</th>\n",
       "      <th>MSE(t+19)</th>\n",
       "      <th>MAE(t+20)</th>\n",
       "      <th>MSE(t+20)</th>\n",
       "      <th>nb_test_datapoints</th>\n",
       "      <th>days_train</th>\n",
       "      <th>days_valid</th>\n",
       "      <th>days_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>walk 1</th>\n",
       "      <td>561.547367</td>\n",
       "      <td>1.037205e+06</td>\n",
       "      <td>577.891881</td>\n",
       "      <td>1.082053e+06</td>\n",
       "      <td>593.580700</td>\n",
       "      <td>1.126843e+06</td>\n",
       "      <td>608.588154</td>\n",
       "      <td>1.171340e+06</td>\n",
       "      <td>622.847988</td>\n",
       "      <td>1.215259e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>740.683599</td>\n",
       "      <td>1.673787e+06</td>\n",
       "      <td>745.841777</td>\n",
       "      <td>1.698870e+06</td>\n",
       "      <td>750.777180</td>\n",
       "      <td>1.722817e+06</td>\n",
       "      <td>690.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2</th>\n",
       "      <td>7.972295</td>\n",
       "      <td>1.186103e+02</td>\n",
       "      <td>14.404034</td>\n",
       "      <td>4.069617e+02</td>\n",
       "      <td>20.986539</td>\n",
       "      <td>8.755714e+02</td>\n",
       "      <td>27.499822</td>\n",
       "      <td>1.517426e+03</td>\n",
       "      <td>33.966309</td>\n",
       "      <td>2.331104e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>115.886719</td>\n",
       "      <td>2.726877e+04</td>\n",
       "      <td>122.193231</td>\n",
       "      <td>3.018695e+04</td>\n",
       "      <td>128.221554</td>\n",
       "      <td>3.310810e+04</td>\n",
       "      <td>690.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3</th>\n",
       "      <td>10.362622</td>\n",
       "      <td>2.305871e+02</td>\n",
       "      <td>19.092851</td>\n",
       "      <td>7.399994e+02</td>\n",
       "      <td>27.493887</td>\n",
       "      <td>1.539609e+03</td>\n",
       "      <td>35.536951</td>\n",
       "      <td>2.580633e+03</td>\n",
       "      <td>43.176288</td>\n",
       "      <td>3.819086e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>115.569247</td>\n",
       "      <td>2.269704e+04</td>\n",
       "      <td>119.501753</td>\n",
       "      <td>2.374068e+04</td>\n",
       "      <td>123.139227</td>\n",
       "      <td>2.474908e+04</td>\n",
       "      <td>690.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4</th>\n",
       "      <td>11.059496</td>\n",
       "      <td>3.588601e+02</td>\n",
       "      <td>18.242590</td>\n",
       "      <td>8.855247e+02</td>\n",
       "      <td>25.567225</td>\n",
       "      <td>1.805116e+03</td>\n",
       "      <td>33.015534</td>\n",
       "      <td>3.165562e+03</td>\n",
       "      <td>40.623195</td>\n",
       "      <td>5.033598e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>149.231835</td>\n",
       "      <td>8.869694e+04</td>\n",
       "      <td>158.084351</td>\n",
       "      <td>1.001841e+05</td>\n",
       "      <td>166.897494</td>\n",
       "      <td>1.123516e+05</td>\n",
       "      <td>690.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5</th>\n",
       "      <td>12.225772</td>\n",
       "      <td>6.050853e+02</td>\n",
       "      <td>25.107330</td>\n",
       "      <td>2.605471e+03</td>\n",
       "      <td>38.720680</td>\n",
       "      <td>6.090567e+03</td>\n",
       "      <td>52.569547</td>\n",
       "      <td>1.116085e+04</td>\n",
       "      <td>66.672031</td>\n",
       "      <td>1.781318e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>225.758257</td>\n",
       "      <td>1.766667e+05</td>\n",
       "      <td>234.885587</td>\n",
       "      <td>1.889199e+05</td>\n",
       "      <td>242.584344</td>\n",
       "      <td>1.998684e+05</td>\n",
       "      <td>92.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>120.633510</td>\n",
       "      <td>2.077036e+05</td>\n",
       "      <td>130.947737</td>\n",
       "      <td>2.173382e+05</td>\n",
       "      <td>141.269806</td>\n",
       "      <td>2.274308e+05</td>\n",
       "      <td>151.442001</td>\n",
       "      <td>2.379530e+05</td>\n",
       "      <td>161.457162</td>\n",
       "      <td>2.488513e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>269.425932</td>\n",
       "      <td>3.978233e+05</td>\n",
       "      <td>276.101340</td>\n",
       "      <td>4.083803e+05</td>\n",
       "      <td>282.323960</td>\n",
       "      <td>4.185788e+05</td>\n",
       "      <td>570.4</td>\n",
       "      <td>290.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          MAE(t+1)      MSE(t+1)    MAE(t+2)      MSE(t+2)    MAE(t+3)  \\\n",
       "walk                                                                     \n",
       "walk 1  561.547367  1.037205e+06  577.891881  1.082053e+06  593.580700   \n",
       "walk 2    7.972295  1.186103e+02   14.404034  4.069617e+02   20.986539   \n",
       "walk 3   10.362622  2.305871e+02   19.092851  7.399994e+02   27.493887   \n",
       "walk 4   11.059496  3.588601e+02   18.242590  8.855247e+02   25.567225   \n",
       "walk 5   12.225772  6.050853e+02   25.107330  2.605471e+03   38.720680   \n",
       "mean    120.633510  2.077036e+05  130.947737  2.173382e+05  141.269806   \n",
       "\n",
       "            MSE(t+3)    MAE(t+4)      MSE(t+4)    MAE(t+5)      MSE(t+5)  ...  \\\n",
       "walk                                                                      ...   \n",
       "walk 1  1.126843e+06  608.588154  1.171340e+06  622.847988  1.215259e+06  ...   \n",
       "walk 2  8.755714e+02   27.499822  1.517426e+03   33.966309  2.331104e+03  ...   \n",
       "walk 3  1.539609e+03   35.536951  2.580633e+03   43.176288  3.819086e+03  ...   \n",
       "walk 4  1.805116e+03   33.015534  3.165562e+03   40.623195  5.033598e+03  ...   \n",
       "walk 5  6.090567e+03   52.569547  1.116085e+04   66.672031  1.781318e+04  ...   \n",
       "mean    2.274308e+05  151.442001  2.379530e+05  161.457162  2.488513e+05  ...   \n",
       "\n",
       "         MAE(t+18)     MSE(t+18)   MAE(t+19)     MSE(t+19)   MAE(t+20)  \\\n",
       "walk                                                                     \n",
       "walk 1  740.683599  1.673787e+06  745.841777  1.698870e+06  750.777180   \n",
       "walk 2  115.886719  2.726877e+04  122.193231  3.018695e+04  128.221554   \n",
       "walk 3  115.569247  2.269704e+04  119.501753  2.374068e+04  123.139227   \n",
       "walk 4  149.231835  8.869694e+04  158.084351  1.001841e+05  166.897494   \n",
       "walk 5  225.758257  1.766667e+05  234.885587  1.889199e+05  242.584344   \n",
       "mean    269.425932  3.978233e+05  276.101340  4.083803e+05  282.323960   \n",
       "\n",
       "           MSE(t+20)  nb_test_datapoints  days_train  days_valid  days_test  \n",
       "walk                                                                         \n",
       "walk 1  1.722817e+06               690.0       230.0        30.0       30.0  \n",
       "walk 2  3.310810e+04               690.0       260.0        30.0       30.0  \n",
       "walk 3  2.474908e+04               690.0       290.0        30.0       30.0  \n",
       "walk 4  1.123516e+05               690.0       320.0        30.0       30.0  \n",
       "walk 5  1.998684e+05                92.0       350.0        30.0        4.0  \n",
       "mean    4.185788e+05               570.4       290.0        30.0       24.8  \n",
       "\n",
       "[6 rows x 44 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " walk_forward_evaluation_untrainable(get_baseline, 230, 30, 30, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-666b282c4a5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m scan_object = talos.Scan(\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/talos/scan/Scan.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, params, model, experiment_name, x_val, y_val, val_split, random_method, seed, performance_target, fraction_limit, round_limit, time_limit, boolean_limit, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, minimize_loss, disable_progress_bar, print_params, clear_session, save_weights)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# start runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mscan_run\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mscan_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/talos/scan/scan_run.py\u001b[0m in \u001b[0;36mscan_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# otherwise proceed with next permutation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mscan_round\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/talos/scan/scan_round.py\u001b[0m in \u001b[0;36mscan_round\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mingest_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/talos/model/ingest_model.py\u001b[0m in \u001b[0;36mingest_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m     through Scan() model paramater.'''\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     return self.model(self.x_train,\n\u001b[0m\u001b[1;32m      7\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-666b282c4a5f>\u001b[0m in \u001b[0;36mtalos_walk\u001b[0;34m(x_train, y_train, x_val, y_val, p)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mbatch_size_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_div'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     history_list = walk_forward_evaluation(model_generator, nb_fit_first=230, nb_validation=30, nb_test=30, \n\u001b[0m\u001b[1;32m     47\u001b[0m                                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'patience'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                       batch_size_fun=batch_size_fun, return_history=True, es_stop_val=True)\n",
      "\u001b[0;32m<ipython-input-14-58e3bb4b21da>\u001b[0m in \u001b[0;36mwalk_forward_evaluation\u001b[0;34m(model_generator, nb_fit_first, nb_validation, nb_test, epochs, plot, verbose, return_history, batch_size_fun, es_stop_val, patience, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mmodel_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# test only on the unaugmented data without padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyper_parameter = {\n",
    "    'hidden_1': [4, 8, 16],\n",
    "    'hidden_2': [0, 4, 8, 16],\n",
    "    'lr': [0.001, 0.01],\n",
    "    'activation': ['relu', 'elu'],\n",
    "    'reg': [lambda x: regularizers.l1(l=x), lambda x: None],\n",
    "    'regw': [5e-4, 1e-3],\n",
    "    'optimizer': ['Adam', 'RMSprop'],\n",
    "    'losses': ['mse', 'mae', custom_loss_function],\n",
    "    'scaling': [MinMaxScaler, StandardScaler],\n",
    "    'batch_size_div': [1],\n",
    "    'epochs': [1500],\n",
    "    'patience': [200]\n",
    "}\n",
    "\n",
    "def get_encoder_decoder(batch_input_shape, p):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(p['hidden_1'], return_sequences=(p['hidden_2'] != 0), \n",
    "                   batch_input_shape=batch_input_shape, kernel_regularizer=p['reg'](p['regw'])))\n",
    "    if p['hidden_2'] != 0:\n",
    "        model.add(LSTM(p['hidden_2'], return_sequences=False, kernel_regularizer=p['reg'](p['regw'])))\n",
    "    model.add(RepeatVector(n_forecast))  # repeat\n",
    "    if p['hidden_2'] != 0:\n",
    "        model.add(LSTM(p['hidden_2'], return_sequences=True, kernel_regularizer=p['reg'](p['regw'])))  # dec\n",
    "    if not predict_one:\n",
    "        model.add(LSTM(p['hidden_1'], return_sequences=True, kernel_regularizer=p['reg'](p['regw'])))  # dec\n",
    "        model.add(TimeDistributed(Dense(1, kernel_regularizer=p['reg'](p['regw']), activation=p['activation'])))\n",
    "        model.add(Reshape((n_forecast,)))\n",
    "    else:\n",
    "        model.add(LSTM(p['hidden_1'], return_sequences=False, kernel_regularizer=p['reg'](p['regw'])))  # dec\n",
    "        model.add(Dense(1, kernel_regularizer=p['reg'](p['regw']), activation=p['activation']))\n",
    "        model.add(Reshape((1,)))\n",
    "    model.compile(loss=p[\"losses\"], optimizer=p[\"optimizer\"], metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    K.set_value(model.optimizer.learning_rate, p['lr'])\n",
    "    return model\n",
    "    \n",
    "\n",
    "def talos_walk(x_train, y_train, x_val, y_val, p):\n",
    "    \"\"\"\n",
    "    walk forward evaluation used by talos to determine the best model to keep\n",
    "    \"\"\"\n",
    "    dg.set_scaler(p['scaling'])\n",
    "    model_generator = lambda batch_input_shape: get_encoder_decoder(batch_input_shape, p)\n",
    "\n",
    "    batch_size_fun = lambda x : int(x.shape[0] / p['batch_size_div'])\n",
    "    history_list = walk_forward_evaluation(model_generator, nb_fit_first=230, nb_validation=30, nb_test=30, \n",
    "                                      epochs=p['epochs'], plot=False, verbose=0, patience = p['patience'],\n",
    "                                      batch_size_fun=batch_size_fun, return_history=True, es_stop_val=True)\n",
    "    # compute history to return, based on the list of history\n",
    "    history = History()\n",
    "    # used to trick talos to compute the right amount of round_epochs. needs to be the first entry of the dict\n",
    "    # create an array of len == mean of number of epoch of each history\n",
    "    history.history['ep'] = np.arange(int(np.mean([len(hist.history['loss']) for hist in history_list]))) + 1\n",
    "    for log in history_list[0].history.keys():\n",
    "        history.history[log] = [np.mean([hist.history[log][-1] for hist in history_list])]\n",
    "    return history, model_generator((1, n_samples, n_features))\n",
    "\n",
    "scan_object = talos.Scan(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    x_val=[],\n",
    "    y_val=[],\n",
    "    params=hyper_parameter,\n",
    "    model=talos_walk,\n",
    "    experiment_name='trends1', \n",
    "    fraction_limit=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_object = talos.Analyze(scan_object)\n",
    "print(\"MAE\", analyze_object.low('mae'))\n",
    "print(\"MSE\", analyze_object.low('mse'))\n",
    "print(\"RMSE\", analyze_object.low('root_mean_squared_error'))\n",
    "print(\"TEST MAE\", analyze_object.low('test_mae'))\n",
    "print(\"TEST MSE\", analyze_object.low('test_mse'))\n",
    "print(\"TEST RMSE\", analyze_object.low('test_root_mean_squared_error'))\n",
    "df = analyze_object.table('test_mae(t+20)', ascending=True)\n",
    "print(df.columns)\n",
    "columns = [f\"test_mae(t+{i})\" for i in range(1, n_forecast+1)]\n",
    "df[['hidden_1', 'hidden_2', 'lr', 'batch_size_div', 'scaling', 'test_mse', 'test_mae'] + columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0][hyper_parameter.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 estimators combined\n",
    "The first estimator should be simple and output an estimate $ \\hat{y}_{(1)}(t) $ of $ y(t) $\n",
    "The correction factor ($ C(t) $) between the estimation and the real value is then computed:\n",
    "\n",
    "$$ y(t) = \\hat{y}_{(1)}(t) * ( 1 + C(t)) $$\n",
    "\n",
    "$$ C(t) = \\frac{y(t) - \\hat{y}_{(1)}(t)}{\\hat{y}_{(1)}(t)} $$\n",
    "\n",
    "This formula is used such that $C(t)$ should be bounded in [-1, 1] if the target is a positive value and the first estimator is relatively closed to the expected value.\n",
    "\n",
    "Anoter estimator is then trained based on the first one. The goal is to be able to predict the correction $ C(t) $ that should be done on the first estimator, in order to reduce the estimation error and get a better prediction. The prediction of $ C(t) $ is written $ \\hat{C}(t) $. Once this estimator is computed, the final prediction can be constructed:\n",
    "\n",
    "$$ \\hat{y}(t) = \\hat{y}_{(1)}(t) * ( 1 + \\hat{C}(t)) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First estimator\n",
    "Construct the predition $\\hat{y}_{(1)}(t)$ and compute $C(t)$ based on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 610ms/step - loss: 0.5932 - mse: 0.5932 - mae: 0.5063 - root_mean_squared_error: 0.7702\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5449 - mse: 0.5449 - mae: 0.4782 - root_mean_squared_error: 0.7382\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5132 - mse: 0.5132 - mae: 0.4598 - root_mean_squared_error: 0.7164\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4884 - mse: 0.4884 - mae: 0.4455 - root_mean_squared_error: 0.6988\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4675 - mse: 0.4675 - mae: 0.4335 - root_mean_squared_error: 0.6837\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4491 - mse: 0.4491 - mae: 0.4230 - root_mean_squared_error: 0.6702\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4327 - mse: 0.4327 - mae: 0.4135 - root_mean_squared_error: 0.6578\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4177 - mse: 0.4177 - mae: 0.4049 - root_mean_squared_error: 0.6463\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4038 - mse: 0.4038 - mae: 0.3969 - root_mean_squared_error: 0.6355\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3909 - mse: 0.3909 - mae: 0.3894 - root_mean_squared_error: 0.6252\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3788 - mse: 0.3788 - mae: 0.3823 - root_mean_squared_error: 0.6154\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3673 - mse: 0.3673 - mae: 0.3756 - root_mean_squared_error: 0.6061\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3564 - mse: 0.3564 - mae: 0.3692 - root_mean_squared_error: 0.5970\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3460 - mse: 0.3460 - mae: 0.3630 - root_mean_squared_error: 0.5882\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3361 - mse: 0.3361 - mae: 0.3570 - root_mean_squared_error: 0.5797\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3266 - mse: 0.3266 - mae: 0.3513 - root_mean_squared_error: 0.5715\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3174 - mse: 0.3174 - mae: 0.3457 - root_mean_squared_error: 0.5634\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3086 - mse: 0.3086 - mae: 0.3403 - root_mean_squared_error: 0.5555\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3001 - mse: 0.3001 - mae: 0.3351 - root_mean_squared_error: 0.5479\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2919 - mse: 0.2919 - mae: 0.3300 - root_mean_squared_error: 0.5403\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2840 - mse: 0.2840 - mae: 0.3251 - root_mean_squared_error: 0.5329\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2763 - mse: 0.2763 - mae: 0.3202 - root_mean_squared_error: 0.5257\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2689 - mse: 0.2689 - mae: 0.3154 - root_mean_squared_error: 0.5185\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2617 - mse: 0.2617 - mae: 0.3107 - root_mean_squared_error: 0.5115\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2547 - mse: 0.2547 - mae: 0.3062 - root_mean_squared_error: 0.5046\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2478 - mse: 0.2478 - mae: 0.3016 - root_mean_squared_error: 0.4978\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2412 - mse: 0.2412 - mae: 0.2972 - root_mean_squared_error: 0.4912\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2348 - mse: 0.2348 - mae: 0.2928 - root_mean_squared_error: 0.4846\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2286 - mse: 0.2286 - mae: 0.2885 - root_mean_squared_error: 0.4781\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2225 - mse: 0.2225 - mae: 0.2843 - root_mean_squared_error: 0.4717\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2166 - mse: 0.2166 - mae: 0.2802 - root_mean_squared_error: 0.4654\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2109 - mse: 0.2109 - mae: 0.2761 - root_mean_squared_error: 0.4592\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2053 - mse: 0.2053 - mae: 0.2720 - root_mean_squared_error: 0.4531\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1998 - mse: 0.1998 - mae: 0.2681 - root_mean_squared_error: 0.4470\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1946 - mse: 0.1946 - mae: 0.2642 - root_mean_squared_error: 0.4411\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1894 - mse: 0.1894 - mae: 0.2603 - root_mean_squared_error: 0.4352\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1844 - mse: 0.1844 - mae: 0.2566 - root_mean_squared_error: 0.4294\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1795 - mse: 0.1795 - mae: 0.2529 - root_mean_squared_error: 0.4237\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1748 - mse: 0.1748 - mae: 0.2492 - root_mean_squared_error: 0.4181\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1702 - mse: 0.1702 - mae: 0.2457 - root_mean_squared_error: 0.4126\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1657 - mse: 0.1657 - mae: 0.2422 - root_mean_squared_error: 0.4071\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1614 - mse: 0.1614 - mae: 0.2388 - root_mean_squared_error: 0.4017\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1572 - mse: 0.1572 - mae: 0.2354 - root_mean_squared_error: 0.3964\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1531 - mse: 0.1531 - mae: 0.2321 - root_mean_squared_error: 0.3912\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1491 - mse: 0.1491 - mae: 0.2289 - root_mean_squared_error: 0.3861\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1452 - mse: 0.1452 - mae: 0.2258 - root_mean_squared_error: 0.3810\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1414 - mse: 0.1414 - mae: 0.2227 - root_mean_squared_error: 0.3761\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1378 - mse: 0.1378 - mae: 0.2198 - root_mean_squared_error: 0.3712\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1342 - mse: 0.1342 - mae: 0.2169 - root_mean_squared_error: 0.3664\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1308 - mse: 0.1308 - mae: 0.2141 - root_mean_squared_error: 0.3616\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1274 - mse: 0.1274 - mae: 0.2113 - root_mean_squared_error: 0.3570\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1242 - mse: 0.1242 - mae: 0.2086 - root_mean_squared_error: 0.3524\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1210 - mse: 0.1210 - mae: 0.2060 - root_mean_squared_error: 0.3479\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1180 - mse: 0.1180 - mae: 0.2035 - root_mean_squared_error: 0.3435\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1150 - mse: 0.1150 - mae: 0.2010 - root_mean_squared_error: 0.3392\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1122 - mse: 0.1122 - mae: 0.1986 - root_mean_squared_error: 0.3349\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1094 - mse: 0.1094 - mae: 0.1962 - root_mean_squared_error: 0.3308\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1067 - mse: 0.1067 - mae: 0.1939 - root_mean_squared_error: 0.3267\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1041 - mse: 0.1041 - mae: 0.1917 - root_mean_squared_error: 0.3227\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1016 - mse: 0.1016 - mae: 0.1895 - root_mean_squared_error: 0.3187\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0992 - mse: 0.0992 - mae: 0.1874 - root_mean_squared_error: 0.3149\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0968 - mse: 0.0968 - mae: 0.1853 - root_mean_squared_error: 0.3111\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0945 - mse: 0.0945 - mae: 0.1833 - root_mean_squared_error: 0.3074\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0923 - mse: 0.0923 - mae: 0.1815 - root_mean_squared_error: 0.3038\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0902 - mse: 0.0902 - mae: 0.1796 - root_mean_squared_error: 0.3003\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0881 - mse: 0.0881 - mae: 0.1778 - root_mean_squared_error: 0.2968\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0861 - mse: 0.0861 - mae: 0.1759 - root_mean_squared_error: 0.2934\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0841 - mse: 0.0841 - mae: 0.1742 - root_mean_squared_error: 0.2900\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0822 - mse: 0.0822 - mae: 0.1725 - root_mean_squared_error: 0.2868\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0804 - mse: 0.0804 - mae: 0.1708 - root_mean_squared_error: 0.2836\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0787 - mse: 0.0787 - mae: 0.1692 - root_mean_squared_error: 0.2804\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0769 - mse: 0.0769 - mae: 0.1677 - root_mean_squared_error: 0.2774\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0753 - mse: 0.0753 - mae: 0.1662 - root_mean_squared_error: 0.2744\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0737 - mse: 0.0737 - mae: 0.1647 - root_mean_squared_error: 0.2715\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0721 - mse: 0.0721 - mae: 0.1632 - root_mean_squared_error: 0.2686\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0706 - mse: 0.0706 - mae: 0.1618 - root_mean_squared_error: 0.2658\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0692 - mse: 0.0692 - mae: 0.1604 - root_mean_squared_error: 0.2630\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0678 - mse: 0.0678 - mae: 0.1591 - root_mean_squared_error: 0.2603\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0664 - mse: 0.0664 - mae: 0.1578 - root_mean_squared_error: 0.2577\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0651 - mse: 0.0651 - mae: 0.1566 - root_mean_squared_error: 0.2551\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0638 - mse: 0.0638 - mae: 0.1552 - root_mean_squared_error: 0.2526\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0626 - mse: 0.0626 - mae: 0.1543 - root_mean_squared_error: 0.2501\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0614 - mse: 0.0614 - mae: 0.1528 - root_mean_squared_error: 0.2477\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0602 - mse: 0.0602 - mae: 0.1519 - root_mean_squared_error: 0.2453\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0591 - mse: 0.0591 - mae: 0.1506 - root_mean_squared_error: 0.2430\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0580 - mse: 0.0580 - mae: 0.1497 - root_mean_squared_error: 0.2408\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0569 - mse: 0.0569 - mae: 0.1485 - root_mean_squared_error: 0.2386\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0559 - mse: 0.0559 - mae: 0.1476 - root_mean_squared_error: 0.2364\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0549 - mse: 0.0549 - mae: 0.1465 - root_mean_squared_error: 0.2343\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0539 - mse: 0.0539 - mae: 0.1456 - root_mean_squared_error: 0.2323\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0530 - mse: 0.0530 - mae: 0.1445 - root_mean_squared_error: 0.2303\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0521 - mse: 0.0521 - mae: 0.1437 - root_mean_squared_error: 0.2283\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0513 - mse: 0.0513 - mae: 0.1426 - root_mean_squared_error: 0.2264\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0504 - mse: 0.0504 - mae: 0.1420 - root_mean_squared_error: 0.2246\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0496 - mse: 0.0496 - mae: 0.1408 - root_mean_squared_error: 0.2228\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0489 - mse: 0.0489 - mae: 0.1406 - root_mean_squared_error: 0.2211\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0481 - mse: 0.0481 - mae: 0.1390 - root_mean_squared_error: 0.2193\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0474 - mse: 0.0474 - mae: 0.1387 - root_mean_squared_error: 0.2177\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0467 - mse: 0.0467 - mae: 0.1374 - root_mean_squared_error: 0.2160\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0460 - mse: 0.0460 - mae: 0.1369 - root_mean_squared_error: 0.2145\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0453 - mse: 0.0453 - mae: 0.1358 - root_mean_squared_error: 0.2129\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0447 - mse: 0.0447 - mae: 0.1355 - root_mean_squared_error: 0.2115\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0441 - mse: 0.0441 - mae: 0.1343 - root_mean_squared_error: 0.2100\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0435 - mse: 0.0435 - mae: 0.1340 - root_mean_squared_error: 0.2086\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0430 - mse: 0.0430 - mae: 0.1328 - root_mean_squared_error: 0.2073\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0424 - mse: 0.0424 - mae: 0.1325 - root_mean_squared_error: 0.2060\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0419 - mse: 0.0419 - mae: 0.1315 - root_mean_squared_error: 0.2047\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0414 - mse: 0.0414 - mae: 0.1311 - root_mean_squared_error: 0.2034\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0409 - mse: 0.0409 - mae: 0.1302 - root_mean_squared_error: 0.2022\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0404 - mse: 0.0404 - mae: 0.1299 - root_mean_squared_error: 0.2011\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0400 - mse: 0.0400 - mae: 0.1290 - root_mean_squared_error: 0.1999\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0395 - mse: 0.0395 - mae: 0.1287 - root_mean_squared_error: 0.1988\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0391 - mse: 0.0391 - mae: 0.1278 - root_mean_squared_error: 0.1977\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0387 - mse: 0.0387 - mae: 0.1276 - root_mean_squared_error: 0.1966\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0383 - mse: 0.0383 - mae: 0.1266 - root_mean_squared_error: 0.1956\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0379 - mse: 0.0379 - mae: 0.1264 - root_mean_squared_error: 0.1946\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0375 - mse: 0.0375 - mae: 0.1255 - root_mean_squared_error: 0.1936\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0371 - mse: 0.0371 - mae: 0.1254 - root_mean_squared_error: 0.1926\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0367 - mse: 0.0367 - mae: 0.1245 - root_mean_squared_error: 0.1916\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0364 - mse: 0.0364 - mae: 0.1245 - root_mean_squared_error: 0.1907\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0360 - mse: 0.0360 - mae: 0.1235 - root_mean_squared_error: 0.1898\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0357 - mse: 0.0357 - mae: 0.1237 - root_mean_squared_error: 0.1889\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0354 - mse: 0.0354 - mae: 0.1225 - root_mean_squared_error: 0.1880\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0350 - mse: 0.0350 - mae: 0.1228 - root_mean_squared_error: 0.1872\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0347 - mse: 0.0347 - mae: 0.1217 - root_mean_squared_error: 0.1863\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0344 - mse: 0.0344 - mae: 0.1220 - root_mean_squared_error: 0.1855\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0341 - mse: 0.0341 - mae: 0.1210 - root_mean_squared_error: 0.1847\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0338 - mse: 0.0338 - mae: 0.1211 - root_mean_squared_error: 0.1838\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0335 - mse: 0.0335 - mae: 0.1203 - root_mean_squared_error: 0.1830\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0332 - mse: 0.0332 - mae: 0.1203 - root_mean_squared_error: 0.1822\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0329 - mse: 0.0329 - mae: 0.1196 - root_mean_squared_error: 0.1815\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0327 - mse: 0.0327 - mae: 0.1194 - root_mean_squared_error: 0.1807\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0324 - mse: 0.0324 - mae: 0.1189 - root_mean_squared_error: 0.1800\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0321 - mse: 0.0321 - mae: 0.1188 - root_mean_squared_error: 0.1792\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0318 - mse: 0.0318 - mae: 0.1181 - root_mean_squared_error: 0.1785\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0316 - mse: 0.0316 - mae: 0.1181 - root_mean_squared_error: 0.1777\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0313 - mse: 0.0313 - mae: 0.1173 - root_mean_squared_error: 0.1770\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0311 - mse: 0.0311 - mae: 0.1175 - root_mean_squared_error: 0.1763\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0308 - mse: 0.0308 - mae: 0.1166 - root_mean_squared_error: 0.1756\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0306 - mse: 0.0306 - mae: 0.1168 - root_mean_squared_error: 0.1749\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0303 - mse: 0.0303 - mae: 0.1159 - root_mean_squared_error: 0.1742\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0301 - mse: 0.0301 - mae: 0.1162 - root_mean_squared_error: 0.1735\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0299 - mse: 0.0299 - mae: 0.1151 - root_mean_squared_error: 0.1728\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0296 - mse: 0.0296 - mae: 0.1156 - root_mean_squared_error: 0.1721\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0294 - mse: 0.0294 - mae: 0.1144 - root_mean_squared_error: 0.1715\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0292 - mse: 0.0292 - mae: 0.1148 - root_mean_squared_error: 0.1708\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0289 - mse: 0.0289 - mae: 0.1138 - root_mean_squared_error: 0.1701\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0287 - mse: 0.0287 - mae: 0.1140 - root_mean_squared_error: 0.1695\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0285 - mse: 0.0285 - mae: 0.1132 - root_mean_squared_error: 0.1688\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0283 - mse: 0.0283 - mae: 0.1133 - root_mean_squared_error: 0.1682\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0281 - mse: 0.0281 - mae: 0.1126 - root_mean_squared_error: 0.1676\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0279 - mse: 0.0279 - mae: 0.1127 - root_mean_squared_error: 0.1669\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0277 - mse: 0.0277 - mae: 0.1119 - root_mean_squared_error: 0.1663\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0275 - mse: 0.0275 - mae: 0.1121 - root_mean_squared_error: 0.1657\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0273 - mse: 0.0273 - mae: 0.1113 - root_mean_squared_error: 0.1651\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0271 - mse: 0.0271 - mae: 0.1116 - root_mean_squared_error: 0.1645\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0269 - mse: 0.0269 - mae: 0.1107 - root_mean_squared_error: 0.1640\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0267 - mse: 0.0267 - mae: 0.1111 - root_mean_squared_error: 0.1634\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0265 - mse: 0.0265 - mae: 0.1101 - root_mean_squared_error: 0.1628\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0263 - mse: 0.0263 - mae: 0.1104 - root_mean_squared_error: 0.1622\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0261 - mse: 0.0261 - mae: 0.1094 - root_mean_squared_error: 0.1616\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0259 - mse: 0.0259 - mae: 0.1098 - root_mean_squared_error: 0.1610\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0258 - mse: 0.0258 - mae: 0.1089 - root_mean_squared_error: 0.1605\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0256 - mse: 0.0256 - mae: 0.1092 - root_mean_squared_error: 0.1599\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0254 - mse: 0.0254 - mae: 0.1083 - root_mean_squared_error: 0.1594\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0252 - mse: 0.0252 - mae: 0.1087 - root_mean_squared_error: 0.1588\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0251 - mse: 0.0251 - mae: 0.1078 - root_mean_squared_error: 0.1583\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0249 - mse: 0.0249 - mae: 0.1082 - root_mean_squared_error: 0.1578\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0247 - mse: 0.0247 - mae: 0.1072 - root_mean_squared_error: 0.1573\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0246 - mse: 0.0246 - mae: 0.1077 - root_mean_squared_error: 0.1568\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0244 - mse: 0.0244 - mae: 0.1067 - root_mean_squared_error: 0.1563\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0243 - mse: 0.0243 - mae: 0.1072 - root_mean_squared_error: 0.1558\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0241 - mse: 0.0241 - mae: 0.1063 - root_mean_squared_error: 0.1553\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0240 - mse: 0.0240 - mae: 0.1067 - root_mean_squared_error: 0.1548\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0238 - mse: 0.0238 - mae: 0.1058 - root_mean_squared_error: 0.1543\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0237 - mse: 0.0237 - mae: 0.1062 - root_mean_squared_error: 0.1538\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0235 - mse: 0.0235 - mae: 0.1053 - root_mean_squared_error: 0.1534\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0234 - mse: 0.0234 - mae: 0.1057 - root_mean_squared_error: 0.1529\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0232 - mse: 0.0232 - mae: 0.1048 - root_mean_squared_error: 0.1524\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1052 - root_mean_squared_error: 0.1520\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0230 - mse: 0.0230 - mae: 0.1043 - root_mean_squared_error: 0.1515\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.1048 - root_mean_squared_error: 0.1511\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.1039 - root_mean_squared_error: 0.1506\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0226 - mse: 0.0226 - mae: 0.1044 - root_mean_squared_error: 0.1502\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0224 - mse: 0.0224 - mae: 0.1034 - root_mean_squared_error: 0.1498\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0223 - mse: 0.0223 - mae: 0.1041 - root_mean_squared_error: 0.1494\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0222 - mse: 0.0222 - mae: 0.1030 - root_mean_squared_error: 0.1490\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0221 - mse: 0.0221 - mae: 0.1036 - root_mean_squared_error: 0.1486\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0220 - mse: 0.0220 - mae: 0.1026 - root_mean_squared_error: 0.1482\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0218 - mse: 0.0218 - mae: 0.1031 - root_mean_squared_error: 0.1478\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0217 - mse: 0.0217 - mae: 0.1024 - root_mean_squared_error: 0.1474\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0216 - mse: 0.0216 - mae: 0.1027 - root_mean_squared_error: 0.1470\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0215 - mse: 0.0215 - mae: 0.1019 - root_mean_squared_error: 0.1466\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0214 - mse: 0.0214 - mae: 0.1023 - root_mean_squared_error: 0.1462\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0213 - mse: 0.0213 - mae: 0.1015 - root_mean_squared_error: 0.1459\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.1020 - root_mean_squared_error: 0.1455\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0211 - mse: 0.0211 - mae: 0.1010 - root_mean_squared_error: 0.1452\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0210 - mse: 0.0210 - mae: 0.1018 - root_mean_squared_error: 0.1448\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0209 - mse: 0.0209 - mae: 0.1006 - root_mean_squared_error: 0.1445\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0208 - mse: 0.0208 - mae: 0.1014 - root_mean_squared_error: 0.1441\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndf_predicted_1 = dg.inverse_transform_y(Y_train_val_pred, idx=train_val_idx, return_type='dict_df')\\ndf_predicted_1\\n\\n# predictions on the training set\\nmodel_prediction = model_generator(batch_input_shape=(batch_size_train, n_samples, dg.n_features))\\nmodel_prediction.set_weights(model.get_weights())\\nY_train_pred = model_prediction.predict(X_train, batch_size=batch_size_train)\\n\\n# predictions on the validation without augmented regions\\nmodel_prediction = model_generator(batch_input_shape=(batch_size_val, n_samples, dg.n_features))\\nmodel_prediction.set_weights(model.get_weights())\\nY_val_pred = model_prediction.predict(X_val, batch_size=batch_size_val)\\ndf_Y_real = {loc: dg.df[loc][dg.target_columns].iloc[train_val_idx] for loc in dg.df}\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X% for training, remaining for test\n",
    "ratio_training = 0.8\n",
    "epochs = 200\n",
    "\n",
    "nb_datapoints = dg.batch_size\n",
    "max_train = int(ratio_training * nb_datapoints)\n",
    "train_idx = np.array(range(max_train))\n",
    "valid_idx = np.array([])\n",
    "test_idx = np.array(range(max_train, nb_datapoints))\n",
    "\n",
    "X_train_1 = dg.get_x(train_idx, scaled=True)\n",
    "Y_train = dg.get_y(train_idx, scaled=True)\n",
    "\n",
    "if len(valid_idx) > 0:\n",
    "    X_val_1 = dg.get_x(val_idx, geo=dg.loc_init, scaled=True, use_previous_scaler=True)\n",
    "    Y_val_unscaled = dg.get_y(val_idx, geo=dg.loc_init, scaled=False)\n",
    "    Y_val_real = dg.remove_padded_y(Y_val_unscaled, idx=val_idx, geo=dg.loc_init)\n",
    "\n",
    "X_test_1 = dg.get_x(test_idx, scaled=True, geo=dg.loc_init, use_previous_scaler=True)\n",
    "Y_test_unscaled = dg.get_y(test_idx, scaled=False, geo=dg.loc_init)\n",
    "Y_test_real = dg.remove_padded_y(Y_test_unscaled, idx=test_idx, geo=dg.loc_init)\n",
    "    \n",
    "\"\"\"\n",
    "X_train_val = dg.get_x(train_val_idx, scaled=True)\n",
    "df_Y_real = {loc: dg.df[loc][dg.target_columns].iloc[train_val_idx] for loc in dg.df}\n",
    "Y_real_train_val = dg.get_y(train_val_idx, scaled=True, use_previous_scaler=True)\n",
    "\"\"\"\n",
    "\n",
    "model_generator = get_dense_model\n",
    "\n",
    "batch_size_train = len(X_train_1)\n",
    "batch_size_test = len(X_test_1)\n",
    "\n",
    "if len(valid_idx) > 0:  # use validation set for an early stop\n",
    "    batch_size_val = len(X_val_1)\n",
    "    model_validation = model_generator(batch_input_shape=(batch_size_val, n_samples, dg.n_features))\n",
    "    val_log = ValidationLogger(model_validation, len(X_val), X_val, Y_val_real, valid_idx, dg.loc_init)\n",
    "    callbacks = [val_log, EarlyStopping(monitor='val_root_mean_squared_error', mode='min', verbose=1, patience=20)]\n",
    "else:\n",
    "    callbacks = None\n",
    "\n",
    "model = model_generator(batch_input_shape=(batch_size_train, n_samples, dg.n_features))\n",
    "history = model.fit(X_train_1, Y_train, batch_size=batch_size_train, epochs=epochs, callbacks=callbacks)\n",
    "# compute the predictions on both the training and the validation\n",
    "Y_train_pred_1 = model.predict(X_train_1, batch_size=batch_size_train)\n",
    "\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_test, n_samples, dg.n_features))\n",
    "model_prediction.set_weights(model.get_weights())\n",
    "Y_test_pred_1 = model_prediction.predict(X_test_1, batch_size=batch_size_test)\n",
    "# unscale and unpad the data\n",
    "Y_test_pred_unscaled_1 = dg.inverse_transform_y(Y_test_pred_1, idx=test_idx, geo=dg.loc_init)\n",
    "Y_test_pred_real_1 = dg.remove_padded_y(Y_test_pred_unscaled_1, idx=test_idx, geo=dg.loc_init)\n",
    "\n",
    "\"\"\"\n",
    "df_predicted_1 = dg.inverse_transform_y(Y_train_val_pred, idx=train_val_idx, return_type='dict_df')\n",
    "df_predicted_1\n",
    "\n",
    "# predictions on the training set\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_train, n_samples, dg.n_features))\n",
    "model_prediction.set_weights(model.get_weights())\n",
    "Y_train_pred = model_prediction.predict(X_train, batch_size=batch_size_train)\n",
    "\n",
    "# predictions on the validation without augmented regions\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_val, n_samples, dg.n_features))\n",
    "model_prediction.set_weights(model.get_weights())\n",
    "Y_val_pred = model_prediction.predict(X_val, batch_size=batch_size_val)\n",
    "df_Y_real = {loc: dg.df[loc][dg.target_columns].iloc[train_val_idx] for loc in dg.df}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor k in dg.df:\\n    # add the data columns to the dataframe\\n    df_c[k] = dg.df[k][data_dg_2]\\n    for t in range(1, n_forecast+1):  # add the target columns\\n        target_t = f'{target}(t+{t})'\\n        df_c[k][f'C(t+{t})'] = (df_Y_real[k][target_t] - df_predicted_1[k][target_t]) / df_predicted_1[k][target_t]\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the corrections and add them to a dataframe\n",
    "data_dg_2 = [f'{topic}(t{i})' for i in range(-n_samples+1, 0, 1) for topic in list_topics] + [topic for topic in list_topics]\n",
    "target_df_2 = [f'{target}(t+{i})' for i in range(1, n_forecast+1)]\n",
    "df_c = {loc : dg.df[loc][data_dg_2 + target_df_2] for loc in dg.df}\n",
    "\"\"\"\n",
    "for k in dg.df:\n",
    "    # add the data columns to the dataframe\n",
    "    df_c[k] = dg.df[k][data_dg_2]\n",
    "    for t in range(1, n_forecast+1):  # add the target columns\n",
    "        target_t = f'{target}(t+{t})'\n",
    "        df_c[k][f'C(t+{t})'] = (df_Y_real[k][target_t] - df_predicted_1[k][target_t]) / df_predicted_1[k][target_t]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Fièvre(t-29)</th>\n",
       "      <th>Mal de gorge(t-29)</th>\n",
       "      <th>Symptôme(t-29)</th>\n",
       "      <th>Fièvre(t-28)</th>\n",
       "      <th>Mal de gorge(t-28)</th>\n",
       "      <th>Symptôme(t-28)</th>\n",
       "      <th>Fièvre(t-27)</th>\n",
       "      <th>Mal de gorge(t-27)</th>\n",
       "      <th>Symptôme(t-27)</th>\n",
       "      <th>Fièvre(t-26)</th>\n",
       "      <th>...</th>\n",
       "      <th>TOT_HOSP(t+11)</th>\n",
       "      <th>TOT_HOSP(t+12)</th>\n",
       "      <th>TOT_HOSP(t+13)</th>\n",
       "      <th>TOT_HOSP(t+14)</th>\n",
       "      <th>TOT_HOSP(t+15)</th>\n",
       "      <th>TOT_HOSP(t+16)</th>\n",
       "      <th>TOT_HOSP(t+17)</th>\n",
       "      <th>TOT_HOSP(t+18)</th>\n",
       "      <th>TOT_HOSP(t+19)</th>\n",
       "      <th>TOT_HOSP(t+20)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">BE</th>\n",
       "      <th>2020-03-04</th>\n",
       "      <td>25.402051</td>\n",
       "      <td>25.829284</td>\n",
       "      <td>22.034428</td>\n",
       "      <td>25.245307</td>\n",
       "      <td>25.878495</td>\n",
       "      <td>21.941446</td>\n",
       "      <td>24.616242</td>\n",
       "      <td>25.042280</td>\n",
       "      <td>21.586268</td>\n",
       "      <td>24.280325</td>\n",
       "      <td>...</td>\n",
       "      <td>253.142857</td>\n",
       "      <td>373.142857</td>\n",
       "      <td>531.428571</td>\n",
       "      <td>729.142857</td>\n",
       "      <td>928.857143</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>1389.428571</td>\n",
       "      <td>1685.857143</td>\n",
       "      <td>2010.285714</td>\n",
       "      <td>2373.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>25.245307</td>\n",
       "      <td>25.878495</td>\n",
       "      <td>21.941446</td>\n",
       "      <td>24.616242</td>\n",
       "      <td>25.042280</td>\n",
       "      <td>21.586268</td>\n",
       "      <td>24.280325</td>\n",
       "      <td>24.475947</td>\n",
       "      <td>21.198360</td>\n",
       "      <td>24.268387</td>\n",
       "      <td>...</td>\n",
       "      <td>373.142857</td>\n",
       "      <td>531.428571</td>\n",
       "      <td>729.142857</td>\n",
       "      <td>928.857143</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>1389.428571</td>\n",
       "      <td>1685.857143</td>\n",
       "      <td>2010.285714</td>\n",
       "      <td>2373.285714</td>\n",
       "      <td>2759.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-06</th>\n",
       "      <td>24.616242</td>\n",
       "      <td>25.042280</td>\n",
       "      <td>21.586268</td>\n",
       "      <td>24.280325</td>\n",
       "      <td>24.475947</td>\n",
       "      <td>21.198360</td>\n",
       "      <td>24.268387</td>\n",
       "      <td>24.651690</td>\n",
       "      <td>20.792376</td>\n",
       "      <td>24.279004</td>\n",
       "      <td>...</td>\n",
       "      <td>531.428571</td>\n",
       "      <td>729.142857</td>\n",
       "      <td>928.857143</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>1389.428571</td>\n",
       "      <td>1685.857143</td>\n",
       "      <td>2010.285714</td>\n",
       "      <td>2373.285714</td>\n",
       "      <td>2759.571429</td>\n",
       "      <td>3170.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-07</th>\n",
       "      <td>24.280325</td>\n",
       "      <td>24.475947</td>\n",
       "      <td>21.198360</td>\n",
       "      <td>24.268387</td>\n",
       "      <td>24.651690</td>\n",
       "      <td>20.792376</td>\n",
       "      <td>24.279004</td>\n",
       "      <td>24.994527</td>\n",
       "      <td>20.590845</td>\n",
       "      <td>24.030306</td>\n",
       "      <td>...</td>\n",
       "      <td>729.142857</td>\n",
       "      <td>928.857143</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>1389.428571</td>\n",
       "      <td>1685.857143</td>\n",
       "      <td>2010.285714</td>\n",
       "      <td>2373.285714</td>\n",
       "      <td>2759.571429</td>\n",
       "      <td>3170.285714</td>\n",
       "      <td>3600.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-08</th>\n",
       "      <td>24.268387</td>\n",
       "      <td>24.651690</td>\n",
       "      <td>20.792376</td>\n",
       "      <td>24.279004</td>\n",
       "      <td>24.994527</td>\n",
       "      <td>20.590845</td>\n",
       "      <td>24.030306</td>\n",
       "      <td>24.884379</td>\n",
       "      <td>20.461919</td>\n",
       "      <td>24.089472</td>\n",
       "      <td>...</td>\n",
       "      <td>928.857143</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>1389.428571</td>\n",
       "      <td>1685.857143</td>\n",
       "      <td>2010.285714</td>\n",
       "      <td>2373.285714</td>\n",
       "      <td>2759.571429</td>\n",
       "      <td>3170.285714</td>\n",
       "      <td>3600.857143</td>\n",
       "      <td>3998.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-17</th>\n",
       "      <td>22.274262</td>\n",
       "      <td>35.131666</td>\n",
       "      <td>22.239621</td>\n",
       "      <td>22.250779</td>\n",
       "      <td>35.052849</td>\n",
       "      <td>22.831868</td>\n",
       "      <td>22.082581</td>\n",
       "      <td>34.776027</td>\n",
       "      <td>23.138793</td>\n",
       "      <td>21.799723</td>\n",
       "      <td>...</td>\n",
       "      <td>2697.571429</td>\n",
       "      <td>2764.142857</td>\n",
       "      <td>2838.857143</td>\n",
       "      <td>2887.857143</td>\n",
       "      <td>2920.714286</td>\n",
       "      <td>2954.285714</td>\n",
       "      <td>2997.000000</td>\n",
       "      <td>3029.142857</td>\n",
       "      <td>3053.428571</td>\n",
       "      <td>3066.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-18</th>\n",
       "      <td>22.250779</td>\n",
       "      <td>35.052849</td>\n",
       "      <td>22.831868</td>\n",
       "      <td>22.082581</td>\n",
       "      <td>34.776027</td>\n",
       "      <td>23.138793</td>\n",
       "      <td>21.799723</td>\n",
       "      <td>33.634505</td>\n",
       "      <td>23.251944</td>\n",
       "      <td>21.560343</td>\n",
       "      <td>...</td>\n",
       "      <td>2764.142857</td>\n",
       "      <td>2838.857143</td>\n",
       "      <td>2887.857143</td>\n",
       "      <td>2920.714286</td>\n",
       "      <td>2954.285714</td>\n",
       "      <td>2997.000000</td>\n",
       "      <td>3029.142857</td>\n",
       "      <td>3053.428571</td>\n",
       "      <td>3066.428571</td>\n",
       "      <td>3082.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-19</th>\n",
       "      <td>22.082581</td>\n",
       "      <td>34.776027</td>\n",
       "      <td>23.138793</td>\n",
       "      <td>21.799723</td>\n",
       "      <td>33.634505</td>\n",
       "      <td>23.251944</td>\n",
       "      <td>21.560343</td>\n",
       "      <td>32.425871</td>\n",
       "      <td>23.347154</td>\n",
       "      <td>21.668993</td>\n",
       "      <td>...</td>\n",
       "      <td>2838.857143</td>\n",
       "      <td>2887.857143</td>\n",
       "      <td>2920.714286</td>\n",
       "      <td>2954.285714</td>\n",
       "      <td>2997.000000</td>\n",
       "      <td>3029.142857</td>\n",
       "      <td>3053.428571</td>\n",
       "      <td>3066.428571</td>\n",
       "      <td>3082.285714</td>\n",
       "      <td>3105.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-20</th>\n",
       "      <td>21.799723</td>\n",
       "      <td>33.634505</td>\n",
       "      <td>23.251944</td>\n",
       "      <td>21.560343</td>\n",
       "      <td>32.425871</td>\n",
       "      <td>23.347154</td>\n",
       "      <td>21.668993</td>\n",
       "      <td>31.181405</td>\n",
       "      <td>23.458170</td>\n",
       "      <td>21.891241</td>\n",
       "      <td>...</td>\n",
       "      <td>2887.857143</td>\n",
       "      <td>2920.714286</td>\n",
       "      <td>2954.285714</td>\n",
       "      <td>2997.000000</td>\n",
       "      <td>3029.142857</td>\n",
       "      <td>3053.428571</td>\n",
       "      <td>3066.428571</td>\n",
       "      <td>3082.285714</td>\n",
       "      <td>3105.428571</td>\n",
       "      <td>3128.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-21</th>\n",
       "      <td>21.560343</td>\n",
       "      <td>32.425871</td>\n",
       "      <td>23.347154</td>\n",
       "      <td>21.668993</td>\n",
       "      <td>31.181405</td>\n",
       "      <td>23.458170</td>\n",
       "      <td>21.891241</td>\n",
       "      <td>29.244671</td>\n",
       "      <td>23.651876</td>\n",
       "      <td>21.963463</td>\n",
       "      <td>...</td>\n",
       "      <td>2920.714286</td>\n",
       "      <td>2954.285714</td>\n",
       "      <td>2997.000000</td>\n",
       "      <td>3029.142857</td>\n",
       "      <td>3053.428571</td>\n",
       "      <td>3066.428571</td>\n",
       "      <td>3082.285714</td>\n",
       "      <td>3105.428571</td>\n",
       "      <td>3128.285714</td>\n",
       "      <td>3122.571429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>383 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Fièvre(t-29)  Mal de gorge(t-29)  Symptôme(t-29)  \\\n",
       "LOC DATE                                                           \n",
       "BE  2020-03-04     25.402051           25.829284       22.034428   \n",
       "    2020-03-05     25.245307           25.878495       21.941446   \n",
       "    2020-03-06     24.616242           25.042280       21.586268   \n",
       "    2020-03-07     24.280325           24.475947       21.198360   \n",
       "    2020-03-08     24.268387           24.651690       20.792376   \n",
       "...                      ...                 ...             ...   \n",
       "    2021-03-17     22.274262           35.131666       22.239621   \n",
       "    2021-03-18     22.250779           35.052849       22.831868   \n",
       "    2021-03-19     22.082581           34.776027       23.138793   \n",
       "    2021-03-20     21.799723           33.634505       23.251944   \n",
       "    2021-03-21     21.560343           32.425871       23.347154   \n",
       "\n",
       "                Fièvre(t-28)  Mal de gorge(t-28)  Symptôme(t-28)  \\\n",
       "LOC DATE                                                           \n",
       "BE  2020-03-04     25.245307           25.878495       21.941446   \n",
       "    2020-03-05     24.616242           25.042280       21.586268   \n",
       "    2020-03-06     24.280325           24.475947       21.198360   \n",
       "    2020-03-07     24.268387           24.651690       20.792376   \n",
       "    2020-03-08     24.279004           24.994527       20.590845   \n",
       "...                      ...                 ...             ...   \n",
       "    2021-03-17     22.250779           35.052849       22.831868   \n",
       "    2021-03-18     22.082581           34.776027       23.138793   \n",
       "    2021-03-19     21.799723           33.634505       23.251944   \n",
       "    2021-03-20     21.560343           32.425871       23.347154   \n",
       "    2021-03-21     21.668993           31.181405       23.458170   \n",
       "\n",
       "                Fièvre(t-27)  Mal de gorge(t-27)  Symptôme(t-27)  \\\n",
       "LOC DATE                                                           \n",
       "BE  2020-03-04     24.616242           25.042280       21.586268   \n",
       "    2020-03-05     24.280325           24.475947       21.198360   \n",
       "    2020-03-06     24.268387           24.651690       20.792376   \n",
       "    2020-03-07     24.279004           24.994527       20.590845   \n",
       "    2020-03-08     24.030306           24.884379       20.461919   \n",
       "...                      ...                 ...             ...   \n",
       "    2021-03-17     22.082581           34.776027       23.138793   \n",
       "    2021-03-18     21.799723           33.634505       23.251944   \n",
       "    2021-03-19     21.560343           32.425871       23.347154   \n",
       "    2021-03-20     21.668993           31.181405       23.458170   \n",
       "    2021-03-21     21.891241           29.244671       23.651876   \n",
       "\n",
       "                Fièvre(t-26)  ...  TOT_HOSP(t+11)  TOT_HOSP(t+12)  \\\n",
       "LOC DATE                      ...                                   \n",
       "BE  2020-03-04     24.280325  ...      253.142857      373.142857   \n",
       "    2020-03-05     24.268387  ...      373.142857      531.428571   \n",
       "    2020-03-06     24.279004  ...      531.428571      729.142857   \n",
       "    2020-03-07     24.030306  ...      729.142857      928.857143   \n",
       "    2020-03-08     24.089472  ...      928.857143     1145.000000   \n",
       "...                      ...  ...             ...             ...   \n",
       "    2021-03-17     21.799723  ...     2697.571429     2764.142857   \n",
       "    2021-03-18     21.560343  ...     2764.142857     2838.857143   \n",
       "    2021-03-19     21.668993  ...     2838.857143     2887.857143   \n",
       "    2021-03-20     21.891241  ...     2887.857143     2920.714286   \n",
       "    2021-03-21     21.963463  ...     2920.714286     2954.285714   \n",
       "\n",
       "                TOT_HOSP(t+13)  TOT_HOSP(t+14)  TOT_HOSP(t+15)  \\\n",
       "LOC DATE                                                         \n",
       "BE  2020-03-04      531.428571      729.142857      928.857143   \n",
       "    2020-03-05      729.142857      928.857143     1145.000000   \n",
       "    2020-03-06      928.857143     1145.000000     1389.428571   \n",
       "    2020-03-07     1145.000000     1389.428571     1685.857143   \n",
       "    2020-03-08     1389.428571     1685.857143     2010.285714   \n",
       "...                        ...             ...             ...   \n",
       "    2021-03-17     2838.857143     2887.857143     2920.714286   \n",
       "    2021-03-18     2887.857143     2920.714286     2954.285714   \n",
       "    2021-03-19     2920.714286     2954.285714     2997.000000   \n",
       "    2021-03-20     2954.285714     2997.000000     3029.142857   \n",
       "    2021-03-21     2997.000000     3029.142857     3053.428571   \n",
       "\n",
       "                TOT_HOSP(t+16)  TOT_HOSP(t+17)  TOT_HOSP(t+18)  \\\n",
       "LOC DATE                                                         \n",
       "BE  2020-03-04     1145.000000     1389.428571     1685.857143   \n",
       "    2020-03-05     1389.428571     1685.857143     2010.285714   \n",
       "    2020-03-06     1685.857143     2010.285714     2373.285714   \n",
       "    2020-03-07     2010.285714     2373.285714     2759.571429   \n",
       "    2020-03-08     2373.285714     2759.571429     3170.285714   \n",
       "...                        ...             ...             ...   \n",
       "    2021-03-17     2954.285714     2997.000000     3029.142857   \n",
       "    2021-03-18     2997.000000     3029.142857     3053.428571   \n",
       "    2021-03-19     3029.142857     3053.428571     3066.428571   \n",
       "    2021-03-20     3053.428571     3066.428571     3082.285714   \n",
       "    2021-03-21     3066.428571     3082.285714     3105.428571   \n",
       "\n",
       "                TOT_HOSP(t+19)  TOT_HOSP(t+20)  \n",
       "LOC DATE                                        \n",
       "BE  2020-03-04     2010.285714     2373.285714  \n",
       "    2020-03-05     2373.285714     2759.571429  \n",
       "    2020-03-06     2759.571429     3170.285714  \n",
       "    2020-03-07     3170.285714     3600.857143  \n",
       "    2020-03-08     3600.857143     3998.571429  \n",
       "...                        ...             ...  \n",
       "    2021-03-17     3053.428571     3066.428571  \n",
       "    2021-03-18     3066.428571     3082.285714  \n",
       "    2021-03-19     3082.285714     3105.428571  \n",
       "    2021-03-20     3105.428571     3128.285714  \n",
       "    2021-03-21     3128.285714     3122.571429  \n",
       "\n",
       "[383 rows x 110 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c['BE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_generator = MinMaxScaler\n",
    "dg_2 = util.DataGenerator(df_c, n_samples, n_forecast, target=target, scaler_generator=scaler_generator, \n",
    "                          scaler_type='batch', augment_merge=0, predict_one=False, cumsum=False,\n",
    "                          data_columns=[k for k in list_topics], no_lag=True)\n",
    "dg_2.set_loc_init(dg.loc_init)  # consider the other localisations as being augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the second estimator to compute $Y_2(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.4726 - mse: 0.4726 - mae: 0.5215 - root_mean_squared_error: 0.6874\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4410 - mse: 0.4410 - mae: 0.5016 - root_mean_squared_error: 0.6641\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4200 - mse: 0.4200 - mae: 0.4883 - root_mean_squared_error: 0.6481\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4035 - mse: 0.4035 - mae: 0.4779 - root_mean_squared_error: 0.6352\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3896 - mse: 0.3896 - mae: 0.4690 - root_mean_squared_error: 0.6242\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3774 - mse: 0.3774 - mae: 0.4612 - root_mean_squared_error: 0.6143\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3664 - mse: 0.3664 - mae: 0.4542 - root_mean_squared_error: 0.6053\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3564 - mse: 0.3564 - mae: 0.4477 - root_mean_squared_error: 0.5970\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3471 - mse: 0.3471 - mae: 0.4417 - root_mean_squared_error: 0.5892\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3385 - mse: 0.3385 - mae: 0.4361 - root_mean_squared_error: 0.5818\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3304 - mse: 0.3304 - mae: 0.4308 - root_mean_squared_error: 0.5748\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3227 - mse: 0.3227 - mae: 0.4257 - root_mean_squared_error: 0.5681\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3154 - mse: 0.3154 - mae: 0.4209 - root_mean_squared_error: 0.5616\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3085 - mse: 0.3085 - mae: 0.4162 - root_mean_squared_error: 0.5554\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3018 - mse: 0.3018 - mae: 0.4118 - root_mean_squared_error: 0.5494\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2955 - mse: 0.2955 - mae: 0.4074 - root_mean_squared_error: 0.5436\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2894 - mse: 0.2894 - mae: 0.4033 - root_mean_squared_error: 0.5379\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2835 - mse: 0.2835 - mae: 0.3992 - root_mean_squared_error: 0.5324\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2778 - mse: 0.2778 - mae: 0.3953 - root_mean_squared_error: 0.5271\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2723 - mse: 0.2723 - mae: 0.3915 - root_mean_squared_error: 0.5218\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2670 - mse: 0.2670 - mae: 0.3878 - root_mean_squared_error: 0.5167\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2618 - mse: 0.2618 - mae: 0.3841 - root_mean_squared_error: 0.5117\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2569 - mse: 0.2569 - mae: 0.3806 - root_mean_squared_error: 0.5068\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2520 - mse: 0.2520 - mae: 0.3772 - root_mean_squared_error: 0.5020\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2473 - mse: 0.2473 - mae: 0.3738 - root_mean_squared_error: 0.4973\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2427 - mse: 0.2427 - mae: 0.3706 - root_mean_squared_error: 0.4927\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2383 - mse: 0.2383 - mae: 0.3674 - root_mean_squared_error: 0.4881\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2340 - mse: 0.2340 - mae: 0.3643 - root_mean_squared_error: 0.4837\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2297 - mse: 0.2297 - mae: 0.3612 - root_mean_squared_error: 0.4793\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2256 - mse: 0.2256 - mae: 0.3583 - root_mean_squared_error: 0.4750\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2216 - mse: 0.2216 - mae: 0.3554 - root_mean_squared_error: 0.4708\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2178 - mse: 0.2178 - mae: 0.3526 - root_mean_squared_error: 0.4666\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2140 - mse: 0.2140 - mae: 0.3499 - root_mean_squared_error: 0.4626\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2103 - mse: 0.2103 - mae: 0.3472 - root_mean_squared_error: 0.4585\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.2067 - mse: 0.2067 - mae: 0.3446 - root_mean_squared_error: 0.4546\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2031 - mse: 0.2031 - mae: 0.3420 - root_mean_squared_error: 0.4507\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1997 - mse: 0.1997 - mae: 0.3395 - root_mean_squared_error: 0.4469\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1964 - mse: 0.1964 - mae: 0.3371 - root_mean_squared_error: 0.4431\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1931 - mse: 0.1931 - mae: 0.3347 - root_mean_squared_error: 0.4394\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1899 - mse: 0.1899 - mae: 0.3324 - root_mean_squared_error: 0.4358\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1868 - mse: 0.1868 - mae: 0.3301 - root_mean_squared_error: 0.4322\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1838 - mse: 0.1838 - mae: 0.3279 - root_mean_squared_error: 0.4287\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1808 - mse: 0.1808 - mae: 0.3257 - root_mean_squared_error: 0.4252\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1779 - mse: 0.1779 - mae: 0.3236 - root_mean_squared_error: 0.4218\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1751 - mse: 0.1751 - mae: 0.3215 - root_mean_squared_error: 0.4185\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1724 - mse: 0.1724 - mae: 0.3194 - root_mean_squared_error: 0.4152\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1697 - mse: 0.1697 - mae: 0.3174 - root_mean_squared_error: 0.4120\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1671 - mse: 0.1671 - mae: 0.3154 - root_mean_squared_error: 0.4088\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1646 - mse: 0.1646 - mae: 0.3135 - root_mean_squared_error: 0.4057\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1621 - mse: 0.1621 - mae: 0.3116 - root_mean_squared_error: 0.4026\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1597 - mse: 0.1597 - mae: 0.3097 - root_mean_squared_error: 0.3996\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1573 - mse: 0.1573 - mae: 0.3078 - root_mean_squared_error: 0.3966\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1550 - mse: 0.1550 - mae: 0.3060 - root_mean_squared_error: 0.3937\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1527 - mse: 0.1527 - mae: 0.3042 - root_mean_squared_error: 0.3908\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1506 - mse: 0.1506 - mae: 0.3024 - root_mean_squared_error: 0.3880\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1484 - mse: 0.1484 - mae: 0.3007 - root_mean_squared_error: 0.3853\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1463 - mse: 0.1463 - mae: 0.2989 - root_mean_squared_error: 0.3826\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1443 - mse: 0.1443 - mae: 0.2972 - root_mean_squared_error: 0.3799\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1423 - mse: 0.1423 - mae: 0.2955 - root_mean_squared_error: 0.3773\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1404 - mse: 0.1404 - mae: 0.2939 - root_mean_squared_error: 0.3747\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1385 - mse: 0.1385 - mae: 0.2922 - root_mean_squared_error: 0.3722\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1367 - mse: 0.1367 - mae: 0.2906 - root_mean_squared_error: 0.3697\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1349 - mse: 0.1349 - mae: 0.2890 - root_mean_squared_error: 0.3673\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1332 - mse: 0.1332 - mae: 0.2875 - root_mean_squared_error: 0.3649\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1315 - mse: 0.1315 - mae: 0.2859 - root_mean_squared_error: 0.3626\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1298 - mse: 0.1298 - mae: 0.2844 - root_mean_squared_error: 0.3603\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1282 - mse: 0.1282 - mae: 0.2829 - root_mean_squared_error: 0.3581\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1266 - mse: 0.1266 - mae: 0.2814 - root_mean_squared_error: 0.3559\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1251 - mse: 0.1251 - mae: 0.2800 - root_mean_squared_error: 0.3537\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1236 - mse: 0.1236 - mae: 0.2786 - root_mean_squared_error: 0.3516\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1222 - mse: 0.1222 - mae: 0.2772 - root_mean_squared_error: 0.3495\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1208 - mse: 0.1208 - mae: 0.2758 - root_mean_squared_error: 0.3475\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1194 - mse: 0.1194 - mae: 0.2744 - root_mean_squared_error: 0.3455\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1180 - mse: 0.1180 - mae: 0.2731 - root_mean_squared_error: 0.3436\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1167 - mse: 0.1167 - mae: 0.2718 - root_mean_squared_error: 0.3417\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1155 - mse: 0.1155 - mae: 0.2706 - root_mean_squared_error: 0.3398\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1142 - mse: 0.1142 - mae: 0.2693 - root_mean_squared_error: 0.3380\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1130 - mse: 0.1130 - mae: 0.2681 - root_mean_squared_error: 0.3362\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1119 - mse: 0.1119 - mae: 0.2669 - root_mean_squared_error: 0.3345\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1107 - mse: 0.1107 - mae: 0.2658 - root_mean_squared_error: 0.3328\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1096 - mse: 0.1096 - mae: 0.2647 - root_mean_squared_error: 0.3311\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1086 - mse: 0.1086 - mae: 0.2636 - root_mean_squared_error: 0.3295\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1075 - mse: 0.1075 - mae: 0.2625 - root_mean_squared_error: 0.3279\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1065 - mse: 0.1065 - mae: 0.2615 - root_mean_squared_error: 0.3264\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1055 - mse: 0.1055 - mae: 0.2605 - root_mean_squared_error: 0.3248\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1046 - mse: 0.1046 - mae: 0.2596 - root_mean_squared_error: 0.3234\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1036 - mse: 0.1036 - mae: 0.2586 - root_mean_squared_error: 0.3219\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1027 - mse: 0.1027 - mae: 0.2577 - root_mean_squared_error: 0.3205\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1019 - mse: 0.1019 - mae: 0.2568 - root_mean_squared_error: 0.3192\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1010 - mse: 0.1010 - mae: 0.2560 - root_mean_squared_error: 0.3179\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1002 - mse: 0.1002 - mae: 0.2552 - root_mean_squared_error: 0.3166\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0994 - mse: 0.0994 - mae: 0.2544 - root_mean_squared_error: 0.3153\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0987 - mse: 0.0987 - mae: 0.2536 - root_mean_squared_error: 0.3141\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0979 - mse: 0.0979 - mae: 0.2530 - root_mean_squared_error: 0.3129\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0972 - mse: 0.0972 - mae: 0.2522 - root_mean_squared_error: 0.3118\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0965 - mse: 0.0965 - mae: 0.2515 - root_mean_squared_error: 0.3107\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0959 - mse: 0.0959 - mae: 0.2509 - root_mean_squared_error: 0.3096\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0952 - mse: 0.0952 - mae: 0.2502 - root_mean_squared_error: 0.3086\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0946 - mse: 0.0946 - mae: 0.2496 - root_mean_squared_error: 0.3076\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0940 - mse: 0.0940 - mae: 0.2491 - root_mean_squared_error: 0.3066\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0934 - mse: 0.0934 - mae: 0.2485 - root_mean_squared_error: 0.3057\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0929 - mse: 0.0929 - mae: 0.2480 - root_mean_squared_error: 0.3048\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0923 - mse: 0.0923 - mae: 0.2474 - root_mean_squared_error: 0.3039\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0918 - mse: 0.0918 - mae: 0.2469 - root_mean_squared_error: 0.3030\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0913 - mse: 0.0913 - mae: 0.2464 - root_mean_squared_error: 0.3022\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0909 - mse: 0.0909 - mae: 0.2460 - root_mean_squared_error: 0.3015\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0904 - mse: 0.0904 - mae: 0.2455 - root_mean_squared_error: 0.3007\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0900 - mse: 0.0900 - mae: 0.2451 - root_mean_squared_error: 0.3000\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0896 - mse: 0.0896 - mae: 0.2447 - root_mean_squared_error: 0.2993\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0892 - mse: 0.0892 - mae: 0.2442 - root_mean_squared_error: 0.2987\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0888 - mse: 0.0888 - mae: 0.2440 - root_mean_squared_error: 0.2980\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0884 - mse: 0.0884 - mae: 0.2435 - root_mean_squared_error: 0.2974\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0881 - mse: 0.0881 - mae: 0.2432 - root_mean_squared_error: 0.2968\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0878 - mse: 0.0878 - mae: 0.2428 - root_mean_squared_error: 0.2963\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0875 - mse: 0.0875 - mae: 0.2425 - root_mean_squared_error: 0.2957\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0872 - mse: 0.0872 - mae: 0.2422 - root_mean_squared_error: 0.2952\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0869 - mse: 0.0869 - mae: 0.2419 - root_mean_squared_error: 0.2947\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0866 - mse: 0.0866 - mae: 0.2416 - root_mean_squared_error: 0.2943\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0863 - mse: 0.0863 - mae: 0.2413 - root_mean_squared_error: 0.2938\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0861 - mse: 0.0861 - mae: 0.2411 - root_mean_squared_error: 0.2934\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0858 - mse: 0.0858 - mae: 0.2408 - root_mean_squared_error: 0.2930\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0856 - mse: 0.0856 - mae: 0.2406 - root_mean_squared_error: 0.2926\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0854 - mse: 0.0854 - mae: 0.2403 - root_mean_squared_error: 0.2922\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0852 - mse: 0.0852 - mae: 0.2402 - root_mean_squared_error: 0.2918\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0849 - mse: 0.0849 - mae: 0.2399 - root_mean_squared_error: 0.2915\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0847 - mse: 0.0847 - mae: 0.2396 - root_mean_squared_error: 0.2911\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0846 - mse: 0.0846 - mae: 0.2395 - root_mean_squared_error: 0.2908\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0844 - mse: 0.0844 - mae: 0.2392 - root_mean_squared_error: 0.2905\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0842 - mse: 0.0842 - mae: 0.2391 - root_mean_squared_error: 0.2902\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0840 - mse: 0.0840 - mae: 0.2388 - root_mean_squared_error: 0.2898\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0838 - mse: 0.0838 - mae: 0.2387 - root_mean_squared_error: 0.2895\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0837 - mse: 0.0837 - mae: 0.2385 - root_mean_squared_error: 0.2893\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0835 - mse: 0.0835 - mae: 0.2383 - root_mean_squared_error: 0.2890\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0833 - mse: 0.0833 - mae: 0.2381 - root_mean_squared_error: 0.2887\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0832 - mse: 0.0832 - mae: 0.2379 - root_mean_squared_error: 0.2884\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0830 - mse: 0.0830 - mae: 0.2378 - root_mean_squared_error: 0.2882\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0829 - mse: 0.0829 - mae: 0.2376 - root_mean_squared_error: 0.2879\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0827 - mse: 0.0827 - mae: 0.2375 - root_mean_squared_error: 0.2876\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0826 - mse: 0.0826 - mae: 0.2373 - root_mean_squared_error: 0.2874\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0825 - mse: 0.0825 - mae: 0.2371 - root_mean_squared_error: 0.2871\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0823 - mse: 0.0823 - mae: 0.2370 - root_mean_squared_error: 0.2869\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0822 - mse: 0.0822 - mae: 0.2368 - root_mean_squared_error: 0.2866\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0820 - mse: 0.0820 - mae: 0.2367 - root_mean_squared_error: 0.2864\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0819 - mse: 0.0819 - mae: 0.2365 - root_mean_squared_error: 0.2862\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0817 - mse: 0.0817 - mae: 0.2363 - root_mean_squared_error: 0.2859\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0816 - mse: 0.0816 - mae: 0.2361 - root_mean_squared_error: 0.2857\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0815 - mse: 0.0815 - mae: 0.2360 - root_mean_squared_error: 0.2854\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0813 - mse: 0.0813 - mae: 0.2358 - root_mean_squared_error: 0.2852\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0812 - mse: 0.0812 - mae: 0.2357 - root_mean_squared_error: 0.2849\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0811 - mse: 0.0811 - mae: 0.2355 - root_mean_squared_error: 0.2847\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0809 - mse: 0.0809 - mae: 0.2354 - root_mean_squared_error: 0.2845\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0808 - mse: 0.0808 - mae: 0.2352 - root_mean_squared_error: 0.2843\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0807 - mse: 0.0807 - mae: 0.2351 - root_mean_squared_error: 0.2840\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0805 - mse: 0.0805 - mae: 0.2349 - root_mean_squared_error: 0.2838\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0804 - mse: 0.0804 - mae: 0.2349 - root_mean_squared_error: 0.2836\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0803 - mse: 0.0803 - mae: 0.2345 - root_mean_squared_error: 0.2833\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0802 - mse: 0.0802 - mae: 0.2346 - root_mean_squared_error: 0.2831\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0800 - mse: 0.0800 - mae: 0.2343 - root_mean_squared_error: 0.2829\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0799 - mse: 0.0799 - mae: 0.2343 - root_mean_squared_error: 0.2827\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0798 - mse: 0.0798 - mae: 0.2340 - root_mean_squared_error: 0.2825\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0797 - mse: 0.0797 - mae: 0.2339 - root_mean_squared_error: 0.2822\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0795 - mse: 0.0795 - mae: 0.2338 - root_mean_squared_error: 0.2820\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0794 - mse: 0.0794 - mae: 0.2337 - root_mean_squared_error: 0.2818\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0793 - mse: 0.0793 - mae: 0.2335 - root_mean_squared_error: 0.2816\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.0792 - mse: 0.0792 - mae: 0.2334 - root_mean_squared_error: 0.2814\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0791 - mse: 0.0791 - mae: 0.2332 - root_mean_squared_error: 0.2812\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.0789 - mse: 0.0789 - mae: 0.2331 - root_mean_squared_error: 0.2810\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0788 - mse: 0.0788 - mae: 0.2330 - root_mean_squared_error: 0.2808\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0787 - mse: 0.0787 - mae: 0.2329 - root_mean_squared_error: 0.2806\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0786 - mse: 0.0786 - mae: 0.2327 - root_mean_squared_error: 0.2804\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0785 - mse: 0.0785 - mae: 0.2326 - root_mean_squared_error: 0.2802\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0784 - mse: 0.0784 - mae: 0.2324 - root_mean_squared_error: 0.2800\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0783 - mse: 0.0783 - mae: 0.2324 - root_mean_squared_error: 0.2798\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0782 - mse: 0.0782 - mae: 0.2322 - root_mean_squared_error: 0.2796\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0781 - mse: 0.0781 - mae: 0.2322 - root_mean_squared_error: 0.2794\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0780 - mse: 0.0780 - mae: 0.2320 - root_mean_squared_error: 0.2792\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0779 - mse: 0.0779 - mae: 0.2319 - root_mean_squared_error: 0.2790\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0778 - mse: 0.0778 - mae: 0.2317 - root_mean_squared_error: 0.2788\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0777 - mse: 0.0777 - mae: 0.2317 - root_mean_squared_error: 0.2787\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0776 - mse: 0.0776 - mae: 0.2315 - root_mean_squared_error: 0.2785\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0775 - mse: 0.0775 - mae: 0.2315 - root_mean_squared_error: 0.2783\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0774 - mse: 0.0774 - mae: 0.2313 - root_mean_squared_error: 0.2781\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0773 - mse: 0.0773 - mae: 0.2313 - root_mean_squared_error: 0.2780\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0772 - mse: 0.0772 - mae: 0.2311 - root_mean_squared_error: 0.2778\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0771 - mse: 0.0771 - mae: 0.2311 - root_mean_squared_error: 0.2776\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0770 - mse: 0.0770 - mae: 0.2308 - root_mean_squared_error: 0.2775\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0769 - mse: 0.0769 - mae: 0.2309 - root_mean_squared_error: 0.2773\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0768 - mse: 0.0768 - mae: 0.2307 - root_mean_squared_error: 0.2771\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0767 - mse: 0.0767 - mae: 0.2307 - root_mean_squared_error: 0.2770\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0766 - mse: 0.0766 - mae: 0.2305 - root_mean_squared_error: 0.2768\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0765 - mse: 0.0765 - mae: 0.2305 - root_mean_squared_error: 0.2767\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0765 - mse: 0.0765 - mae: 0.2303 - root_mean_squared_error: 0.2765\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0764 - mse: 0.0764 - mae: 0.2303 - root_mean_squared_error: 0.2764\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0763 - mse: 0.0763 - mae: 0.2301 - root_mean_squared_error: 0.2762\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.0762 - mse: 0.0762 - mae: 0.2301 - root_mean_squared_error: 0.2761\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0761 - mse: 0.0761 - mae: 0.2300 - root_mean_squared_error: 0.2759\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0761 - mse: 0.0761 - mae: 0.2300 - root_mean_squared_error: 0.2758\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0760 - mse: 0.0760 - mae: 0.2298 - root_mean_squared_error: 0.2756\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0759 - mse: 0.0759 - mae: 0.2298 - root_mean_squared_error: 0.2755\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.0758 - mse: 0.0758 - mae: 0.2296 - root_mean_squared_error: 0.2754\n"
     ]
    }
   ],
   "source": [
    "X_train_2 = dg_2.get_x(train_idx, scaled=True)\n",
    "Y_train_2 = dg_2.get_y(train_idx, scaled=True)\n",
    "\n",
    "if len(valid_idx) > 0:\n",
    "    X_val_2 = dg_2.get_x(valid_idx, scaled=True, use_previous_scaler=True, geo=dg_2.loc_init)\n",
    "    Y_val_unscaled_2 = dg.get_y(valid_idx, geo=dg.loc_init, scaled=False)\n",
    "    Y_val_real_2 = dg.remove_padded_y(Y_val_unscaled, idx=val_idx, geo=dg.loc_init)\n",
    "    batch_size_val = len(X_val_2)\n",
    "    \n",
    "X_test_2 = dg_2.get_x(test_idx, scaled=True, use_previous_scaler=True, geo=dg_2.loc_init)\n",
    "Y_test_unscaled_2 = dg.get_y(test_idx, scaled=False, geo=dg_2.loc_init)\n",
    "Y_test_real_2 = dg.remove_padded_y(Y_test_unscaled_2, idx=test_idx, geo=dg_2.loc_init)\n",
    "\n",
    "model_generator = get_dense_model\n",
    "\n",
    "batch_size_train = len(X_train_2)\n",
    "batch_size_test = len(X_test_2)\n",
    "model = model_generator(batch_input_shape=(batch_size_train, n_samples, dg_2.n_features))\n",
    "    \n",
    "history = model.fit(X_train_2, Y_train_2, batch_size=batch_size_train, epochs=epochs, callbacks=None)\n",
    "# compute the predictions on both the training and the validation\n",
    "Y_train_pred_2 = model.predict(X_train_2, batch_size=batch_size_train)\n",
    "\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_test, n_samples, dg_2.n_features))\n",
    "model_prediction.set_weights(model.get_weights())\n",
    "Y_test_pred_2 = model_prediction.predict(X_test_2, batch_size=batch_size_test)\n",
    "# unscale and unpad the data\n",
    "Y_test_pred_unscaled_2 = dg_2.inverse_transform_y(Y_test_pred_2, idx=test_idx, geo=dg_2.loc_init)\n",
    "Y_test_pred_real_2 = dg.remove_padded_y(Y_test_pred_unscaled_2, idx=test_idx, geo=dg_2.loc_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the final prediction by combining the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssembleLayerTimeDist(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    assemble the prediction from the trends and the predictions from another model\n",
    "    the same weight is used at every timestep (1 trainable parameter)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape):\n",
    "        super(AssembleLayer, self).__init__(name='')\n",
    "        self.dense_trends = tf.keras.layers.Dense(1, use_bias=False, \n",
    "                                                 kernel_constraint=tf.keras.constraints.MinMaxNorm(0.001, 0.2))\n",
    "        batch_input_shape = (input_shape[0], input_shape[1], 1)\n",
    "        self.time_dist = TimeDistributed(self.dense_trends, batch_input_shape=batch_input_shape)\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x_trends = input_tensor[:, :, 1:]\n",
    "        x_hosp = input_tensor[:, :, :1]\n",
    "        x_trends = tf.keras.layers.Subtract()([x_trends, x_hosp])  # x_trends - x_hosp\n",
    "        x_trends = self.time_dist(x_trends, training=training)  # apply simple weight\n",
    "        return tf.keras.layers.Add()([x_trends, x_hosp])  # final prediction = x_hosp + (x_trends - x_hosp) * c\n",
    "\n",
    "class MinMaxPositive(tf.keras.constraints.Constraint):\n",
    "    \n",
    "    def __init__(self, min_value, max_value):\n",
    "        self.min_max_norm = tf.keras.constraints.MinMaxNorm(min_value, max_value)\n",
    "        self.non_neg = tf.keras.constraints.NonNeg()\n",
    "        \n",
    "    def __call__(self, w):\n",
    "        return self.min_max_norm(self.non_neg(w))\n",
    "    \n",
    "class AssembleLayer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    assemble the prediction from the trends and the predictions from another model\n",
    "    different weight are used at every timestep (n_forecast trainable parameter)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_input_shape):\n",
    "        super(AssembleLayer, self).__init__(name='')\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[1,batch_input_shape[1]], \n",
    "                                      constraint=MinMaxPositive(0.001, 0.2))\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x_trends = input_tensor[:, :, 1]\n",
    "        x_hosp = input_tensor[:, :, 0]\n",
    "        x_trends = tf.keras.layers.Subtract()([x_trends, x_hosp])  # x_trends - x_hosp\n",
    "        x_trends = tf.multiply(x_trends, self.kernel)  # apply simple weight: (x_trends - x_hosp) * c\n",
    "        return tf.keras.layers.Add()([x_hosp, x_trends])  # final prediction = x_hosp + (x_trends - x_hosp) * c\n",
    "\n",
    "    \n",
    "def get_assemble(batch_input_shape):\n",
    "    model = AssembleLayer(batch_input_shape)\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0229 - mse: 0.0229 - mae: 0.1087 - root_mean_squared_error: 0.1514\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1020 - root_mean_squared_error: 0.1432\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1020 - root_mean_squared_error: 0.1431\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1019 - root_mean_squared_error: 0.1431\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1018 - root_mean_squared_error: 0.1430\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1018 - root_mean_squared_error: 0.1430\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1017 - root_mean_squared_error: 0.1429\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1017 - root_mean_squared_error: 0.1429\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1017 - root_mean_squared_error: 0.1429\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1016 - root_mean_squared_error: 0.1428\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1016 - root_mean_squared_error: 0.1428\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1016 - root_mean_squared_error: 0.1428\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1015 - root_mean_squared_error: 0.1428\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1015 - root_mean_squared_error: 0.1427\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1015 - root_mean_squared_error: 0.1427\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1014 - root_mean_squared_error: 0.1427\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1014 - root_mean_squared_error: 0.1427\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1014 - root_mean_squared_error: 0.1426\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1013 - root_mean_squared_error: 0.1426\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1013 - root_mean_squared_error: 0.1426\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1013 - root_mean_squared_error: 0.1426\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1012 - root_mean_squared_error: 0.1425\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1012 - root_mean_squared_error: 0.1425\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1012 - root_mean_squared_error: 0.1425\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1012 - root_mean_squared_error: 0.1425\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1011 - root_mean_squared_error: 0.1425\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1011 - root_mean_squared_error: 0.1424\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1011 - root_mean_squared_error: 0.1424\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1011 - root_mean_squared_error: 0.1424\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1010 - root_mean_squared_error: 0.1424\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1010 - root_mean_squared_error: 0.1423\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1010 - root_mean_squared_error: 0.1423\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1010 - root_mean_squared_error: 0.1423\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1009 - root_mean_squared_error: 0.1423\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1009 - root_mean_squared_error: 0.1423\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1009 - root_mean_squared_error: 0.1423\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1009 - root_mean_squared_error: 0.1422\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1008 - root_mean_squared_error: 0.1422\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1008 - root_mean_squared_error: 0.1422\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1008 - root_mean_squared_error: 0.1422\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1008 - root_mean_squared_error: 0.1422\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1008 - root_mean_squared_error: 0.1422\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1007 - root_mean_squared_error: 0.1421\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1007 - root_mean_squared_error: 0.1421\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1007 - root_mean_squared_error: 0.1421\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1007 - root_mean_squared_error: 0.1421\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1007 - root_mean_squared_error: 0.1421\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1006 - root_mean_squared_error: 0.1421\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1006 - root_mean_squared_error: 0.1420\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1006 - root_mean_squared_error: 0.1420\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1006 - root_mean_squared_error: 0.1420\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1006 - root_mean_squared_error: 0.1420\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1006 - root_mean_squared_error: 0.1420\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1005 - root_mean_squared_error: 0.1420\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1005 - root_mean_squared_error: 0.1420\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1005 - root_mean_squared_error: 0.1419\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1005 - root_mean_squared_error: 0.1419\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1005 - root_mean_squared_error: 0.1419\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1005 - root_mean_squared_error: 0.1419\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1004 - root_mean_squared_error: 0.1419\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1004 - root_mean_squared_error: 0.1419\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1004 - root_mean_squared_error: 0.1419\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1004 - root_mean_squared_error: 0.1419\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1004 - root_mean_squared_error: 0.1419\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1004 - root_mean_squared_error: 0.1418\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1004 - root_mean_squared_error: 0.1418\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1003 - root_mean_squared_error: 0.1418\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1003 - root_mean_squared_error: 0.1418\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1003 - root_mean_squared_error: 0.1418\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1003 - root_mean_squared_error: 0.1418\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1003 - root_mean_squared_error: 0.1418\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1003 - root_mean_squared_error: 0.1418\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1003 - root_mean_squared_error: 0.1418\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1003 - root_mean_squared_error: 0.1417\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1002 - root_mean_squared_error: 0.1417\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1002 - root_mean_squared_error: 0.1417\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1002 - root_mean_squared_error: 0.1417\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1002 - root_mean_squared_error: 0.1417\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1002 - root_mean_squared_error: 0.1417\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1002 - root_mean_squared_error: 0.1417\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1002 - root_mean_squared_error: 0.1417\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1002 - root_mean_squared_error: 0.1417\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1002 - root_mean_squared_error: 0.1417\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1002 - root_mean_squared_error: 0.1417\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1002 - root_mean_squared_error: 0.1417\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1416\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1415\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1415\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1415\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1415\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1415\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1000 - root_mean_squared_error: 0.1414\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1001 - root_mean_squared_error: 0.1414\n"
     ]
    }
   ],
   "source": [
    "X_train_assembled = np.stack([Y_train_pred_1, Y_train_pred_2], axis=2)\n",
    "Y_train_assembled = Y_train  # output the target of the first model\n",
    "X_test_assembled = np.stack([Y_test_pred_1, Y_test_pred_2], axis=2)\n",
    "Y_test_assembled_real = Y_test_real\n",
    "\n",
    "batch_size_train = len(X_train_assembled)\n",
    "batch_size_test = len(X_test_assembled)\n",
    "\n",
    "model_generator = get_assemble\n",
    "\n",
    "model_train = model_generator(batch_input_shape=(batch_size_train, n_forecast, 2))\n",
    "model_train.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model_train.fit(X_train_assembled, Y_train_assembled, batch_size=batch_size_train, epochs=epochs)\n",
    "Y_train_assembled_pred = model_train.predict(X_train_assembled, batch_size=batch_size_train)\n",
    "\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_test, n_forecast, 2))\n",
    "model_prediction.set_weights(model_train.get_weights())\n",
    "Y_test_assembled_pred = model_prediction.predict(X_test_assembled, batch_size=batch_size_test)\n",
    "# unscale and unpad the data\n",
    "Y_test_assembled_pred_unscaled = dg_2.inverse_transform_y(Y_test_assembled_pred, idx=test_idx, geo=dg.loc_init)\n",
    "Y_test_assembled_pred_real = dg.remove_padded_y(Y_test_assembled_pred_unscaled, idx=test_idx, geo=dg.loc_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factor t+1 = -0.0000\n",
      "factor t+2 = -0.0000\n",
      "factor t+3 = -0.0000\n",
      "factor t+4 = -0.0000\n",
      "factor t+5 = -0.0000\n",
      "factor t+6 = -0.0000\n",
      "factor t+7 = -0.0000\n",
      "factor t+8 = -0.0000\n",
      "factor t+9 = 0.0211\n",
      "factor t+10 = 0.2000\n",
      "factor t+11 = -0.0000\n",
      "factor t+12 = 0.0108\n",
      "factor t+13 = 0.1821\n",
      "factor t+14 = 0.1240\n",
      "factor t+15 = 0.0273\n",
      "factor t+16 = 0.1335\n",
      "factor t+17 = 0.1739\n",
      "factor t+18 = 0.1423\n",
      "factor t+19 = 0.1470\n",
      "factor t+20 = 0.1803\n"
     ]
    }
   ],
   "source": [
    "factors = model_prediction.weights[0].numpy().reshape(n_forecast)\n",
    "for t in range(n_forecast):\n",
    "    print(f'factor t+{t+1} = {factors[t]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE(t+1)</th>\n",
       "      <th>MSE(t+1)</th>\n",
       "      <th>MAE(t+2)</th>\n",
       "      <th>MSE(t+2)</th>\n",
       "      <th>MAE(t+3)</th>\n",
       "      <th>MSE(t+3)</th>\n",
       "      <th>MAE(t+4)</th>\n",
       "      <th>MSE(t+4)</th>\n",
       "      <th>MAE(t+5)</th>\n",
       "      <th>MSE(t+5)</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE(t+16)</th>\n",
       "      <th>MSE(t+16)</th>\n",
       "      <th>MAE(t+17)</th>\n",
       "      <th>MSE(t+17)</th>\n",
       "      <th>MAE(t+18)</th>\n",
       "      <th>MSE(t+18)</th>\n",
       "      <th>MAE(t+19)</th>\n",
       "      <th>MSE(t+19)</th>\n",
       "      <th>MAE(t+20)</th>\n",
       "      <th>MSE(t+20)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model 1</th>\n",
       "      <td>46.301735</td>\n",
       "      <td>4519.842861</td>\n",
       "      <td>53.660593</td>\n",
       "      <td>6037.545269</td>\n",
       "      <td>57.017152</td>\n",
       "      <td>6804.437714</td>\n",
       "      <td>92.142339</td>\n",
       "      <td>17386.522419</td>\n",
       "      <td>99.520317</td>\n",
       "      <td>21496.896407</td>\n",
       "      <td>...</td>\n",
       "      <td>192.759223</td>\n",
       "      <td>81760.808359</td>\n",
       "      <td>203.040257</td>\n",
       "      <td>92316.238270</td>\n",
       "      <td>170.655297</td>\n",
       "      <td>67699.264536</td>\n",
       "      <td>176.040426</td>\n",
       "      <td>72553.914441</td>\n",
       "      <td>187.205886</td>\n",
       "      <td>82463.686242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model trends</th>\n",
       "      <td>489.610372</td>\n",
       "      <td>422401.105809</td>\n",
       "      <td>490.480402</td>\n",
       "      <td>425800.190226</td>\n",
       "      <td>472.158875</td>\n",
       "      <td>398935.995617</td>\n",
       "      <td>468.150494</td>\n",
       "      <td>404959.783940</td>\n",
       "      <td>464.351435</td>\n",
       "      <td>381828.321917</td>\n",
       "      <td>...</td>\n",
       "      <td>449.631476</td>\n",
       "      <td>413856.249018</td>\n",
       "      <td>453.291030</td>\n",
       "      <td>422988.612215</td>\n",
       "      <td>459.944266</td>\n",
       "      <td>440231.149542</td>\n",
       "      <td>465.275833</td>\n",
       "      <td>457639.884950</td>\n",
       "      <td>459.667272</td>\n",
       "      <td>448504.637763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assembled</th>\n",
       "      <td>46.301735</td>\n",
       "      <td>4519.842861</td>\n",
       "      <td>53.660593</td>\n",
       "      <td>6037.545269</td>\n",
       "      <td>57.017152</td>\n",
       "      <td>6804.437714</td>\n",
       "      <td>92.142339</td>\n",
       "      <td>17386.522419</td>\n",
       "      <td>99.520317</td>\n",
       "      <td>21496.896407</td>\n",
       "      <td>...</td>\n",
       "      <td>216.905374</td>\n",
       "      <td>101453.374895</td>\n",
       "      <td>233.867003</td>\n",
       "      <td>119913.462140</td>\n",
       "      <td>197.287998</td>\n",
       "      <td>87107.050618</td>\n",
       "      <td>204.577251</td>\n",
       "      <td>95000.988559</td>\n",
       "      <td>221.062858</td>\n",
       "      <td>112156.604992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                MAE(t+1)       MSE(t+1)    MAE(t+2)       MSE(t+2)  \\\n",
       "model                                                                \n",
       "model 1        46.301735    4519.842861   53.660593    6037.545269   \n",
       "model trends  489.610372  422401.105809  490.480402  425800.190226   \n",
       "assembled      46.301735    4519.842861   53.660593    6037.545269   \n",
       "\n",
       "                MAE(t+3)       MSE(t+3)    MAE(t+4)       MSE(t+4)  \\\n",
       "model                                                                \n",
       "model 1        57.017152    6804.437714   92.142339   17386.522419   \n",
       "model trends  472.158875  398935.995617  468.150494  404959.783940   \n",
       "assembled      57.017152    6804.437714   92.142339   17386.522419   \n",
       "\n",
       "                MAE(t+5)       MSE(t+5)  ...   MAE(t+16)      MSE(t+16)  \\\n",
       "model                                    ...                              \n",
       "model 1        99.520317   21496.896407  ...  192.759223   81760.808359   \n",
       "model trends  464.351435  381828.321917  ...  449.631476  413856.249018   \n",
       "assembled      99.520317   21496.896407  ...  216.905374  101453.374895   \n",
       "\n",
       "               MAE(t+17)      MSE(t+17)   MAE(t+18)      MSE(t+18)  \\\n",
       "model                                                                \n",
       "model 1       203.040257   92316.238270  170.655297   67699.264536   \n",
       "model trends  453.291030  422988.612215  459.944266  440231.149542   \n",
       "assembled     233.867003  119913.462140  197.287998   87107.050618   \n",
       "\n",
       "               MAE(t+19)      MSE(t+19)   MAE(t+20)      MSE(t+20)  \n",
       "model                                                               \n",
       "model 1       176.040426   72553.914441  187.205886   82463.686242  \n",
       "model trends  465.275833  457639.884950  459.667272  448504.637763  \n",
       "assembled     204.577251   95000.988559  221.062858  112156.604992  \n",
       "\n",
       "[3 rows x 40 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_1 = compute_error(Y_test_pred_real_1, Y_test_real)\n",
    "error_1['model'] = 'model 1'\n",
    "error_2 = compute_error(Y_test_pred_real_2, Y_test_real_2)\n",
    "error_2['model'] = 'model trends'\n",
    "error_assembled = compute_error(Y_test_assembled_pred_real, Y_test_assembled_real)\n",
    "error_assembled['model'] = 'assembled'\n",
    "pd.concat([error_1, error_2, error_assembled]).set_index('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_assembly(model_generator_1: callable, dg_1: util.DataGenerator,\n",
    "                          model_generator_2: callable, dg_2: util.DataGenerator,\n",
    "                          model_generator_as: callable, \n",
    "                          nb_fit_first: int, nb_valid: int, nb_test: int, verbose: int = 1,\n",
    "                          return_history: bool = False, batch_size_fun: callable = None,\n",
    "                          es_stop_val: bool = False, epochs: int = 200,\n",
    "                          train_val: bool = False) -> Union[pd.DataFrame, List[History]]:\n",
    "    \"\"\"\n",
    "    evaluates a model using a walk forward evaluation: multiples fit are done, each followed by at most nb_test\n",
    "    to evaluate the model\n",
    "    :param model_generator_1: function returning the first model to evaluate\n",
    "    :param target_1: target for the first model\n",
    "    :param data_columns_1: data columns for the first model\n",
    "    :param model_generator_2: function returning the second model to evaluate\n",
    "    :param target_2: target for the second model\n",
    "    :param data_columns_2: data columns for the second model\n",
    "    :param model_generator_as: function returning the assembly model\n",
    "    :param nb_fit_first: number of datapoints used for the first fit\n",
    "    :param nb_valid: number of datapoints used for each validation set\n",
    "    :param nb_test: number of datapoints used for the test set (at most)\n",
    "    :param epochs: number of epochs used on each fit\n",
    "    :param verbose: verbose level. Passed to fit and used to display the error dataframe\n",
    "    :param return_history: if True, returns the list of history of each walk\n",
    "    :param batch_size_fun: function used to compute the batch size based on the X_train tensor\n",
    "        if not specified, default to batch_size = len(train_idx)\n",
    "    :param train_val: if True, the assembler will be trained using the validation set instead of the test set\n",
    "    \"\"\"\n",
    "    # initial index used\n",
    "    max_len = dg.batch_size\n",
    "    train_idx = np.arange(nb_fit_first)\n",
    "    valid_idx = np.arange(nb_fit_first, nb_fit_first + nb_valid)\n",
    "    if nb_fit_first + nb_valid >= max_len:\n",
    "        finished = True  # no test can be done\n",
    "    else:\n",
    "        finished = False  # a test set can be created\n",
    "        test_idx = np.arange(nb_fit_first + nb_valid, min(nb_fit_first + nb_valid + nb_test, max_len))\n",
    "    df_error = pd.DataFrame()\n",
    "    walk = 0\n",
    "    last_iter = False  # True when the last iteration is reached\n",
    "    all_history = []\n",
    "    \n",
    "    list_dg = [dg_1, dg_2]\n",
    "    \n",
    "    while not finished:\n",
    "        X_train, Y_train, pred_train = [], [], []\n",
    "        X_val, Y_val_unscaled, Y_val_real, pred_val = [], [], [], []\n",
    "        X_test, Y_test_unscaled, Y_test_real = [], [], []\n",
    "        pred_test, pred_test_unscaled, pred_test_real = [], [], []\n",
    "        \n",
    "        # train the two first models\n",
    "        for i, model_generator in enumerate([model_generator_1, model_generator_2]):\n",
    "            dg_i = list_dg[i]\n",
    "            X_train.append(dg_i.get_x(train_idx, scaled=True))\n",
    "            Y_train.append(dg_i.get_y(train_idx, scaled=True))\n",
    "\n",
    "            if len(valid_idx) > 0:  # validation set: validation logger and early stop if needed\n",
    "                X_val.append(dg_i.get_x(valid_idx, geo=dg_i.loc_init, scaled=True, use_previous_scaler=True))\n",
    "                Y_val_unscaled.append(dg_i.get_y(valid_idx, geo=dg_i.loc_init, scaled=False))\n",
    "                Y_val_real.append(dg_i.remove_padded_y(Y_val_unscaled[-1], idx=valid_idx, geo=dg_i.loc_init))\n",
    "                batch_size_val = len(X_val[-1])\n",
    "                model_validation = model_generator(batch_input_shape=(batch_size_val, n_samples, dg_i.n_features))\n",
    "                val_log = ValidationLogger(model_validation, batch_size_val, X_val[-1], Y_val_real[-1], \n",
    "                                             valid_idx, dg_i.loc_init)\n",
    "                if es_stop_val:\n",
    "                    callbacks = [val_log, EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=25)]\n",
    "                else:\n",
    "                    callbacks = [val_log]\n",
    "            else:\n",
    "                callbacks = None            \n",
    "\n",
    "            # test set\n",
    "            X_test.append(dg_i.get_x(test_idx, scaled=True, geo=dg_i.loc_init, use_previous_scaler=True))\n",
    "            Y_test_unscaled.append(dg_i.get_y(test_idx, scaled=False, geo=dg_i.loc_init))\n",
    "            Y_test_real.append(dg_i.remove_padded_y(Y_test_unscaled[-1], idx=test_idx, geo=dg_i.loc_init))\n",
    "\n",
    "            batch_size_train = len(X_train[-1])\n",
    "            batch_size_test = len(X_test[-1])\n",
    "\n",
    "            # training of model 1\n",
    "            model_train = model_generator(batch_input_shape=(batch_size_train, dg_i.n_samples, dg_i.n_features))\n",
    "            history = model_train.fit(X_train[-1], Y_train[-1], batch_size=batch_size_train, epochs=epochs, \n",
    "                                      callbacks=callbacks, verbose=verbose)\n",
    "            # prediction on the training set\n",
    "            pred_train.append(model_train.predict(X_train[-1], batch_size=batch_size_train))\n",
    "            # prediction on the validation set\n",
    "            if train_val:\n",
    "                model_validation.set_weights(model_train.get_weights())\n",
    "                pred_val.append(model_validation.predict(X_val[-1], batch_size=batch_size_val))\n",
    "            # prediction on the test set\n",
    "            model_prediction = model_generator(batch_input_shape=(batch_size_test, n_samples, dg_i.n_features))\n",
    "            model_prediction.set_weights(model_train.get_weights())\n",
    "            pred_test.append(model_prediction.predict(X_test[-1], batch_size=batch_size_test))\n",
    "            pred_test_unscaled.append(dg_i.inverse_transform_y(pred_test[-1], idx=test_idx, geo=dg_i.loc_init))\n",
    "            pred_test_real.append(dg_i.remove_padded_y(pred_test_unscaled[-1], idx=test_idx, geo=dg_i.loc_init))\n",
    "        \n",
    "        # train the assembly model\n",
    "        if train_val:\n",
    "            X_train_as = np.stack(pred_val, axis=2)\n",
    "            Y_train_as = Y_val_unscaled[0]  # output the target of the first model\n",
    "        else:\n",
    "            X_train_as = np.stack(pred_train, axis=2)\n",
    "            Y_train_as = Y_train[0]  # output the target of the first model\n",
    "        X_test_as = np.stack(pred_test, axis=2)\n",
    "        Y_test_as_real = Y_test_real[0]\n",
    "\n",
    "        batch_size_train = len(X_train_as)\n",
    "        batch_size_test = len(X_test_as)\n",
    "        dg_as = list_dg[0]  # used for inverse scaling and unpadding\n",
    "\n",
    "        model_train = model_generator_as(batch_input_shape=(batch_size_train, n_forecast, 2))\n",
    "        model_train.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                              metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "        history = model_train.fit(X_train_as, Y_train_as, batch_size=batch_size_train, epochs=epochs, \n",
    "                                  verbose=verbose)\n",
    "        pred_train_as = model_train.predict(X_train_as, batch_size=batch_size_train)\n",
    "\n",
    "        model_prediction = model_generator_as(batch_input_shape=(batch_size_test, n_forecast, 2))\n",
    "        model_prediction.set_weights(model_train.get_weights())\n",
    "        pred_test_as = model_prediction.predict(X_test_as, batch_size=batch_size_test)\n",
    "        # unscale and unpad the data\n",
    "        pred_test_as_unscaled = dg_as.inverse_transform_y(pred_test_as, idx=test_idx, geo=dg_as.loc_init)\n",
    "        pred_test_as_real = dg_as.remove_padded_y(pred_test_as_unscaled, idx=test_idx, geo=dg_as.loc_init)\n",
    "        \n",
    "        \"\"\"\n",
    "        if return_history:\n",
    "            # add test metrics to the history for this walk, based on the unpadded unscaled data \n",
    "            # except for the loss\n",
    "            for metric in model_train.metrics:\n",
    "                metric.reset_states()\n",
    "                if metric.name == 'loss':\n",
    "                    metric.update_state(Y_predicted_real, Y_test_real)\n",
    "                else:\n",
    "                    # compute metric accross each horizon\n",
    "                    for i in range(n_forecast):\n",
    "                        metric.update_state(Y_predicted_real[:, i], Y_test_real[:, i])\n",
    "                        history.history[f\"test_{metric.name}(t+{i+1})\"] = [metric.result().numpy()]\n",
    "                        metric.reset_states()\n",
    "                    # compute mean of metric on all horizon\n",
    "                    metric.update_state(Y_predicted_real, Y_test_real)\n",
    "                # prepend name for test set\n",
    "                history.history[f\"test_{metric.name}\"] = [metric.result().numpy()]\n",
    "            # add number of unpadded datapoints\n",
    "            history.history['nb_test_datapoints'] = [len(Y_test_real)]\n",
    "            all_history.append(history)\n",
    "        \"\"\"\n",
    "        if not return_history or verbose != 0:\n",
    "            # compute the error using the unpadded and unscaled data\n",
    "            for i in range(3):\n",
    "                if i == 2:  # assembled model\n",
    "                    error = compute_error(Y_test_as_real, pred_test_as_real)\n",
    "                    name = f'walk {walk + 1} model assembled'\n",
    "                    error['nb_test_datapoints'] = len(Y_test_as_real)\n",
    "                else:  # two first model\n",
    "                    error = compute_error(Y_test_real[i], pred_test_real[i])\n",
    "                    name = f'walk {walk + 1} model {i+1}'\n",
    "                    error['nb_test_datapoints'] = len(Y_test_real[i])\n",
    "                error['name'] = name\n",
    "                error['days_train'] = len(train_idx)\n",
    "                error['days_valid'] = len(valid_idx)\n",
    "                error['days_test'] = len(test_idx)\n",
    "                error = error.set_index('name')\n",
    "                df_error = df_error.append(error)\n",
    "        \n",
    "        if verbose != 0:\n",
    "            display(df_error)\n",
    "        if last_iter:\n",
    "            finished = True\n",
    "        # indexes for next fit\n",
    "        train_idx = np.arange(train_idx[-1] + 1 + nb_test)\n",
    "        valid_idx += nb_test\n",
    "        if test_idx[-1] + nb_test >= max_len:  # last iteration, less points can be used for the test set\n",
    "            last_iter = True  # last iteration to be done\n",
    "            test_idx = np.arange(test_idx[-1], max_len)\n",
    "        else:\n",
    "            test_idx += nb_test\n",
    "        walk += 1\n",
    "    if not return_history:  # compute the mean across all walks\n",
    "        len_df = len(df_error)\n",
    "        df_error.loc['mean model 1'] = df_error.iloc[range(0, len_df, 3)].mean()\n",
    "        df_error.loc['mean model 2'] = df_error.iloc[range(1, len_df, 3)].mean()\n",
    "        df_error.loc['mean model as'] = df_error.iloc[range(2, len_df, 3)].mean()\n",
    "    return df_error if not return_history else all_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE(t+1)</th>\n",
       "      <th>MSE(t+1)</th>\n",
       "      <th>MAE(t+2)</th>\n",
       "      <th>MSE(t+2)</th>\n",
       "      <th>MAE(t+3)</th>\n",
       "      <th>MSE(t+3)</th>\n",
       "      <th>MAE(t+4)</th>\n",
       "      <th>MSE(t+4)</th>\n",
       "      <th>MAE(t+5)</th>\n",
       "      <th>MSE(t+5)</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE(t+18)</th>\n",
       "      <th>MSE(t+18)</th>\n",
       "      <th>MAE(t+19)</th>\n",
       "      <th>MSE(t+19)</th>\n",
       "      <th>MAE(t+20)</th>\n",
       "      <th>MSE(t+20)</th>\n",
       "      <th>nb_test_datapoints</th>\n",
       "      <th>days_train</th>\n",
       "      <th>days_valid</th>\n",
       "      <th>days_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>walk 1 model 1</th>\n",
       "      <td>505.729054</td>\n",
       "      <td>6.495597e+05</td>\n",
       "      <td>367.698798</td>\n",
       "      <td>3.314301e+05</td>\n",
       "      <td>287.733333</td>\n",
       "      <td>2.582611e+05</td>\n",
       "      <td>546.258443</td>\n",
       "      <td>8.225574e+05</td>\n",
       "      <td>906.823279</td>\n",
       "      <td>1.867693e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>373.337518</td>\n",
       "      <td>2.849747e+05</td>\n",
       "      <td>305.817006</td>\n",
       "      <td>2.305208e+05</td>\n",
       "      <td>1108.105145</td>\n",
       "      <td>2.481657e+06</td>\n",
       "      <td>690.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 1 model 2</th>\n",
       "      <td>1391.445220</td>\n",
       "      <td>4.391445e+06</td>\n",
       "      <td>1343.777545</td>\n",
       "      <td>4.013385e+06</td>\n",
       "      <td>1345.440652</td>\n",
       "      <td>4.036105e+06</td>\n",
       "      <td>1041.435222</td>\n",
       "      <td>2.408800e+06</td>\n",
       "      <td>1401.400006</td>\n",
       "      <td>4.337240e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>778.828143</td>\n",
       "      <td>1.173737e+06</td>\n",
       "      <td>946.891243</td>\n",
       "      <td>1.822219e+06</td>\n",
       "      <td>874.493834</td>\n",
       "      <td>1.457478e+06</td>\n",
       "      <td>690.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 1 model assembled</th>\n",
       "      <td>505.729054</td>\n",
       "      <td>6.495597e+05</td>\n",
       "      <td>377.837689</td>\n",
       "      <td>3.468586e+05</td>\n",
       "      <td>257.448125</td>\n",
       "      <td>1.704215e+05</td>\n",
       "      <td>543.114882</td>\n",
       "      <td>8.090072e+05</td>\n",
       "      <td>997.831722</td>\n",
       "      <td>2.239625e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>372.727778</td>\n",
       "      <td>2.816934e+05</td>\n",
       "      <td>294.536576</td>\n",
       "      <td>1.809401e+05</td>\n",
       "      <td>1061.104396</td>\n",
       "      <td>2.243174e+06</td>\n",
       "      <td>690.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model 1</th>\n",
       "      <td>291.021760</td>\n",
       "      <td>2.342361e+05</td>\n",
       "      <td>1505.612206</td>\n",
       "      <td>4.349818e+06</td>\n",
       "      <td>171.640618</td>\n",
       "      <td>6.541077e+04</td>\n",
       "      <td>2627.766763</td>\n",
       "      <td>1.369565e+07</td>\n",
       "      <td>218.472706</td>\n",
       "      <td>1.284775e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>143.291524</td>\n",
       "      <td>3.564636e+04</td>\n",
       "      <td>304.719407</td>\n",
       "      <td>1.874394e+05</td>\n",
       "      <td>1927.646953</td>\n",
       "      <td>6.955810e+06</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model 2</th>\n",
       "      <td>855.737703</td>\n",
       "      <td>1.607352e+06</td>\n",
       "      <td>782.806488</td>\n",
       "      <td>1.330130e+06</td>\n",
       "      <td>1174.951677</td>\n",
       "      <td>2.590533e+06</td>\n",
       "      <td>877.761759</td>\n",
       "      <td>1.617189e+06</td>\n",
       "      <td>702.588339</td>\n",
       "      <td>1.103183e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>835.200132</td>\n",
       "      <td>1.373516e+06</td>\n",
       "      <td>1347.151070</td>\n",
       "      <td>3.195768e+06</td>\n",
       "      <td>1227.133184</td>\n",
       "      <td>2.679594e+06</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model assembled</th>\n",
       "      <td>177.524717</td>\n",
       "      <td>8.842340e+04</td>\n",
       "      <td>1411.958880</td>\n",
       "      <td>3.858575e+06</td>\n",
       "      <td>317.541731</td>\n",
       "      <td>2.045684e+05</td>\n",
       "      <td>2568.513971</td>\n",
       "      <td>1.309135e+07</td>\n",
       "      <td>193.574692</td>\n",
       "      <td>9.313355e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>142.618120</td>\n",
       "      <td>3.480660e+04</td>\n",
       "      <td>304.719407</td>\n",
       "      <td>1.874394e+05</td>\n",
       "      <td>1917.795324</td>\n",
       "      <td>6.880855e+06</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model 1</th>\n",
       "      <td>1910.884468</td>\n",
       "      <td>6.667652e+06</td>\n",
       "      <td>824.999120</td>\n",
       "      <td>1.331562e+06</td>\n",
       "      <td>1039.749117</td>\n",
       "      <td>1.980204e+06</td>\n",
       "      <td>2211.926907</td>\n",
       "      <td>9.000141e+06</td>\n",
       "      <td>3560.623825</td>\n",
       "      <td>2.350303e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>309.331914</td>\n",
       "      <td>2.040848e+05</td>\n",
       "      <td>1393.034758</td>\n",
       "      <td>3.561685e+06</td>\n",
       "      <td>1535.475131</td>\n",
       "      <td>4.278128e+06</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model 2</th>\n",
       "      <td>926.832844</td>\n",
       "      <td>1.622634e+06</td>\n",
       "      <td>924.541521</td>\n",
       "      <td>1.637535e+06</td>\n",
       "      <td>1006.655671</td>\n",
       "      <td>1.874619e+06</td>\n",
       "      <td>709.833043</td>\n",
       "      <td>1.067497e+06</td>\n",
       "      <td>915.665302</td>\n",
       "      <td>1.603025e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>881.143818</td>\n",
       "      <td>1.556287e+06</td>\n",
       "      <td>1623.210785</td>\n",
       "      <td>4.496581e+06</td>\n",
       "      <td>804.738286</td>\n",
       "      <td>1.347325e+06</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model assembled</th>\n",
       "      <td>1896.967443</td>\n",
       "      <td>6.572168e+06</td>\n",
       "      <td>800.686504</td>\n",
       "      <td>1.255881e+06</td>\n",
       "      <td>1037.457259</td>\n",
       "      <td>1.971550e+06</td>\n",
       "      <td>1940.505829</td>\n",
       "      <td>6.981097e+06</td>\n",
       "      <td>3524.381629</td>\n",
       "      <td>2.302833e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>419.878856</td>\n",
       "      <td>3.595604e+05</td>\n",
       "      <td>1393.034758</td>\n",
       "      <td>3.561685e+06</td>\n",
       "      <td>1382.399802</td>\n",
       "      <td>3.531937e+06</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model 1</th>\n",
       "      <td>847.619484</td>\n",
       "      <td>1.300956e+06</td>\n",
       "      <td>1716.170359</td>\n",
       "      <td>5.490841e+06</td>\n",
       "      <td>1564.656900</td>\n",
       "      <td>4.685261e+06</td>\n",
       "      <td>91.433028</td>\n",
       "      <td>2.361894e+04</td>\n",
       "      <td>218.332242</td>\n",
       "      <td>7.184164e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>2054.282037</td>\n",
       "      <td>8.457605e+06</td>\n",
       "      <td>571.356656</td>\n",
       "      <td>8.158491e+05</td>\n",
       "      <td>246.262811</td>\n",
       "      <td>2.213600e+05</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model 2</th>\n",
       "      <td>991.964965</td>\n",
       "      <td>2.157063e+06</td>\n",
       "      <td>1073.990788</td>\n",
       "      <td>2.224166e+06</td>\n",
       "      <td>1122.736995</td>\n",
       "      <td>2.279685e+06</td>\n",
       "      <td>1190.798200</td>\n",
       "      <td>2.508173e+06</td>\n",
       "      <td>1134.871086</td>\n",
       "      <td>2.310398e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1149.594357</td>\n",
       "      <td>2.615537e+06</td>\n",
       "      <td>1086.333443</td>\n",
       "      <td>2.508667e+06</td>\n",
       "      <td>1254.204508</td>\n",
       "      <td>3.091365e+06</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model assembled</th>\n",
       "      <td>866.458913</td>\n",
       "      <td>1.419481e+06</td>\n",
       "      <td>1707.621895</td>\n",
       "      <td>5.437084e+06</td>\n",
       "      <td>1558.604453</td>\n",
       "      <td>4.644485e+06</td>\n",
       "      <td>238.282344</td>\n",
       "      <td>1.349922e+05</td>\n",
       "      <td>130.231953</td>\n",
       "      <td>3.788976e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>2011.011000</td>\n",
       "      <td>8.095803e+06</td>\n",
       "      <td>571.356656</td>\n",
       "      <td>8.158491e+05</td>\n",
       "      <td>407.457288</td>\n",
       "      <td>4.684729e+05</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model 1</th>\n",
       "      <td>1567.037428</td>\n",
       "      <td>5.218093e+06</td>\n",
       "      <td>1280.539743</td>\n",
       "      <td>3.505202e+06</td>\n",
       "      <td>918.858736</td>\n",
       "      <td>1.859540e+06</td>\n",
       "      <td>82.999594</td>\n",
       "      <td>2.234444e+04</td>\n",
       "      <td>599.768851</td>\n",
       "      <td>9.534962e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>335.532761</td>\n",
       "      <td>3.016101e+05</td>\n",
       "      <td>819.154688</td>\n",
       "      <td>1.723126e+06</td>\n",
       "      <td>1479.974544</td>\n",
       "      <td>5.224067e+06</td>\n",
       "      <td>322.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model 2</th>\n",
       "      <td>584.747283</td>\n",
       "      <td>9.826039e+05</td>\n",
       "      <td>895.035211</td>\n",
       "      <td>2.111186e+06</td>\n",
       "      <td>663.156067</td>\n",
       "      <td>1.419572e+06</td>\n",
       "      <td>617.283078</td>\n",
       "      <td>1.197121e+06</td>\n",
       "      <td>656.654726</td>\n",
       "      <td>1.431076e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>718.237489</td>\n",
       "      <td>1.719102e+06</td>\n",
       "      <td>1111.966105</td>\n",
       "      <td>3.580778e+06</td>\n",
       "      <td>1190.966891</td>\n",
       "      <td>3.561910e+06</td>\n",
       "      <td>322.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model assembled</th>\n",
       "      <td>1346.722234</td>\n",
       "      <td>4.071888e+06</td>\n",
       "      <td>1203.029203</td>\n",
       "      <td>3.188175e+06</td>\n",
       "      <td>819.624656</td>\n",
       "      <td>1.693803e+06</td>\n",
       "      <td>84.777487</td>\n",
       "      <td>2.481240e+04</td>\n",
       "      <td>597.336304</td>\n",
       "      <td>9.553434e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>363.235707</td>\n",
       "      <td>4.564412e+05</td>\n",
       "      <td>873.833261</td>\n",
       "      <td>2.010395e+06</td>\n",
       "      <td>1475.927120</td>\n",
       "      <td>5.198336e+06</td>\n",
       "      <td>322.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model 1</th>\n",
       "      <td>1024.458439</td>\n",
       "      <td>2.814099e+06</td>\n",
       "      <td>1139.004045</td>\n",
       "      <td>3.001771e+06</td>\n",
       "      <td>796.527741</td>\n",
       "      <td>1.769735e+06</td>\n",
       "      <td>1112.076947</td>\n",
       "      <td>4.712863e+06</td>\n",
       "      <td>1100.804181</td>\n",
       "      <td>5.304908e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>643.155151</td>\n",
       "      <td>1.856784e+06</td>\n",
       "      <td>678.816503</td>\n",
       "      <td>1.303724e+06</td>\n",
       "      <td>1259.492917</td>\n",
       "      <td>3.832204e+06</td>\n",
       "      <td>616.4</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model 2</th>\n",
       "      <td>950.145603</td>\n",
       "      <td>2.152219e+06</td>\n",
       "      <td>1004.030311</td>\n",
       "      <td>2.263280e+06</td>\n",
       "      <td>1062.588212</td>\n",
       "      <td>2.440103e+06</td>\n",
       "      <td>887.422260</td>\n",
       "      <td>1.759756e+06</td>\n",
       "      <td>962.235892</td>\n",
       "      <td>2.156985e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>872.600788</td>\n",
       "      <td>1.687636e+06</td>\n",
       "      <td>1223.110529</td>\n",
       "      <td>3.120802e+06</td>\n",
       "      <td>1070.307341</td>\n",
       "      <td>2.427534e+06</td>\n",
       "      <td>616.4</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model as</th>\n",
       "      <td>958.680472</td>\n",
       "      <td>2.560304e+06</td>\n",
       "      <td>1100.226834</td>\n",
       "      <td>2.817315e+06</td>\n",
       "      <td>798.135245</td>\n",
       "      <td>1.736965e+06</td>\n",
       "      <td>1075.038902</td>\n",
       "      <td>4.208251e+06</td>\n",
       "      <td>1088.671260</td>\n",
       "      <td>5.270865e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>661.894292</td>\n",
       "      <td>1.845661e+06</td>\n",
       "      <td>687.496132</td>\n",
       "      <td>1.351262e+06</td>\n",
       "      <td>1248.936786</td>\n",
       "      <td>3.664555e+06</td>\n",
       "      <td>616.4</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           MAE(t+1)      MSE(t+1)     MAE(t+2)      MSE(t+2)  \\\n",
       "name                                                                           \n",
       "walk 1 model 1           505.729054  6.495597e+05   367.698798  3.314301e+05   \n",
       "walk 1 model 2          1391.445220  4.391445e+06  1343.777545  4.013385e+06   \n",
       "walk 1 model assembled   505.729054  6.495597e+05   377.837689  3.468586e+05   \n",
       "walk 2 model 1           291.021760  2.342361e+05  1505.612206  4.349818e+06   \n",
       "walk 2 model 2           855.737703  1.607352e+06   782.806488  1.330130e+06   \n",
       "walk 2 model assembled   177.524717  8.842340e+04  1411.958880  3.858575e+06   \n",
       "walk 3 model 1          1910.884468  6.667652e+06   824.999120  1.331562e+06   \n",
       "walk 3 model 2           926.832844  1.622634e+06   924.541521  1.637535e+06   \n",
       "walk 3 model assembled  1896.967443  6.572168e+06   800.686504  1.255881e+06   \n",
       "walk 4 model 1           847.619484  1.300956e+06  1716.170359  5.490841e+06   \n",
       "walk 4 model 2           991.964965  2.157063e+06  1073.990788  2.224166e+06   \n",
       "walk 4 model assembled   866.458913  1.419481e+06  1707.621895  5.437084e+06   \n",
       "walk 5 model 1          1567.037428  5.218093e+06  1280.539743  3.505202e+06   \n",
       "walk 5 model 2           584.747283  9.826039e+05   895.035211  2.111186e+06   \n",
       "walk 5 model assembled  1346.722234  4.071888e+06  1203.029203  3.188175e+06   \n",
       "mean model 1            1024.458439  2.814099e+06  1139.004045  3.001771e+06   \n",
       "mean model 2             950.145603  2.152219e+06  1004.030311  2.263280e+06   \n",
       "mean model as            958.680472  2.560304e+06  1100.226834  2.817315e+06   \n",
       "\n",
       "                           MAE(t+3)      MSE(t+3)     MAE(t+4)      MSE(t+4)  \\\n",
       "name                                                                           \n",
       "walk 1 model 1           287.733333  2.582611e+05   546.258443  8.225574e+05   \n",
       "walk 1 model 2          1345.440652  4.036105e+06  1041.435222  2.408800e+06   \n",
       "walk 1 model assembled   257.448125  1.704215e+05   543.114882  8.090072e+05   \n",
       "walk 2 model 1           171.640618  6.541077e+04  2627.766763  1.369565e+07   \n",
       "walk 2 model 2          1174.951677  2.590533e+06   877.761759  1.617189e+06   \n",
       "walk 2 model assembled   317.541731  2.045684e+05  2568.513971  1.309135e+07   \n",
       "walk 3 model 1          1039.749117  1.980204e+06  2211.926907  9.000141e+06   \n",
       "walk 3 model 2          1006.655671  1.874619e+06   709.833043  1.067497e+06   \n",
       "walk 3 model assembled  1037.457259  1.971550e+06  1940.505829  6.981097e+06   \n",
       "walk 4 model 1          1564.656900  4.685261e+06    91.433028  2.361894e+04   \n",
       "walk 4 model 2          1122.736995  2.279685e+06  1190.798200  2.508173e+06   \n",
       "walk 4 model assembled  1558.604453  4.644485e+06   238.282344  1.349922e+05   \n",
       "walk 5 model 1           918.858736  1.859540e+06    82.999594  2.234444e+04   \n",
       "walk 5 model 2           663.156067  1.419572e+06   617.283078  1.197121e+06   \n",
       "walk 5 model assembled   819.624656  1.693803e+06    84.777487  2.481240e+04   \n",
       "mean model 1             796.527741  1.769735e+06  1112.076947  4.712863e+06   \n",
       "mean model 2            1062.588212  2.440103e+06   887.422260  1.759756e+06   \n",
       "mean model as            798.135245  1.736965e+06  1075.038902  4.208251e+06   \n",
       "\n",
       "                           MAE(t+5)      MSE(t+5)  ...    MAE(t+18)  \\\n",
       "name                                               ...                \n",
       "walk 1 model 1           906.823279  1.867693e+06  ...   373.337518   \n",
       "walk 1 model 2          1401.400006  4.337240e+06  ...   778.828143   \n",
       "walk 1 model assembled   997.831722  2.239625e+06  ...   372.727778   \n",
       "walk 2 model 1           218.472706  1.284775e+05  ...   143.291524   \n",
       "walk 2 model 2           702.588339  1.103183e+06  ...   835.200132   \n",
       "walk 2 model assembled   193.574692  9.313355e+04  ...   142.618120   \n",
       "walk 3 model 1          3560.623825  2.350303e+07  ...   309.331914   \n",
       "walk 3 model 2           915.665302  1.603025e+06  ...   881.143818   \n",
       "walk 3 model assembled  3524.381629  2.302833e+07  ...   419.878856   \n",
       "walk 4 model 1           218.332242  7.184164e+04  ...  2054.282037   \n",
       "walk 4 model 2          1134.871086  2.310398e+06  ...  1149.594357   \n",
       "walk 4 model assembled   130.231953  3.788976e+04  ...  2011.011000   \n",
       "walk 5 model 1           599.768851  9.534962e+05  ...   335.532761   \n",
       "walk 5 model 2           656.654726  1.431076e+06  ...   718.237489   \n",
       "walk 5 model assembled   597.336304  9.553434e+05  ...   363.235707   \n",
       "mean model 1            1100.804181  5.304908e+06  ...   643.155151   \n",
       "mean model 2             962.235892  2.156985e+06  ...   872.600788   \n",
       "mean model as           1088.671260  5.270865e+06  ...   661.894292   \n",
       "\n",
       "                           MSE(t+18)    MAE(t+19)     MSE(t+19)    MAE(t+20)  \\\n",
       "name                                                                           \n",
       "walk 1 model 1          2.849747e+05   305.817006  2.305208e+05  1108.105145   \n",
       "walk 1 model 2          1.173737e+06   946.891243  1.822219e+06   874.493834   \n",
       "walk 1 model assembled  2.816934e+05   294.536576  1.809401e+05  1061.104396   \n",
       "walk 2 model 1          3.564636e+04   304.719407  1.874394e+05  1927.646953   \n",
       "walk 2 model 2          1.373516e+06  1347.151070  3.195768e+06  1227.133184   \n",
       "walk 2 model assembled  3.480660e+04   304.719407  1.874394e+05  1917.795324   \n",
       "walk 3 model 1          2.040848e+05  1393.034758  3.561685e+06  1535.475131   \n",
       "walk 3 model 2          1.556287e+06  1623.210785  4.496581e+06   804.738286   \n",
       "walk 3 model assembled  3.595604e+05  1393.034758  3.561685e+06  1382.399802   \n",
       "walk 4 model 1          8.457605e+06   571.356656  8.158491e+05   246.262811   \n",
       "walk 4 model 2          2.615537e+06  1086.333443  2.508667e+06  1254.204508   \n",
       "walk 4 model assembled  8.095803e+06   571.356656  8.158491e+05   407.457288   \n",
       "walk 5 model 1          3.016101e+05   819.154688  1.723126e+06  1479.974544   \n",
       "walk 5 model 2          1.719102e+06  1111.966105  3.580778e+06  1190.966891   \n",
       "walk 5 model assembled  4.564412e+05   873.833261  2.010395e+06  1475.927120   \n",
       "mean model 1            1.856784e+06   678.816503  1.303724e+06  1259.492917   \n",
       "mean model 2            1.687636e+06  1223.110529  3.120802e+06  1070.307341   \n",
       "mean model as           1.845661e+06   687.496132  1.351262e+06  1248.936786   \n",
       "\n",
       "                           MSE(t+20)  nb_test_datapoints  days_train  \\\n",
       "name                                                                   \n",
       "walk 1 model 1          2.481657e+06               690.0       220.0   \n",
       "walk 1 model 2          1.457478e+06               690.0       220.0   \n",
       "walk 1 model assembled  2.243174e+06               690.0       220.0   \n",
       "walk 2 model 1          6.955810e+06               690.0       250.0   \n",
       "walk 2 model 2          2.679594e+06               690.0       250.0   \n",
       "walk 2 model assembled  6.880855e+06               690.0       250.0   \n",
       "walk 3 model 1          4.278128e+06               690.0       280.0   \n",
       "walk 3 model 2          1.347325e+06               690.0       280.0   \n",
       "walk 3 model assembled  3.531937e+06               690.0       280.0   \n",
       "walk 4 model 1          2.213600e+05               690.0       310.0   \n",
       "walk 4 model 2          3.091365e+06               690.0       310.0   \n",
       "walk 4 model assembled  4.684729e+05               690.0       310.0   \n",
       "walk 5 model 1          5.224067e+06               322.0       340.0   \n",
       "walk 5 model 2          3.561910e+06               322.0       340.0   \n",
       "walk 5 model assembled  5.198336e+06               322.0       340.0   \n",
       "mean model 1            3.832204e+06               616.4       280.0   \n",
       "mean model 2            2.427534e+06               616.4       280.0   \n",
       "mean model as           3.664555e+06               616.4       280.0   \n",
       "\n",
       "                        days_valid  days_test  \n",
       "name                                           \n",
       "walk 1 model 1                30.0       30.0  \n",
       "walk 1 model 2                30.0       30.0  \n",
       "walk 1 model assembled        30.0       30.0  \n",
       "walk 2 model 1                30.0       30.0  \n",
       "walk 2 model 2                30.0       30.0  \n",
       "walk 2 model assembled        30.0       30.0  \n",
       "walk 3 model 1                30.0       30.0  \n",
       "walk 3 model 2                30.0       30.0  \n",
       "walk 3 model assembled        30.0       30.0  \n",
       "walk 4 model 1                30.0       30.0  \n",
       "walk 4 model 2                30.0       30.0  \n",
       "walk 4 model assembled        30.0       30.0  \n",
       "walk 5 model 1                30.0       14.0  \n",
       "walk 5 model 2                30.0       14.0  \n",
       "walk 5 model assembled        30.0       14.0  \n",
       "mean model 1                  30.0       26.8  \n",
       "mean model 2                  30.0       26.8  \n",
       "mean model as                 30.0       26.8  \n",
       "\n",
       "[18 rows x 44 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_all_inputs = lambda batch_input_shape: get_dense_model(batch_input_shape, use_lambda=False)\n",
    "\n",
    "df_errors_1 = walk_forward_assembly(get_dense_model, dg, dense_all_inputs, dg_2, get_assemble,\n",
    "                     220, 30, 30, epochs=10, verbose=0)\n",
    "df_errors_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 1004 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2c1c03f160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2bf866df70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 1004 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2be4473e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2c1c563310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 1004 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2bcc2860d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2c1c03cdc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 1004 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2bcc286550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2be4067790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 1004 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2bf8157b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE(t+1)</th>\n",
       "      <th>MSE(t+1)</th>\n",
       "      <th>MAE(t+2)</th>\n",
       "      <th>MSE(t+2)</th>\n",
       "      <th>MAE(t+3)</th>\n",
       "      <th>MSE(t+3)</th>\n",
       "      <th>MAE(t+4)</th>\n",
       "      <th>MSE(t+4)</th>\n",
       "      <th>MAE(t+5)</th>\n",
       "      <th>MSE(t+5)</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE(t+18)</th>\n",
       "      <th>MSE(t+18)</th>\n",
       "      <th>MAE(t+19)</th>\n",
       "      <th>MSE(t+19)</th>\n",
       "      <th>MAE(t+20)</th>\n",
       "      <th>MSE(t+20)</th>\n",
       "      <th>nb_test_datapoints</th>\n",
       "      <th>days_train</th>\n",
       "      <th>days_valid</th>\n",
       "      <th>days_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>walk 1 model 1</th>\n",
       "      <td>204.429573</td>\n",
       "      <td>1.331389e+05</td>\n",
       "      <td>205.129543</td>\n",
       "      <td>1.452423e+05</td>\n",
       "      <td>202.585008</td>\n",
       "      <td>1.270908e+05</td>\n",
       "      <td>197.969829</td>\n",
       "      <td>1.340822e+05</td>\n",
       "      <td>191.819972</td>\n",
       "      <td>1.262198e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>249.734384</td>\n",
       "      <td>125296.996112</td>\n",
       "      <td>256.339936</td>\n",
       "      <td>131194.444790</td>\n",
       "      <td>264.854176</td>\n",
       "      <td>136616.251551</td>\n",
       "      <td>690.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 1 model 2</th>\n",
       "      <td>857.449403</td>\n",
       "      <td>1.495423e+06</td>\n",
       "      <td>847.176601</td>\n",
       "      <td>1.448076e+06</td>\n",
       "      <td>835.923811</td>\n",
       "      <td>1.397476e+06</td>\n",
       "      <td>824.758933</td>\n",
       "      <td>1.346553e+06</td>\n",
       "      <td>811.446414</td>\n",
       "      <td>1.291361e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>674.533051</td>\n",
       "      <td>762345.293831</td>\n",
       "      <td>664.535309</td>\n",
       "      <td>733380.787768</td>\n",
       "      <td>656.679076</td>\n",
       "      <td>710527.486339</td>\n",
       "      <td>690.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 1 model assembled</th>\n",
       "      <td>201.955836</td>\n",
       "      <td>1.322176e+05</td>\n",
       "      <td>205.129543</td>\n",
       "      <td>1.452423e+05</td>\n",
       "      <td>199.594343</td>\n",
       "      <td>1.259375e+05</td>\n",
       "      <td>197.969829</td>\n",
       "      <td>1.340822e+05</td>\n",
       "      <td>191.819972</td>\n",
       "      <td>1.262198e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>249.088411</td>\n",
       "      <td>131389.363513</td>\n",
       "      <td>254.568326</td>\n",
       "      <td>137070.766181</td>\n",
       "      <td>262.835581</td>\n",
       "      <td>142791.064706</td>\n",
       "      <td>690.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model 1</th>\n",
       "      <td>52.007630</td>\n",
       "      <td>6.159798e+03</td>\n",
       "      <td>54.983300</td>\n",
       "      <td>6.696802e+03</td>\n",
       "      <td>68.911206</td>\n",
       "      <td>1.136799e+04</td>\n",
       "      <td>63.867185</td>\n",
       "      <td>8.187963e+03</td>\n",
       "      <td>73.878844</td>\n",
       "      <td>1.051365e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>288.565513</td>\n",
       "      <td>134055.002845</td>\n",
       "      <td>280.916750</td>\n",
       "      <td>124126.537738</td>\n",
       "      <td>295.172741</td>\n",
       "      <td>134822.216432</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model 2</th>\n",
       "      <td>670.180804</td>\n",
       "      <td>7.335546e+05</td>\n",
       "      <td>667.310006</td>\n",
       "      <td>7.271113e+05</td>\n",
       "      <td>665.244090</td>\n",
       "      <td>7.209573e+05</td>\n",
       "      <td>664.271445</td>\n",
       "      <td>7.189750e+05</td>\n",
       "      <td>661.604074</td>\n",
       "      <td>7.115761e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>641.832572</td>\n",
       "      <td>661392.908836</td>\n",
       "      <td>639.022650</td>\n",
       "      <td>651663.177974</td>\n",
       "      <td>637.856855</td>\n",
       "      <td>647534.203214</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model assembled</th>\n",
       "      <td>52.007630</td>\n",
       "      <td>6.159798e+03</td>\n",
       "      <td>54.983300</td>\n",
       "      <td>6.696802e+03</td>\n",
       "      <td>66.046014</td>\n",
       "      <td>1.031034e+04</td>\n",
       "      <td>63.867185</td>\n",
       "      <td>8.187963e+03</td>\n",
       "      <td>73.878844</td>\n",
       "      <td>1.051365e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>266.641956</td>\n",
       "      <td>114242.110146</td>\n",
       "      <td>264.741106</td>\n",
       "      <td>110616.636843</td>\n",
       "      <td>275.981683</td>\n",
       "      <td>118478.243034</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model 1</th>\n",
       "      <td>27.072408</td>\n",
       "      <td>1.723828e+03</td>\n",
       "      <td>22.036108</td>\n",
       "      <td>1.135663e+03</td>\n",
       "      <td>28.552819</td>\n",
       "      <td>1.925100e+03</td>\n",
       "      <td>34.695761</td>\n",
       "      <td>2.839635e+03</td>\n",
       "      <td>40.259983</td>\n",
       "      <td>4.027213e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>181.839963</td>\n",
       "      <td>61375.584389</td>\n",
       "      <td>192.251272</td>\n",
       "      <td>67272.050903</td>\n",
       "      <td>205.909921</td>\n",
       "      <td>76221.800874</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model 2</th>\n",
       "      <td>514.696630</td>\n",
       "      <td>3.736666e+05</td>\n",
       "      <td>518.534040</td>\n",
       "      <td>3.837259e+05</td>\n",
       "      <td>519.054807</td>\n",
       "      <td>3.882305e+05</td>\n",
       "      <td>519.897722</td>\n",
       "      <td>3.936795e+05</td>\n",
       "      <td>521.354873</td>\n",
       "      <td>3.999022e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>489.723424</td>\n",
       "      <td>420968.063969</td>\n",
       "      <td>486.550280</td>\n",
       "      <td>419020.725349</td>\n",
       "      <td>484.203095</td>\n",
       "      <td>421384.767956</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model assembled</th>\n",
       "      <td>27.072408</td>\n",
       "      <td>1.723828e+03</td>\n",
       "      <td>22.036108</td>\n",
       "      <td>1.135663e+03</td>\n",
       "      <td>28.552819</td>\n",
       "      <td>1.925100e+03</td>\n",
       "      <td>34.695761</td>\n",
       "      <td>2.839635e+03</td>\n",
       "      <td>40.259983</td>\n",
       "      <td>4.027213e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>175.504337</td>\n",
       "      <td>58867.934419</td>\n",
       "      <td>184.672965</td>\n",
       "      <td>64193.072188</td>\n",
       "      <td>196.658130</td>\n",
       "      <td>71922.696493</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model 1</th>\n",
       "      <td>18.096105</td>\n",
       "      <td>9.535292e+02</td>\n",
       "      <td>36.606444</td>\n",
       "      <td>4.368720e+03</td>\n",
       "      <td>33.972302</td>\n",
       "      <td>3.299719e+03</td>\n",
       "      <td>34.487282</td>\n",
       "      <td>4.648510e+03</td>\n",
       "      <td>33.937061</td>\n",
       "      <td>4.277822e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>115.547299</td>\n",
       "      <td>30936.309376</td>\n",
       "      <td>143.826924</td>\n",
       "      <td>48725.034722</td>\n",
       "      <td>152.221130</td>\n",
       "      <td>53973.578772</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model 2</th>\n",
       "      <td>463.693088</td>\n",
       "      <td>3.259088e+05</td>\n",
       "      <td>454.789024</td>\n",
       "      <td>3.169509e+05</td>\n",
       "      <td>448.204276</td>\n",
       "      <td>3.106587e+05</td>\n",
       "      <td>441.139990</td>\n",
       "      <td>3.051107e+05</td>\n",
       "      <td>435.878111</td>\n",
       "      <td>3.012721e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>417.102521</td>\n",
       "      <td>323426.360733</td>\n",
       "      <td>418.891692</td>\n",
       "      <td>328750.074410</td>\n",
       "      <td>423.487900</td>\n",
       "      <td>340886.677480</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model assembled</th>\n",
       "      <td>18.096105</td>\n",
       "      <td>9.535292e+02</td>\n",
       "      <td>33.779024</td>\n",
       "      <td>3.829357e+03</td>\n",
       "      <td>29.816522</td>\n",
       "      <td>2.585158e+03</td>\n",
       "      <td>34.487282</td>\n",
       "      <td>4.648510e+03</td>\n",
       "      <td>33.937061</td>\n",
       "      <td>4.277822e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>109.961991</td>\n",
       "      <td>27595.022560</td>\n",
       "      <td>129.193613</td>\n",
       "      <td>36459.903636</td>\n",
       "      <td>135.379400</td>\n",
       "      <td>39469.205750</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model 1</th>\n",
       "      <td>31.068291</td>\n",
       "      <td>2.290588e+03</td>\n",
       "      <td>39.909974</td>\n",
       "      <td>3.832537e+03</td>\n",
       "      <td>25.366202</td>\n",
       "      <td>1.604787e+03</td>\n",
       "      <td>50.361551</td>\n",
       "      <td>6.360558e+03</td>\n",
       "      <td>39.198889</td>\n",
       "      <td>3.594980e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>193.509169</td>\n",
       "      <td>82774.435044</td>\n",
       "      <td>245.966797</td>\n",
       "      <td>131540.094327</td>\n",
       "      <td>257.514023</td>\n",
       "      <td>143283.869474</td>\n",
       "      <td>322.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model 2</th>\n",
       "      <td>355.076026</td>\n",
       "      <td>2.059965e+05</td>\n",
       "      <td>355.770596</td>\n",
       "      <td>2.088753e+05</td>\n",
       "      <td>362.292266</td>\n",
       "      <td>2.283724e+05</td>\n",
       "      <td>361.486870</td>\n",
       "      <td>2.277429e+05</td>\n",
       "      <td>367.926998</td>\n",
       "      <td>2.450415e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>476.388426</td>\n",
       "      <td>609806.420371</td>\n",
       "      <td>489.535956</td>\n",
       "      <td>655337.689234</td>\n",
       "      <td>497.947143</td>\n",
       "      <td>670794.236261</td>\n",
       "      <td>322.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model assembled</th>\n",
       "      <td>22.028614</td>\n",
       "      <td>1.191882e+03</td>\n",
       "      <td>31.683385</td>\n",
       "      <td>2.477060e+03</td>\n",
       "      <td>25.366202</td>\n",
       "      <td>1.604787e+03</td>\n",
       "      <td>42.843710</td>\n",
       "      <td>4.650568e+03</td>\n",
       "      <td>39.198889</td>\n",
       "      <td>3.594980e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>177.407158</td>\n",
       "      <td>68575.009474</td>\n",
       "      <td>207.488579</td>\n",
       "      <td>92371.342298</td>\n",
       "      <td>216.492797</td>\n",
       "      <td>99642.408861</td>\n",
       "      <td>322.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model 1</th>\n",
       "      <td>66.534802</td>\n",
       "      <td>2.885333e+04</td>\n",
       "      <td>71.733074</td>\n",
       "      <td>3.225520e+04</td>\n",
       "      <td>71.877507</td>\n",
       "      <td>2.905768e+04</td>\n",
       "      <td>76.276322</td>\n",
       "      <td>3.122377e+04</td>\n",
       "      <td>75.818950</td>\n",
       "      <td>2.972669e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>205.839266</td>\n",
       "      <td>86887.665553</td>\n",
       "      <td>223.860336</td>\n",
       "      <td>100571.632496</td>\n",
       "      <td>235.134398</td>\n",
       "      <td>108983.543420</td>\n",
       "      <td>616.4</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model 2</th>\n",
       "      <td>572.219190</td>\n",
       "      <td>6.269099e+05</td>\n",
       "      <td>568.716053</td>\n",
       "      <td>6.169478e+05</td>\n",
       "      <td>566.143850</td>\n",
       "      <td>6.091390e+05</td>\n",
       "      <td>562.310992</td>\n",
       "      <td>5.984123e+05</td>\n",
       "      <td>559.642094</td>\n",
       "      <td>5.898306e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>539.915999</td>\n",
       "      <td>555587.809548</td>\n",
       "      <td>539.707177</td>\n",
       "      <td>557630.490947</td>\n",
       "      <td>540.034814</td>\n",
       "      <td>558225.474250</td>\n",
       "      <td>616.4</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model as</th>\n",
       "      <td>64.232119</td>\n",
       "      <td>2.844933e+04</td>\n",
       "      <td>69.522272</td>\n",
       "      <td>3.187623e+04</td>\n",
       "      <td>69.875180</td>\n",
       "      <td>2.847257e+04</td>\n",
       "      <td>74.772754</td>\n",
       "      <td>3.088178e+04</td>\n",
       "      <td>75.818950</td>\n",
       "      <td>2.972669e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>195.720771</td>\n",
       "      <td>80133.888023</td>\n",
       "      <td>208.132918</td>\n",
       "      <td>88142.344229</td>\n",
       "      <td>217.469518</td>\n",
       "      <td>94460.723769</td>\n",
       "      <td>616.4</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          MAE(t+1)      MSE(t+1)    MAE(t+2)      MSE(t+2)  \\\n",
       "name                                                                         \n",
       "walk 1 model 1          204.429573  1.331389e+05  205.129543  1.452423e+05   \n",
       "walk 1 model 2          857.449403  1.495423e+06  847.176601  1.448076e+06   \n",
       "walk 1 model assembled  201.955836  1.322176e+05  205.129543  1.452423e+05   \n",
       "walk 2 model 1           52.007630  6.159798e+03   54.983300  6.696802e+03   \n",
       "walk 2 model 2          670.180804  7.335546e+05  667.310006  7.271113e+05   \n",
       "walk 2 model assembled   52.007630  6.159798e+03   54.983300  6.696802e+03   \n",
       "walk 3 model 1           27.072408  1.723828e+03   22.036108  1.135663e+03   \n",
       "walk 3 model 2          514.696630  3.736666e+05  518.534040  3.837259e+05   \n",
       "walk 3 model assembled   27.072408  1.723828e+03   22.036108  1.135663e+03   \n",
       "walk 4 model 1           18.096105  9.535292e+02   36.606444  4.368720e+03   \n",
       "walk 4 model 2          463.693088  3.259088e+05  454.789024  3.169509e+05   \n",
       "walk 4 model assembled   18.096105  9.535292e+02   33.779024  3.829357e+03   \n",
       "walk 5 model 1           31.068291  2.290588e+03   39.909974  3.832537e+03   \n",
       "walk 5 model 2          355.076026  2.059965e+05  355.770596  2.088753e+05   \n",
       "walk 5 model assembled   22.028614  1.191882e+03   31.683385  2.477060e+03   \n",
       "mean model 1             66.534802  2.885333e+04   71.733074  3.225520e+04   \n",
       "mean model 2            572.219190  6.269099e+05  568.716053  6.169478e+05   \n",
       "mean model as            64.232119  2.844933e+04   69.522272  3.187623e+04   \n",
       "\n",
       "                          MAE(t+3)      MSE(t+3)    MAE(t+4)      MSE(t+4)  \\\n",
       "name                                                                         \n",
       "walk 1 model 1          202.585008  1.270908e+05  197.969829  1.340822e+05   \n",
       "walk 1 model 2          835.923811  1.397476e+06  824.758933  1.346553e+06   \n",
       "walk 1 model assembled  199.594343  1.259375e+05  197.969829  1.340822e+05   \n",
       "walk 2 model 1           68.911206  1.136799e+04   63.867185  8.187963e+03   \n",
       "walk 2 model 2          665.244090  7.209573e+05  664.271445  7.189750e+05   \n",
       "walk 2 model assembled   66.046014  1.031034e+04   63.867185  8.187963e+03   \n",
       "walk 3 model 1           28.552819  1.925100e+03   34.695761  2.839635e+03   \n",
       "walk 3 model 2          519.054807  3.882305e+05  519.897722  3.936795e+05   \n",
       "walk 3 model assembled   28.552819  1.925100e+03   34.695761  2.839635e+03   \n",
       "walk 4 model 1           33.972302  3.299719e+03   34.487282  4.648510e+03   \n",
       "walk 4 model 2          448.204276  3.106587e+05  441.139990  3.051107e+05   \n",
       "walk 4 model assembled   29.816522  2.585158e+03   34.487282  4.648510e+03   \n",
       "walk 5 model 1           25.366202  1.604787e+03   50.361551  6.360558e+03   \n",
       "walk 5 model 2          362.292266  2.283724e+05  361.486870  2.277429e+05   \n",
       "walk 5 model assembled   25.366202  1.604787e+03   42.843710  4.650568e+03   \n",
       "mean model 1             71.877507  2.905768e+04   76.276322  3.122377e+04   \n",
       "mean model 2            566.143850  6.091390e+05  562.310992  5.984123e+05   \n",
       "mean model as            69.875180  2.847257e+04   74.772754  3.088178e+04   \n",
       "\n",
       "                          MAE(t+5)      MSE(t+5)  ...   MAE(t+18)  \\\n",
       "name                                              ...               \n",
       "walk 1 model 1          191.819972  1.262198e+05  ...  249.734384   \n",
       "walk 1 model 2          811.446414  1.291361e+06  ...  674.533051   \n",
       "walk 1 model assembled  191.819972  1.262198e+05  ...  249.088411   \n",
       "walk 2 model 1           73.878844  1.051365e+04  ...  288.565513   \n",
       "walk 2 model 2          661.604074  7.115761e+05  ...  641.832572   \n",
       "walk 2 model assembled   73.878844  1.051365e+04  ...  266.641956   \n",
       "walk 3 model 1           40.259983  4.027213e+03  ...  181.839963   \n",
       "walk 3 model 2          521.354873  3.999022e+05  ...  489.723424   \n",
       "walk 3 model assembled   40.259983  4.027213e+03  ...  175.504337   \n",
       "walk 4 model 1           33.937061  4.277822e+03  ...  115.547299   \n",
       "walk 4 model 2          435.878111  3.012721e+05  ...  417.102521   \n",
       "walk 4 model assembled   33.937061  4.277822e+03  ...  109.961991   \n",
       "walk 5 model 1           39.198889  3.594980e+03  ...  193.509169   \n",
       "walk 5 model 2          367.926998  2.450415e+05  ...  476.388426   \n",
       "walk 5 model assembled   39.198889  3.594980e+03  ...  177.407158   \n",
       "mean model 1             75.818950  2.972669e+04  ...  205.839266   \n",
       "mean model 2            559.642094  5.898306e+05  ...  539.915999   \n",
       "mean model as            75.818950  2.972669e+04  ...  195.720771   \n",
       "\n",
       "                            MSE(t+18)   MAE(t+19)      MSE(t+19)   MAE(t+20)  \\\n",
       "name                                                                           \n",
       "walk 1 model 1          125296.996112  256.339936  131194.444790  264.854176   \n",
       "walk 1 model 2          762345.293831  664.535309  733380.787768  656.679076   \n",
       "walk 1 model assembled  131389.363513  254.568326  137070.766181  262.835581   \n",
       "walk 2 model 1          134055.002845  280.916750  124126.537738  295.172741   \n",
       "walk 2 model 2          661392.908836  639.022650  651663.177974  637.856855   \n",
       "walk 2 model assembled  114242.110146  264.741106  110616.636843  275.981683   \n",
       "walk 3 model 1           61375.584389  192.251272   67272.050903  205.909921   \n",
       "walk 3 model 2          420968.063969  486.550280  419020.725349  484.203095   \n",
       "walk 3 model assembled   58867.934419  184.672965   64193.072188  196.658130   \n",
       "walk 4 model 1           30936.309376  143.826924   48725.034722  152.221130   \n",
       "walk 4 model 2          323426.360733  418.891692  328750.074410  423.487900   \n",
       "walk 4 model assembled   27595.022560  129.193613   36459.903636  135.379400   \n",
       "walk 5 model 1           82774.435044  245.966797  131540.094327  257.514023   \n",
       "walk 5 model 2          609806.420371  489.535956  655337.689234  497.947143   \n",
       "walk 5 model assembled   68575.009474  207.488579   92371.342298  216.492797   \n",
       "mean model 1             86887.665553  223.860336  100571.632496  235.134398   \n",
       "mean model 2            555587.809548  539.707177  557630.490947  540.034814   \n",
       "mean model as            80133.888023  208.132918   88142.344229  217.469518   \n",
       "\n",
       "                            MSE(t+20)  nb_test_datapoints  days_train  \\\n",
       "name                                                                    \n",
       "walk 1 model 1          136616.251551               690.0       220.0   \n",
       "walk 1 model 2          710527.486339               690.0       220.0   \n",
       "walk 1 model assembled  142791.064706               690.0       220.0   \n",
       "walk 2 model 1          134822.216432               690.0       250.0   \n",
       "walk 2 model 2          647534.203214               690.0       250.0   \n",
       "walk 2 model assembled  118478.243034               690.0       250.0   \n",
       "walk 3 model 1           76221.800874               690.0       280.0   \n",
       "walk 3 model 2          421384.767956               690.0       280.0   \n",
       "walk 3 model assembled   71922.696493               690.0       280.0   \n",
       "walk 4 model 1           53973.578772               690.0       310.0   \n",
       "walk 4 model 2          340886.677480               690.0       310.0   \n",
       "walk 4 model assembled   39469.205750               690.0       310.0   \n",
       "walk 5 model 1          143283.869474               322.0       340.0   \n",
       "walk 5 model 2          670794.236261               322.0       340.0   \n",
       "walk 5 model assembled   99642.408861               322.0       340.0   \n",
       "mean model 1            108983.543420               616.4       280.0   \n",
       "mean model 2            558225.474250               616.4       280.0   \n",
       "mean model as            94460.723769               616.4       280.0   \n",
       "\n",
       "                        days_valid  days_test  \n",
       "name                                           \n",
       "walk 1 model 1                30.0       30.0  \n",
       "walk 1 model 2                30.0       30.0  \n",
       "walk 1 model assembled        30.0       30.0  \n",
       "walk 2 model 1                30.0       30.0  \n",
       "walk 2 model 2                30.0       30.0  \n",
       "walk 2 model assembled        30.0       30.0  \n",
       "walk 3 model 1                30.0       30.0  \n",
       "walk 3 model 2                30.0       30.0  \n",
       "walk 3 model assembled        30.0       30.0  \n",
       "walk 4 model 1                30.0       30.0  \n",
       "walk 4 model 2                30.0       30.0  \n",
       "walk 4 model assembled        30.0       30.0  \n",
       "walk 5 model 1                30.0       14.0  \n",
       "walk 5 model 2                30.0       14.0  \n",
       "walk 5 model assembled        30.0       14.0  \n",
       "mean model 1                  30.0       26.8  \n",
       "mean model 2                  30.0       26.8  \n",
       "mean model as                 30.0       26.8  \n",
       "\n",
       "[18 rows x 44 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_all_inputs = lambda batch_input_shape: get_dense_model(batch_input_shape, use_lambda=False)\n",
    "\n",
    "df_errors_1 = walk_forward_assembly(get_dense_model, dg, dense_all_inputs, dg_2, get_assemble,\n",
    "                     220, 30, 30, epochs=1000, verbose=0)\n",
    "df_errors_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2be43fab80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2be43faee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 1006 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2c3c345f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2b841e7af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 1006 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2b8471dd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2c3c4161f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 1006 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2b6c789040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2b840d5e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 1006 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2bf8748790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2be463d3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE(t+1)</th>\n",
       "      <th>MSE(t+1)</th>\n",
       "      <th>MAE(t+2)</th>\n",
       "      <th>MSE(t+2)</th>\n",
       "      <th>MAE(t+3)</th>\n",
       "      <th>MSE(t+3)</th>\n",
       "      <th>MAE(t+4)</th>\n",
       "      <th>MSE(t+4)</th>\n",
       "      <th>MAE(t+5)</th>\n",
       "      <th>MSE(t+5)</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE(t+18)</th>\n",
       "      <th>MSE(t+18)</th>\n",
       "      <th>MAE(t+19)</th>\n",
       "      <th>MSE(t+19)</th>\n",
       "      <th>MAE(t+20)</th>\n",
       "      <th>MSE(t+20)</th>\n",
       "      <th>nb_test_datapoints</th>\n",
       "      <th>days_train</th>\n",
       "      <th>days_valid</th>\n",
       "      <th>days_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>walk 1 model 1</th>\n",
       "      <td>205.359575</td>\n",
       "      <td>1.346230e+05</td>\n",
       "      <td>206.817237</td>\n",
       "      <td>1.470677e+05</td>\n",
       "      <td>200.159321</td>\n",
       "      <td>1.256150e+05</td>\n",
       "      <td>199.045412</td>\n",
       "      <td>1.349203e+05</td>\n",
       "      <td>190.704709</td>\n",
       "      <td>1.248638e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>240.018200</td>\n",
       "      <td>114373.306643</td>\n",
       "      <td>262.643173</td>\n",
       "      <td>134264.002544</td>\n",
       "      <td>283.073171</td>\n",
       "      <td>157382.808950</td>\n",
       "      <td>690.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 1 model 2</th>\n",
       "      <td>841.290383</td>\n",
       "      <td>1.447244e+06</td>\n",
       "      <td>829.425927</td>\n",
       "      <td>1.393877e+06</td>\n",
       "      <td>819.041499</td>\n",
       "      <td>1.348182e+06</td>\n",
       "      <td>806.733575</td>\n",
       "      <td>1.293622e+06</td>\n",
       "      <td>795.734421</td>\n",
       "      <td>1.244872e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>665.686862</td>\n",
       "      <td>748575.307426</td>\n",
       "      <td>658.039359</td>\n",
       "      <td>725174.397743</td>\n",
       "      <td>650.516138</td>\n",
       "      <td>703504.262707</td>\n",
       "      <td>690.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 1 model assembled</th>\n",
       "      <td>297.220818</td>\n",
       "      <td>3.160214e+05</td>\n",
       "      <td>322.036726</td>\n",
       "      <td>3.660531e+05</td>\n",
       "      <td>281.706538</td>\n",
       "      <td>2.929486e+05</td>\n",
       "      <td>305.862600</td>\n",
       "      <td>3.362027e+05</td>\n",
       "      <td>296.632224</td>\n",
       "      <td>3.131678e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>260.692604</td>\n",
       "      <td>167475.930469</td>\n",
       "      <td>267.856831</td>\n",
       "      <td>160082.746594</td>\n",
       "      <td>287.608146</td>\n",
       "      <td>182790.909115</td>\n",
       "      <td>690.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model 1</th>\n",
       "      <td>71.842190</td>\n",
       "      <td>1.262219e+04</td>\n",
       "      <td>75.284750</td>\n",
       "      <td>1.403222e+04</td>\n",
       "      <td>57.506645</td>\n",
       "      <td>7.005416e+03</td>\n",
       "      <td>67.501287</td>\n",
       "      <td>8.906750e+03</td>\n",
       "      <td>68.921244</td>\n",
       "      <td>9.187643e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>247.395637</td>\n",
       "      <td>98306.425981</td>\n",
       "      <td>276.476172</td>\n",
       "      <td>120763.396158</td>\n",
       "      <td>336.145325</td>\n",
       "      <td>176129.433482</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model 2</th>\n",
       "      <td>668.161522</td>\n",
       "      <td>7.269513e+05</td>\n",
       "      <td>666.492029</td>\n",
       "      <td>7.227301e+05</td>\n",
       "      <td>667.852844</td>\n",
       "      <td>7.291234e+05</td>\n",
       "      <td>662.682302</td>\n",
       "      <td>7.176208e+05</td>\n",
       "      <td>664.898275</td>\n",
       "      <td>7.249901e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>645.621536</td>\n",
       "      <td>690285.045987</td>\n",
       "      <td>643.085628</td>\n",
       "      <td>682697.303833</td>\n",
       "      <td>643.439762</td>\n",
       "      <td>683055.872792</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model assembled</th>\n",
       "      <td>95.795288</td>\n",
       "      <td>1.812105e+04</td>\n",
       "      <td>91.452093</td>\n",
       "      <td>1.654700e+04</td>\n",
       "      <td>121.977498</td>\n",
       "      <td>2.745544e+04</td>\n",
       "      <td>108.942616</td>\n",
       "      <td>2.281274e+04</td>\n",
       "      <td>104.010207</td>\n",
       "      <td>2.097799e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>173.132937</td>\n",
       "      <td>55173.405647</td>\n",
       "      <td>206.121002</td>\n",
       "      <td>75005.786388</td>\n",
       "      <td>275.649527</td>\n",
       "      <td>124863.169330</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model 1</th>\n",
       "      <td>25.612452</td>\n",
       "      <td>1.362142e+03</td>\n",
       "      <td>34.678906</td>\n",
       "      <td>2.481711e+03</td>\n",
       "      <td>28.395934</td>\n",
       "      <td>1.930895e+03</td>\n",
       "      <td>46.985452</td>\n",
       "      <td>4.566551e+03</td>\n",
       "      <td>51.432681</td>\n",
       "      <td>5.418842e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>201.428402</td>\n",
       "      <td>72240.033528</td>\n",
       "      <td>213.548231</td>\n",
       "      <td>80074.549069</td>\n",
       "      <td>225.765716</td>\n",
       "      <td>89175.017045</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model 2</th>\n",
       "      <td>509.567174</td>\n",
       "      <td>3.623869e+05</td>\n",
       "      <td>512.489133</td>\n",
       "      <td>3.705272e+05</td>\n",
       "      <td>514.975564</td>\n",
       "      <td>3.770876e+05</td>\n",
       "      <td>518.482096</td>\n",
       "      <td>3.859664e+05</td>\n",
       "      <td>520.508537</td>\n",
       "      <td>3.923984e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>503.130252</td>\n",
       "      <td>430010.240862</td>\n",
       "      <td>499.798975</td>\n",
       "      <td>432065.186890</td>\n",
       "      <td>498.120994</td>\n",
       "      <td>436426.027179</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model assembled</th>\n",
       "      <td>81.225940</td>\n",
       "      <td>1.020617e+04</td>\n",
       "      <td>72.169019</td>\n",
       "      <td>8.550672e+03</td>\n",
       "      <td>107.980807</td>\n",
       "      <td>1.936847e+04</td>\n",
       "      <td>70.053672</td>\n",
       "      <td>9.248593e+03</td>\n",
       "      <td>65.733438</td>\n",
       "      <td>9.085088e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>162.231078</td>\n",
       "      <td>53483.386900</td>\n",
       "      <td>175.188118</td>\n",
       "      <td>59855.688131</td>\n",
       "      <td>188.412832</td>\n",
       "      <td>67586.217000</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model 1</th>\n",
       "      <td>18.380711</td>\n",
       "      <td>1.018541e+03</td>\n",
       "      <td>29.766837</td>\n",
       "      <td>2.322321e+03</td>\n",
       "      <td>34.884461</td>\n",
       "      <td>3.226554e+03</td>\n",
       "      <td>38.179416</td>\n",
       "      <td>4.127840e+03</td>\n",
       "      <td>31.083974</td>\n",
       "      <td>3.507991e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>114.547475</td>\n",
       "      <td>30383.312172</td>\n",
       "      <td>142.934815</td>\n",
       "      <td>47439.658171</td>\n",
       "      <td>134.239531</td>\n",
       "      <td>41603.721972</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model 2</th>\n",
       "      <td>468.133596</td>\n",
       "      <td>3.283473e+05</td>\n",
       "      <td>457.574439</td>\n",
       "      <td>3.167498e+05</td>\n",
       "      <td>451.745046</td>\n",
       "      <td>3.125318e+05</td>\n",
       "      <td>442.084881</td>\n",
       "      <td>3.028048e+05</td>\n",
       "      <td>434.755077</td>\n",
       "      <td>2.969650e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>417.032652</td>\n",
       "      <td>327080.993822</td>\n",
       "      <td>418.422239</td>\n",
       "      <td>337445.863478</td>\n",
       "      <td>423.262689</td>\n",
       "      <td>350729.225720</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model assembled</th>\n",
       "      <td>104.646593</td>\n",
       "      <td>1.722091e+04</td>\n",
       "      <td>72.952459</td>\n",
       "      <td>9.423085e+03</td>\n",
       "      <td>69.054410</td>\n",
       "      <td>9.192680e+03</td>\n",
       "      <td>69.987214</td>\n",
       "      <td>1.019110e+04</td>\n",
       "      <td>90.448734</td>\n",
       "      <td>1.607298e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>95.756219</td>\n",
       "      <td>22567.860280</td>\n",
       "      <td>116.715116</td>\n",
       "      <td>32274.225793</td>\n",
       "      <td>114.184589</td>\n",
       "      <td>30285.889301</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model 1</th>\n",
       "      <td>21.402437</td>\n",
       "      <td>1.202098e+03</td>\n",
       "      <td>36.126577</td>\n",
       "      <td>3.118889e+03</td>\n",
       "      <td>26.470610</td>\n",
       "      <td>1.712978e+03</td>\n",
       "      <td>46.230885</td>\n",
       "      <td>5.858630e+03</td>\n",
       "      <td>39.759442</td>\n",
       "      <td>3.927504e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>229.514465</td>\n",
       "      <td>115138.165231</td>\n",
       "      <td>217.141629</td>\n",
       "      <td>101772.598932</td>\n",
       "      <td>265.134787</td>\n",
       "      <td>150677.946206</td>\n",
       "      <td>322.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model 2</th>\n",
       "      <td>367.401246</td>\n",
       "      <td>2.184310e+05</td>\n",
       "      <td>371.315740</td>\n",
       "      <td>2.286952e+05</td>\n",
       "      <td>374.985176</td>\n",
       "      <td>2.389450e+05</td>\n",
       "      <td>376.797060</td>\n",
       "      <td>2.431189e+05</td>\n",
       "      <td>382.856079</td>\n",
       "      <td>2.573243e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>483.620183</td>\n",
       "      <td>585818.685442</td>\n",
       "      <td>492.909764</td>\n",
       "      <td>618277.933543</td>\n",
       "      <td>502.266759</td>\n",
       "      <td>641258.599799</td>\n",
       "      <td>322.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model assembled</th>\n",
       "      <td>86.872126</td>\n",
       "      <td>1.257510e+04</td>\n",
       "      <td>47.917973</td>\n",
       "      <td>4.412828e+03</td>\n",
       "      <td>72.241747</td>\n",
       "      <td>9.450415e+03</td>\n",
       "      <td>46.986675</td>\n",
       "      <td>4.666508e+03</td>\n",
       "      <td>61.957582</td>\n",
       "      <td>7.805565e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>190.373468</td>\n",
       "      <td>74930.422722</td>\n",
       "      <td>172.670989</td>\n",
       "      <td>61081.658937</td>\n",
       "      <td>231.724666</td>\n",
       "      <td>105971.467565</td>\n",
       "      <td>322.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model 1</th>\n",
       "      <td>68.519473</td>\n",
       "      <td>3.016559e+04</td>\n",
       "      <td>76.534861</td>\n",
       "      <td>3.380457e+04</td>\n",
       "      <td>69.483394</td>\n",
       "      <td>2.789816e+04</td>\n",
       "      <td>79.588490</td>\n",
       "      <td>3.167602e+04</td>\n",
       "      <td>76.380410</td>\n",
       "      <td>2.938116e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>206.580836</td>\n",
       "      <td>86088.248711</td>\n",
       "      <td>222.548804</td>\n",
       "      <td>96862.840975</td>\n",
       "      <td>248.871706</td>\n",
       "      <td>122993.785531</td>\n",
       "      <td>616.4</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model 2</th>\n",
       "      <td>570.910784</td>\n",
       "      <td>6.166721e+05</td>\n",
       "      <td>567.459454</td>\n",
       "      <td>6.065158e+05</td>\n",
       "      <td>565.720026</td>\n",
       "      <td>6.011739e+05</td>\n",
       "      <td>561.355983</td>\n",
       "      <td>5.886267e+05</td>\n",
       "      <td>559.750478</td>\n",
       "      <td>5.833101e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>543.018297</td>\n",
       "      <td>556354.054708</td>\n",
       "      <td>542.451193</td>\n",
       "      <td>559132.137097</td>\n",
       "      <td>543.521268</td>\n",
       "      <td>562994.797639</td>\n",
       "      <td>616.4</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model as</th>\n",
       "      <td>133.152153</td>\n",
       "      <td>7.482892e+04</td>\n",
       "      <td>121.305654</td>\n",
       "      <td>8.099733e+04</td>\n",
       "      <td>130.592200</td>\n",
       "      <td>7.168312e+04</td>\n",
       "      <td>120.366555</td>\n",
       "      <td>7.662433e+04</td>\n",
       "      <td>123.756437</td>\n",
       "      <td>7.342188e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>176.437261</td>\n",
       "      <td>74726.201203</td>\n",
       "      <td>187.710411</td>\n",
       "      <td>77660.021169</td>\n",
       "      <td>219.515952</td>\n",
       "      <td>102299.530462</td>\n",
       "      <td>616.4</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          MAE(t+1)      MSE(t+1)    MAE(t+2)      MSE(t+2)  \\\n",
       "name                                                                         \n",
       "walk 1 model 1          205.359575  1.346230e+05  206.817237  1.470677e+05   \n",
       "walk 1 model 2          841.290383  1.447244e+06  829.425927  1.393877e+06   \n",
       "walk 1 model assembled  297.220818  3.160214e+05  322.036726  3.660531e+05   \n",
       "walk 2 model 1           71.842190  1.262219e+04   75.284750  1.403222e+04   \n",
       "walk 2 model 2          668.161522  7.269513e+05  666.492029  7.227301e+05   \n",
       "walk 2 model assembled   95.795288  1.812105e+04   91.452093  1.654700e+04   \n",
       "walk 3 model 1           25.612452  1.362142e+03   34.678906  2.481711e+03   \n",
       "walk 3 model 2          509.567174  3.623869e+05  512.489133  3.705272e+05   \n",
       "walk 3 model assembled   81.225940  1.020617e+04   72.169019  8.550672e+03   \n",
       "walk 4 model 1           18.380711  1.018541e+03   29.766837  2.322321e+03   \n",
       "walk 4 model 2          468.133596  3.283473e+05  457.574439  3.167498e+05   \n",
       "walk 4 model assembled  104.646593  1.722091e+04   72.952459  9.423085e+03   \n",
       "walk 5 model 1           21.402437  1.202098e+03   36.126577  3.118889e+03   \n",
       "walk 5 model 2          367.401246  2.184310e+05  371.315740  2.286952e+05   \n",
       "walk 5 model assembled   86.872126  1.257510e+04   47.917973  4.412828e+03   \n",
       "mean model 1             68.519473  3.016559e+04   76.534861  3.380457e+04   \n",
       "mean model 2            570.910784  6.166721e+05  567.459454  6.065158e+05   \n",
       "mean model as           133.152153  7.482892e+04  121.305654  8.099733e+04   \n",
       "\n",
       "                          MAE(t+3)      MSE(t+3)    MAE(t+4)      MSE(t+4)  \\\n",
       "name                                                                         \n",
       "walk 1 model 1          200.159321  1.256150e+05  199.045412  1.349203e+05   \n",
       "walk 1 model 2          819.041499  1.348182e+06  806.733575  1.293622e+06   \n",
       "walk 1 model assembled  281.706538  2.929486e+05  305.862600  3.362027e+05   \n",
       "walk 2 model 1           57.506645  7.005416e+03   67.501287  8.906750e+03   \n",
       "walk 2 model 2          667.852844  7.291234e+05  662.682302  7.176208e+05   \n",
       "walk 2 model assembled  121.977498  2.745544e+04  108.942616  2.281274e+04   \n",
       "walk 3 model 1           28.395934  1.930895e+03   46.985452  4.566551e+03   \n",
       "walk 3 model 2          514.975564  3.770876e+05  518.482096  3.859664e+05   \n",
       "walk 3 model assembled  107.980807  1.936847e+04   70.053672  9.248593e+03   \n",
       "walk 4 model 1           34.884461  3.226554e+03   38.179416  4.127840e+03   \n",
       "walk 4 model 2          451.745046  3.125318e+05  442.084881  3.028048e+05   \n",
       "walk 4 model assembled   69.054410  9.192680e+03   69.987214  1.019110e+04   \n",
       "walk 5 model 1           26.470610  1.712978e+03   46.230885  5.858630e+03   \n",
       "walk 5 model 2          374.985176  2.389450e+05  376.797060  2.431189e+05   \n",
       "walk 5 model assembled   72.241747  9.450415e+03   46.986675  4.666508e+03   \n",
       "mean model 1             69.483394  2.789816e+04   79.588490  3.167602e+04   \n",
       "mean model 2            565.720026  6.011739e+05  561.355983  5.886267e+05   \n",
       "mean model as           130.592200  7.168312e+04  120.366555  7.662433e+04   \n",
       "\n",
       "                          MAE(t+5)      MSE(t+5)  ...   MAE(t+18)  \\\n",
       "name                                              ...               \n",
       "walk 1 model 1          190.704709  1.248638e+05  ...  240.018200   \n",
       "walk 1 model 2          795.734421  1.244872e+06  ...  665.686862   \n",
       "walk 1 model assembled  296.632224  3.131678e+05  ...  260.692604   \n",
       "walk 2 model 1           68.921244  9.187643e+03  ...  247.395637   \n",
       "walk 2 model 2          664.898275  7.249901e+05  ...  645.621536   \n",
       "walk 2 model assembled  104.010207  2.097799e+04  ...  173.132937   \n",
       "walk 3 model 1           51.432681  5.418842e+03  ...  201.428402   \n",
       "walk 3 model 2          520.508537  3.923984e+05  ...  503.130252   \n",
       "walk 3 model assembled   65.733438  9.085088e+03  ...  162.231078   \n",
       "walk 4 model 1           31.083974  3.507991e+03  ...  114.547475   \n",
       "walk 4 model 2          434.755077  2.969650e+05  ...  417.032652   \n",
       "walk 4 model assembled   90.448734  1.607298e+04  ...   95.756219   \n",
       "walk 5 model 1           39.759442  3.927504e+03  ...  229.514465   \n",
       "walk 5 model 2          382.856079  2.573243e+05  ...  483.620183   \n",
       "walk 5 model assembled   61.957582  7.805565e+03  ...  190.373468   \n",
       "mean model 1             76.380410  2.938116e+04  ...  206.580836   \n",
       "mean model 2            559.750478  5.833101e+05  ...  543.018297   \n",
       "mean model as           123.756437  7.342188e+04  ...  176.437261   \n",
       "\n",
       "                            MSE(t+18)   MAE(t+19)      MSE(t+19)   MAE(t+20)  \\\n",
       "name                                                                           \n",
       "walk 1 model 1          114373.306643  262.643173  134264.002544  283.073171   \n",
       "walk 1 model 2          748575.307426  658.039359  725174.397743  650.516138   \n",
       "walk 1 model assembled  167475.930469  267.856831  160082.746594  287.608146   \n",
       "walk 2 model 1           98306.425981  276.476172  120763.396158  336.145325   \n",
       "walk 2 model 2          690285.045987  643.085628  682697.303833  643.439762   \n",
       "walk 2 model assembled   55173.405647  206.121002   75005.786388  275.649527   \n",
       "walk 3 model 1           72240.033528  213.548231   80074.549069  225.765716   \n",
       "walk 3 model 2          430010.240862  499.798975  432065.186890  498.120994   \n",
       "walk 3 model assembled   53483.386900  175.188118   59855.688131  188.412832   \n",
       "walk 4 model 1           30383.312172  142.934815   47439.658171  134.239531   \n",
       "walk 4 model 2          327080.993822  418.422239  337445.863478  423.262689   \n",
       "walk 4 model assembled   22567.860280  116.715116   32274.225793  114.184589   \n",
       "walk 5 model 1          115138.165231  217.141629  101772.598932  265.134787   \n",
       "walk 5 model 2          585818.685442  492.909764  618277.933543  502.266759   \n",
       "walk 5 model assembled   74930.422722  172.670989   61081.658937  231.724666   \n",
       "mean model 1             86088.248711  222.548804   96862.840975  248.871706   \n",
       "mean model 2            556354.054708  542.451193  559132.137097  543.521268   \n",
       "mean model as            74726.201203  187.710411   77660.021169  219.515952   \n",
       "\n",
       "                            MSE(t+20)  nb_test_datapoints  days_train  \\\n",
       "name                                                                    \n",
       "walk 1 model 1          157382.808950               690.0       220.0   \n",
       "walk 1 model 2          703504.262707               690.0       220.0   \n",
       "walk 1 model assembled  182790.909115               690.0       220.0   \n",
       "walk 2 model 1          176129.433482               690.0       250.0   \n",
       "walk 2 model 2          683055.872792               690.0       250.0   \n",
       "walk 2 model assembled  124863.169330               690.0       250.0   \n",
       "walk 3 model 1           89175.017045               690.0       280.0   \n",
       "walk 3 model 2          436426.027179               690.0       280.0   \n",
       "walk 3 model assembled   67586.217000               690.0       280.0   \n",
       "walk 4 model 1           41603.721972               690.0       310.0   \n",
       "walk 4 model 2          350729.225720               690.0       310.0   \n",
       "walk 4 model assembled   30285.889301               690.0       310.0   \n",
       "walk 5 model 1          150677.946206               322.0       340.0   \n",
       "walk 5 model 2          641258.599799               322.0       340.0   \n",
       "walk 5 model assembled  105971.467565               322.0       340.0   \n",
       "mean model 1            122993.785531               616.4       280.0   \n",
       "mean model 2            562994.797639               616.4       280.0   \n",
       "mean model as           102299.530462               616.4       280.0   \n",
       "\n",
       "                        days_valid  days_test  \n",
       "name                                           \n",
       "walk 1 model 1                30.0       30.0  \n",
       "walk 1 model 2                30.0       30.0  \n",
       "walk 1 model assembled        30.0       30.0  \n",
       "walk 2 model 1                30.0       30.0  \n",
       "walk 2 model 2                30.0       30.0  \n",
       "walk 2 model assembled        30.0       30.0  \n",
       "walk 3 model 1                30.0       30.0  \n",
       "walk 3 model 2                30.0       30.0  \n",
       "walk 3 model assembled        30.0       30.0  \n",
       "walk 4 model 1                30.0       30.0  \n",
       "walk 4 model 2                30.0       30.0  \n",
       "walk 4 model assembled        30.0       30.0  \n",
       "walk 5 model 1                30.0       14.0  \n",
       "walk 5 model 2                30.0       14.0  \n",
       "walk 5 model assembled        30.0       14.0  \n",
       "mean model 1                  30.0       26.8  \n",
       "mean model 2                  30.0       26.8  \n",
       "mean model as                 30.0       26.8  \n",
       "\n",
       "[18 rows x 44 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_errors_2 = walk_forward_assembly(get_dense_model, dg, dense_all_inputs, dg_2, get_assemble,\n",
    "                     220, 30, 30, epochs=1000, verbose=0, train_val=True)\n",
    "df_errors_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2c3c298e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2bf85e21f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 1006 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2bf8640dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2b8e4ea0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 1006 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2be44734c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2bcc22cf70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 1006 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2c1c0ae1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2c1c1c3430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 1006 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2bcc284f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 1005 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f2b8c0c6310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE(t+1)</th>\n",
       "      <th>MSE(t+1)</th>\n",
       "      <th>MAE(t+2)</th>\n",
       "      <th>MSE(t+2)</th>\n",
       "      <th>MAE(t+3)</th>\n",
       "      <th>MSE(t+3)</th>\n",
       "      <th>MAE(t+4)</th>\n",
       "      <th>MSE(t+4)</th>\n",
       "      <th>MAE(t+5)</th>\n",
       "      <th>MSE(t+5)</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE(t+18)</th>\n",
       "      <th>MSE(t+18)</th>\n",
       "      <th>MAE(t+19)</th>\n",
       "      <th>MSE(t+19)</th>\n",
       "      <th>MAE(t+20)</th>\n",
       "      <th>MSE(t+20)</th>\n",
       "      <th>nb_test_datapoints</th>\n",
       "      <th>days_train</th>\n",
       "      <th>days_valid</th>\n",
       "      <th>days_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>walk 1 model 1</th>\n",
       "      <td>205.749016</td>\n",
       "      <td>1.339197e+05</td>\n",
       "      <td>202.760374</td>\n",
       "      <td>1.299890e+05</td>\n",
       "      <td>200.868403</td>\n",
       "      <td>1.262319e+05</td>\n",
       "      <td>192.446887</td>\n",
       "      <td>1.166317e+05</td>\n",
       "      <td>191.836021</td>\n",
       "      <td>1.257932e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>239.936145</td>\n",
       "      <td>114804.513530</td>\n",
       "      <td>272.426521</td>\n",
       "      <td>146042.923094</td>\n",
       "      <td>286.587098</td>\n",
       "      <td>162413.274494</td>\n",
       "      <td>690.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 1 model 2</th>\n",
       "      <td>835.605648</td>\n",
       "      <td>1.424888e+06</td>\n",
       "      <td>825.436267</td>\n",
       "      <td>1.377794e+06</td>\n",
       "      <td>813.266471</td>\n",
       "      <td>1.328243e+06</td>\n",
       "      <td>801.772555</td>\n",
       "      <td>1.277278e+06</td>\n",
       "      <td>790.706878</td>\n",
       "      <td>1.227559e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>657.213486</td>\n",
       "      <td>727436.426914</td>\n",
       "      <td>648.669434</td>\n",
       "      <td>702575.216201</td>\n",
       "      <td>640.329349</td>\n",
       "      <td>680391.055828</td>\n",
       "      <td>690.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 1 model assembled</th>\n",
       "      <td>215.559065</td>\n",
       "      <td>1.610753e+05</td>\n",
       "      <td>211.433742</td>\n",
       "      <td>1.556858e+05</td>\n",
       "      <td>207.850032</td>\n",
       "      <td>1.507055e+05</td>\n",
       "      <td>205.680109</td>\n",
       "      <td>1.426536e+05</td>\n",
       "      <td>237.923851</td>\n",
       "      <td>1.789305e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>249.725255</td>\n",
       "      <td>153097.665829</td>\n",
       "      <td>267.920236</td>\n",
       "      <td>157619.214255</td>\n",
       "      <td>279.544339</td>\n",
       "      <td>171704.346638</td>\n",
       "      <td>690.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model 1</th>\n",
       "      <td>71.420210</td>\n",
       "      <td>1.271642e+04</td>\n",
       "      <td>77.922360</td>\n",
       "      <td>1.468552e+04</td>\n",
       "      <td>77.634159</td>\n",
       "      <td>1.373748e+04</td>\n",
       "      <td>58.555984</td>\n",
       "      <td>7.127024e+03</td>\n",
       "      <td>62.104880</td>\n",
       "      <td>7.650332e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>291.823872</td>\n",
       "      <td>136538.775333</td>\n",
       "      <td>270.571675</td>\n",
       "      <td>113992.431222</td>\n",
       "      <td>301.793588</td>\n",
       "      <td>140616.609658</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model 2</th>\n",
       "      <td>673.358360</td>\n",
       "      <td>7.384478e+05</td>\n",
       "      <td>666.365277</td>\n",
       "      <td>7.228764e+05</td>\n",
       "      <td>669.652507</td>\n",
       "      <td>7.351309e+05</td>\n",
       "      <td>668.153975</td>\n",
       "      <td>7.329114e+05</td>\n",
       "      <td>662.486237</td>\n",
       "      <td>7.217583e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>649.059020</td>\n",
       "      <td>717857.357715</td>\n",
       "      <td>650.436145</td>\n",
       "      <td>721874.545948</td>\n",
       "      <td>650.174747</td>\n",
       "      <td>719809.016537</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model assembled</th>\n",
       "      <td>55.440799</td>\n",
       "      <td>6.271443e+03</td>\n",
       "      <td>50.734829</td>\n",
       "      <td>5.332254e+03</td>\n",
       "      <td>53.013030</td>\n",
       "      <td>5.587842e+03</td>\n",
       "      <td>77.886593</td>\n",
       "      <td>1.259902e+04</td>\n",
       "      <td>79.385070</td>\n",
       "      <td>1.336015e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>207.547867</td>\n",
       "      <td>72509.470759</td>\n",
       "      <td>192.820727</td>\n",
       "      <td>61779.693250</td>\n",
       "      <td>218.484094</td>\n",
       "      <td>78835.238792</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model 1</th>\n",
       "      <td>25.382978</td>\n",
       "      <td>1.339485e+03</td>\n",
       "      <td>22.300119</td>\n",
       "      <td>1.088184e+03</td>\n",
       "      <td>30.854684</td>\n",
       "      <td>2.194573e+03</td>\n",
       "      <td>34.201942</td>\n",
       "      <td>2.925755e+03</td>\n",
       "      <td>37.972773</td>\n",
       "      <td>3.699749e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>202.658485</td>\n",
       "      <td>72923.050808</td>\n",
       "      <td>217.495675</td>\n",
       "      <td>85380.889956</td>\n",
       "      <td>224.421786</td>\n",
       "      <td>88798.277138</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model 2</th>\n",
       "      <td>514.325145</td>\n",
       "      <td>3.714952e+05</td>\n",
       "      <td>517.711508</td>\n",
       "      <td>3.797647e+05</td>\n",
       "      <td>519.561025</td>\n",
       "      <td>3.863178e+05</td>\n",
       "      <td>520.821350</td>\n",
       "      <td>3.941537e+05</td>\n",
       "      <td>521.006565</td>\n",
       "      <td>3.957890e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>492.137094</td>\n",
       "      <td>422924.883309</td>\n",
       "      <td>486.308763</td>\n",
       "      <td>418327.780337</td>\n",
       "      <td>483.022303</td>\n",
       "      <td>416224.761455</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model assembled</th>\n",
       "      <td>107.141853</td>\n",
       "      <td>3.613893e+04</td>\n",
       "      <td>128.934626</td>\n",
       "      <td>4.892904e+04</td>\n",
       "      <td>128.927829</td>\n",
       "      <td>5.080849e+04</td>\n",
       "      <td>128.620982</td>\n",
       "      <td>5.146298e+04</td>\n",
       "      <td>124.533941</td>\n",
       "      <td>5.082550e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>160.295079</td>\n",
       "      <td>70328.073548</td>\n",
       "      <td>172.318911</td>\n",
       "      <td>74530.458888</td>\n",
       "      <td>177.962549</td>\n",
       "      <td>77686.596600</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model 1</th>\n",
       "      <td>26.338581</td>\n",
       "      <td>1.830152e+03</td>\n",
       "      <td>31.982908</td>\n",
       "      <td>2.680333e+03</td>\n",
       "      <td>25.122785</td>\n",
       "      <td>2.220701e+03</td>\n",
       "      <td>30.540708</td>\n",
       "      <td>3.406859e+03</td>\n",
       "      <td>45.282531</td>\n",
       "      <td>5.960852e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>114.647226</td>\n",
       "      <td>30352.643199</td>\n",
       "      <td>127.004964</td>\n",
       "      <td>37660.755215</td>\n",
       "      <td>153.277219</td>\n",
       "      <td>55274.902847</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model 2</th>\n",
       "      <td>461.045148</td>\n",
       "      <td>3.174840e+05</td>\n",
       "      <td>450.115962</td>\n",
       "      <td>3.078064e+05</td>\n",
       "      <td>440.813960</td>\n",
       "      <td>2.968626e+05</td>\n",
       "      <td>433.586100</td>\n",
       "      <td>2.901058e+05</td>\n",
       "      <td>427.542547</td>\n",
       "      <td>2.858979e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>410.127144</td>\n",
       "      <td>339329.172890</td>\n",
       "      <td>414.030611</td>\n",
       "      <td>354265.997317</td>\n",
       "      <td>416.278998</td>\n",
       "      <td>363704.758530</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model assembled</th>\n",
       "      <td>119.297878</td>\n",
       "      <td>4.755902e+04</td>\n",
       "      <td>116.542494</td>\n",
       "      <td>4.615863e+04</td>\n",
       "      <td>153.208148</td>\n",
       "      <td>6.867602e+04</td>\n",
       "      <td>152.177331</td>\n",
       "      <td>6.926258e+04</td>\n",
       "      <td>118.289818</td>\n",
       "      <td>4.824438e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>131.974094</td>\n",
       "      <td>53417.493599</td>\n",
       "      <td>134.938036</td>\n",
       "      <td>55835.205677</td>\n",
       "      <td>132.430094</td>\n",
       "      <td>47280.935685</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model 1</th>\n",
       "      <td>24.625295</td>\n",
       "      <td>1.504178e+03</td>\n",
       "      <td>29.338505</td>\n",
       "      <td>2.301101e+03</td>\n",
       "      <td>39.935957</td>\n",
       "      <td>3.957851e+03</td>\n",
       "      <td>49.445978</td>\n",
       "      <td>6.375077e+03</td>\n",
       "      <td>50.619633</td>\n",
       "      <td>7.319459e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>201.555691</td>\n",
       "      <td>88695.892847</td>\n",
       "      <td>244.489660</td>\n",
       "      <td>129890.339716</td>\n",
       "      <td>222.024775</td>\n",
       "      <td>106467.611036</td>\n",
       "      <td>322.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model 2</th>\n",
       "      <td>356.590360</td>\n",
       "      <td>2.067132e+05</td>\n",
       "      <td>359.504280</td>\n",
       "      <td>2.174370e+05</td>\n",
       "      <td>360.258388</td>\n",
       "      <td>2.224072e+05</td>\n",
       "      <td>362.600340</td>\n",
       "      <td>2.411013e+05</td>\n",
       "      <td>369.955494</td>\n",
       "      <td>2.558248e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>480.876624</td>\n",
       "      <td>650002.936169</td>\n",
       "      <td>486.815208</td>\n",
       "      <td>674849.434121</td>\n",
       "      <td>499.302236</td>\n",
       "      <td>699363.240001</td>\n",
       "      <td>322.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model assembled</th>\n",
       "      <td>172.152448</td>\n",
       "      <td>8.119660e+04</td>\n",
       "      <td>138.978991</td>\n",
       "      <td>5.599305e+04</td>\n",
       "      <td>128.365812</td>\n",
       "      <td>4.856766e+04</td>\n",
       "      <td>124.288551</td>\n",
       "      <td>4.580927e+04</td>\n",
       "      <td>126.094859</td>\n",
       "      <td>4.705693e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>112.250546</td>\n",
       "      <td>21753.508645</td>\n",
       "      <td>128.779153</td>\n",
       "      <td>29290.900158</td>\n",
       "      <td>121.070510</td>\n",
       "      <td>24930.444150</td>\n",
       "      <td>322.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model 1</th>\n",
       "      <td>70.703216</td>\n",
       "      <td>3.026200e+04</td>\n",
       "      <td>72.860853</td>\n",
       "      <td>3.014883e+04</td>\n",
       "      <td>74.883198</td>\n",
       "      <td>2.966850e+04</td>\n",
       "      <td>73.038300</td>\n",
       "      <td>2.729328e+04</td>\n",
       "      <td>77.563168</td>\n",
       "      <td>3.008472e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>210.124284</td>\n",
       "      <td>88662.975143</td>\n",
       "      <td>226.397699</td>\n",
       "      <td>102593.467841</td>\n",
       "      <td>237.620893</td>\n",
       "      <td>110714.135034</td>\n",
       "      <td>616.4</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model 2</th>\n",
       "      <td>568.184932</td>\n",
       "      <td>6.118056e+05</td>\n",
       "      <td>563.826659</td>\n",
       "      <td>6.011357e+05</td>\n",
       "      <td>560.710470</td>\n",
       "      <td>5.937922e+05</td>\n",
       "      <td>557.386864</td>\n",
       "      <td>5.871101e+05</td>\n",
       "      <td>554.339544</td>\n",
       "      <td>5.773657e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>537.882674</td>\n",
       "      <td>571510.155399</td>\n",
       "      <td>537.252032</td>\n",
       "      <td>574378.594785</td>\n",
       "      <td>537.821527</td>\n",
       "      <td>575898.566470</td>\n",
       "      <td>616.4</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model as</th>\n",
       "      <td>133.918409</td>\n",
       "      <td>6.644826e+04</td>\n",
       "      <td>129.324936</td>\n",
       "      <td>6.241975e+04</td>\n",
       "      <td>134.272970</td>\n",
       "      <td>6.486911e+04</td>\n",
       "      <td>137.730713</td>\n",
       "      <td>6.435749e+04</td>\n",
       "      <td>137.245508</td>\n",
       "      <td>6.768350e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>172.358568</td>\n",
       "      <td>74221.242476</td>\n",
       "      <td>179.355413</td>\n",
       "      <td>75811.094446</td>\n",
       "      <td>185.898317</td>\n",
       "      <td>80087.512373</td>\n",
       "      <td>616.4</td>\n",
       "      <td>280.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          MAE(t+1)      MSE(t+1)    MAE(t+2)      MSE(t+2)  \\\n",
       "name                                                                         \n",
       "walk 1 model 1          205.749016  1.339197e+05  202.760374  1.299890e+05   \n",
       "walk 1 model 2          835.605648  1.424888e+06  825.436267  1.377794e+06   \n",
       "walk 1 model assembled  215.559065  1.610753e+05  211.433742  1.556858e+05   \n",
       "walk 2 model 1           71.420210  1.271642e+04   77.922360  1.468552e+04   \n",
       "walk 2 model 2          673.358360  7.384478e+05  666.365277  7.228764e+05   \n",
       "walk 2 model assembled   55.440799  6.271443e+03   50.734829  5.332254e+03   \n",
       "walk 3 model 1           25.382978  1.339485e+03   22.300119  1.088184e+03   \n",
       "walk 3 model 2          514.325145  3.714952e+05  517.711508  3.797647e+05   \n",
       "walk 3 model assembled  107.141853  3.613893e+04  128.934626  4.892904e+04   \n",
       "walk 4 model 1           26.338581  1.830152e+03   31.982908  2.680333e+03   \n",
       "walk 4 model 2          461.045148  3.174840e+05  450.115962  3.078064e+05   \n",
       "walk 4 model assembled  119.297878  4.755902e+04  116.542494  4.615863e+04   \n",
       "walk 5 model 1           24.625295  1.504178e+03   29.338505  2.301101e+03   \n",
       "walk 5 model 2          356.590360  2.067132e+05  359.504280  2.174370e+05   \n",
       "walk 5 model assembled  172.152448  8.119660e+04  138.978991  5.599305e+04   \n",
       "mean model 1             70.703216  3.026200e+04   72.860853  3.014883e+04   \n",
       "mean model 2            568.184932  6.118056e+05  563.826659  6.011357e+05   \n",
       "mean model as           133.918409  6.644826e+04  129.324936  6.241975e+04   \n",
       "\n",
       "                          MAE(t+3)      MSE(t+3)    MAE(t+4)      MSE(t+4)  \\\n",
       "name                                                                         \n",
       "walk 1 model 1          200.868403  1.262319e+05  192.446887  1.166317e+05   \n",
       "walk 1 model 2          813.266471  1.328243e+06  801.772555  1.277278e+06   \n",
       "walk 1 model assembled  207.850032  1.507055e+05  205.680109  1.426536e+05   \n",
       "walk 2 model 1           77.634159  1.373748e+04   58.555984  7.127024e+03   \n",
       "walk 2 model 2          669.652507  7.351309e+05  668.153975  7.329114e+05   \n",
       "walk 2 model assembled   53.013030  5.587842e+03   77.886593  1.259902e+04   \n",
       "walk 3 model 1           30.854684  2.194573e+03   34.201942  2.925755e+03   \n",
       "walk 3 model 2          519.561025  3.863178e+05  520.821350  3.941537e+05   \n",
       "walk 3 model assembled  128.927829  5.080849e+04  128.620982  5.146298e+04   \n",
       "walk 4 model 1           25.122785  2.220701e+03   30.540708  3.406859e+03   \n",
       "walk 4 model 2          440.813960  2.968626e+05  433.586100  2.901058e+05   \n",
       "walk 4 model assembled  153.208148  6.867602e+04  152.177331  6.926258e+04   \n",
       "walk 5 model 1           39.935957  3.957851e+03   49.445978  6.375077e+03   \n",
       "walk 5 model 2          360.258388  2.224072e+05  362.600340  2.411013e+05   \n",
       "walk 5 model assembled  128.365812  4.856766e+04  124.288551  4.580927e+04   \n",
       "mean model 1             74.883198  2.966850e+04   73.038300  2.729328e+04   \n",
       "mean model 2            560.710470  5.937922e+05  557.386864  5.871101e+05   \n",
       "mean model as           134.272970  6.486911e+04  137.730713  6.435749e+04   \n",
       "\n",
       "                          MAE(t+5)      MSE(t+5)  ...   MAE(t+18)  \\\n",
       "name                                              ...               \n",
       "walk 1 model 1          191.836021  1.257932e+05  ...  239.936145   \n",
       "walk 1 model 2          790.706878  1.227559e+06  ...  657.213486   \n",
       "walk 1 model assembled  237.923851  1.789305e+05  ...  249.725255   \n",
       "walk 2 model 1           62.104880  7.650332e+03  ...  291.823872   \n",
       "walk 2 model 2          662.486237  7.217583e+05  ...  649.059020   \n",
       "walk 2 model assembled   79.385070  1.336015e+04  ...  207.547867   \n",
       "walk 3 model 1           37.972773  3.699749e+03  ...  202.658485   \n",
       "walk 3 model 2          521.006565  3.957890e+05  ...  492.137094   \n",
       "walk 3 model assembled  124.533941  5.082550e+04  ...  160.295079   \n",
       "walk 4 model 1           45.282531  5.960852e+03  ...  114.647226   \n",
       "walk 4 model 2          427.542547  2.858979e+05  ...  410.127144   \n",
       "walk 4 model assembled  118.289818  4.824438e+04  ...  131.974094   \n",
       "walk 5 model 1           50.619633  7.319459e+03  ...  201.555691   \n",
       "walk 5 model 2          369.955494  2.558248e+05  ...  480.876624   \n",
       "walk 5 model assembled  126.094859  4.705693e+04  ...  112.250546   \n",
       "mean model 1             77.563168  3.008472e+04  ...  210.124284   \n",
       "mean model 2            554.339544  5.773657e+05  ...  537.882674   \n",
       "mean model as           137.245508  6.768350e+04  ...  172.358568   \n",
       "\n",
       "                            MSE(t+18)   MAE(t+19)      MSE(t+19)   MAE(t+20)  \\\n",
       "name                                                                           \n",
       "walk 1 model 1          114804.513530  272.426521  146042.923094  286.587098   \n",
       "walk 1 model 2          727436.426914  648.669434  702575.216201  640.329349   \n",
       "walk 1 model assembled  153097.665829  267.920236  157619.214255  279.544339   \n",
       "walk 2 model 1          136538.775333  270.571675  113992.431222  301.793588   \n",
       "walk 2 model 2          717857.357715  650.436145  721874.545948  650.174747   \n",
       "walk 2 model assembled   72509.470759  192.820727   61779.693250  218.484094   \n",
       "walk 3 model 1           72923.050808  217.495675   85380.889956  224.421786   \n",
       "walk 3 model 2          422924.883309  486.308763  418327.780337  483.022303   \n",
       "walk 3 model assembled   70328.073548  172.318911   74530.458888  177.962549   \n",
       "walk 4 model 1           30352.643199  127.004964   37660.755215  153.277219   \n",
       "walk 4 model 2          339329.172890  414.030611  354265.997317  416.278998   \n",
       "walk 4 model assembled   53417.493599  134.938036   55835.205677  132.430094   \n",
       "walk 5 model 1           88695.892847  244.489660  129890.339716  222.024775   \n",
       "walk 5 model 2          650002.936169  486.815208  674849.434121  499.302236   \n",
       "walk 5 model assembled   21753.508645  128.779153   29290.900158  121.070510   \n",
       "mean model 1             88662.975143  226.397699  102593.467841  237.620893   \n",
       "mean model 2            571510.155399  537.252032  574378.594785  537.821527   \n",
       "mean model as            74221.242476  179.355413   75811.094446  185.898317   \n",
       "\n",
       "                            MSE(t+20)  nb_test_datapoints  days_train  \\\n",
       "name                                                                    \n",
       "walk 1 model 1          162413.274494               690.0       220.0   \n",
       "walk 1 model 2          680391.055828               690.0       220.0   \n",
       "walk 1 model assembled  171704.346638               690.0       220.0   \n",
       "walk 2 model 1          140616.609658               690.0       250.0   \n",
       "walk 2 model 2          719809.016537               690.0       250.0   \n",
       "walk 2 model assembled   78835.238792               690.0       250.0   \n",
       "walk 3 model 1           88798.277138               690.0       280.0   \n",
       "walk 3 model 2          416224.761455               690.0       280.0   \n",
       "walk 3 model assembled   77686.596600               690.0       280.0   \n",
       "walk 4 model 1           55274.902847               690.0       310.0   \n",
       "walk 4 model 2          363704.758530               690.0       310.0   \n",
       "walk 4 model assembled   47280.935685               690.0       310.0   \n",
       "walk 5 model 1          106467.611036               322.0       340.0   \n",
       "walk 5 model 2          699363.240001               322.0       340.0   \n",
       "walk 5 model assembled   24930.444150               322.0       340.0   \n",
       "mean model 1            110714.135034               616.4       280.0   \n",
       "mean model 2            575898.566470               616.4       280.0   \n",
       "mean model as            80087.512373               616.4       280.0   \n",
       "\n",
       "                        days_valid  days_test  \n",
       "name                                           \n",
       "walk 1 model 1                30.0       30.0  \n",
       "walk 1 model 2                30.0       30.0  \n",
       "walk 1 model assembled        30.0       30.0  \n",
       "walk 2 model 1                30.0       30.0  \n",
       "walk 2 model 2                30.0       30.0  \n",
       "walk 2 model assembled        30.0       30.0  \n",
       "walk 3 model 1                30.0       30.0  \n",
       "walk 3 model 2                30.0       30.0  \n",
       "walk 3 model assembled        30.0       30.0  \n",
       "walk 4 model 1                30.0       30.0  \n",
       "walk 4 model 2                30.0       30.0  \n",
       "walk 4 model assembled        30.0       30.0  \n",
       "walk 5 model 1                30.0       14.0  \n",
       "walk 5 model 2                30.0       14.0  \n",
       "walk 5 model assembled        30.0       14.0  \n",
       "mean model 1                  30.0       26.8  \n",
       "mean model 2                  30.0       26.8  \n",
       "mean model as                 30.0       26.8  \n",
       "\n",
       "[18 rows x 44 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_errors_2 = walk_forward_assembly(get_dense_model, dg, dense_all_inputs, dg_2, get_assemble,\n",
    "                     220, 30, 30, epochs=1000, verbose=0, train_val=True)\n",
    "df_errors_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark for all models\n",
    "Run experiments for all models and save results to csv and source code of model to txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_dense_model\n",
      "results saved to ../res/2021-04-19_get_dense_model.csv\n",
      "results saved to ../res/2021-04-19_get_dense_model.txt\n",
      "get_encoder_decoder\n",
      "results saved to ../res/2021-04-19_get_encoder_decoder.csv\n",
      "results saved to ../res/2021-04-19_get_encoder_decoder.txt\n",
      "get_baseline\n",
      "results saved to ../res/2021-04-19_get_baseline.csv\n",
      "results saved to ../res/2021-04-19_get_baseline.txt\n",
      "get_custom_linear_regression\n",
      "results saved to ../res/2021-04-19_get_custom_linear_regression.csv\n",
      "results saved to ../res/2021-04-19_get_custom_linear_regression.txt\n",
      "get_assemble\n",
      "results saved to ../res/2021-04-19_get_assemble.csv\n",
      "results saved to ../res/2021-04-19_get_assemble.txt\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True  # remove warnings from tensorflow\n",
    "\n",
    "result_dir = \"../res/\"\n",
    "\n",
    "model_list_trainable = [\n",
    "    get_dense_model,\n",
    "    get_encoder_decoder\n",
    "]\n",
    "\n",
    "model_list_untrainable = [\n",
    "    get_baseline,\n",
    "    get_custom_linear_regression,\n",
    "]\n",
    "\n",
    "model_list_assemble = [\n",
    "    (get_dense_model, dg, get_encoder_decoder, dg_2, get_assemble)\n",
    "]\n",
    "\n",
    "nb_fit = 250\n",
    "nb_eval = 10\n",
    "nb_test = 30\n",
    "epochs = 300\n",
    "es_stop_val = True\n",
    "\n",
    "today_date = str(date.today())\n",
    "\n",
    "def benchmark(model_generator, walk):\n",
    "    model_name = model_generator.__name__\n",
    "    print(model_name)\n",
    "    df_errors = walk(model_generator, nb_fit, nb_eval, nb_test, \n",
    "                                        epochs=epochs, verbose=0, es_stop_val=es_stop_val)\n",
    "    # register csv for error\n",
    "    filename = f'{result_dir}{today_date}_{model_name}_{target}'\n",
    "    file_csv = f'{filename}.csv'\n",
    "    file_txt = f'{filename}.txt'\n",
    "    \n",
    "    i = 0\n",
    "    while os.path.isfile(file_csv) or os.path.isfile(file_txt):\n",
    "        i += 1\n",
    "        file_csv = f'{filename}_{i}.csv'\n",
    "        file_txt = f'{filename}_{i}.txt'\n",
    "    df_errors.to_csv(file_csv)\n",
    "    print(f'results saved to {file_csv}')\n",
    "    \n",
    "    # register source code of the model and additional infos\n",
    "    info_txt = inspect.getsource(model_generator) + f'\\n\\nepochs = {epochs}, es_stop_val = {es_stop_val}' + \\\n",
    "        '\\n\\n' + str(dg)\n",
    "    with open(file_txt, 'w') as file:\n",
    "        file.write(info_txt)\n",
    "    print(f'results saved to {file_txt}')\n",
    "\n",
    "for model_generator in model_list_trainable:\n",
    "    benchmark(model_generator, walk_forward_evaluation)\n",
    "    \n",
    "for model_generator in model_list_untrainable:\n",
    "    benchmark(model_generator, walk_forward_evaluation_untrainable)\n",
    "\n",
    "# assembly benchmark\n",
    "for gen_1, dg_model_1, gen_2, dg_model_2, gen_as in model_list_assemble:\n",
    "    model_name = gen_as.__name__\n",
    "    print(model_name)\n",
    "    df_errors = walk_forward_assembly(gen_1, dg_model_1, gen_2, dg_model_2, gen_as,\n",
    "                     nb_fit, nb_eval, nb_test, epochs=epochs, verbose=0)\n",
    "    # register csv for error\n",
    "    filename = f'{result_dir}{today_date}_{model_name}'\n",
    "    file_csv = f'{filename}.csv'\n",
    "    file_txt = f'{filename}.txt'\n",
    "    \n",
    "    i = 0\n",
    "    while os.path.isfile(file_csv) or os.path.isfile(file_txt):\n",
    "        i += 1\n",
    "        file_csv = f'{filename}_{i}.csv'\n",
    "        file_txt = f'{filename}_{i}.txt'\n",
    "    df_errors.to_csv(file_csv)\n",
    "    print(f'results saved to {file_csv}')\n",
    "    \n",
    "    # register source code of the models and additional infos\n",
    "    info_txt = 'model 1:\\n\\n' + inspect.getsource(gen_1) + \\\n",
    "        '\\n\\nmodel 2:\\n\\n' + inspect.getsource(gen_2) + \\\n",
    "        '\\n\\nmodel as:\\n\\n' + inspect.getsource(gen_as) + \\\n",
    "        f'\\n\\nepochs = {epochs}, es_stop_val = {es_stop_val}' + \\\n",
    "        '\\n\\ndg_1:\\n' + str(dg_model_1) + \\\n",
    "        '\\n\\ndg_2:\\n' + str(dg_model_2)\n",
    "    with open(file_txt, 'w') as file:\n",
    "        file.write(info_txt)\n",
    "    print(f'results saved to {file_txt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
