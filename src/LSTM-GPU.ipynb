{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, date, timedelta\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import io\n",
    "from time import sleep\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Reshape, TimeDistributed, LSTM, Lambda, Bidirectional, RepeatVector, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ProgbarLogger, History\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.metrics import get as metric_get\n",
    "import re\n",
    "import talos\n",
    "from copy import deepcopy, copy\n",
    "import time\n",
    "import util\n",
    "import functools \n",
    "import itertools\n",
    "import networkx as nx\n",
    "from typing import List, Iterator, Tuple, Dict, Union\n",
    "import json\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "# Set up GPU:\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "load hospitalisations and trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics considered\n",
    "list_topics = {\n",
    "    'Fièvre': '/m/0cjf0',\n",
    "    'Mal de gorge': '/m/0b76bty',\n",
    "    #'Dyspnée': '/m/01cdt5',\n",
    "    #'Agueusie': '/m/05sfr2',\n",
    "    #'Anosmie': '/m/0m7pl',\n",
    "    #'Coronavirus': '/m/01cpyy',\n",
    "    #'Virus': '/m/0g9pc',\n",
    "    #'Température corporelle humaine': '/g/1213j0cz',\n",
    "    #'Épidémie': '/m/0hn9s',\n",
    "    'Symptôme': '/m/01b_06',\n",
    "    #'Thermomètre': '/m/07mf1',\n",
    "    #'Grippe espagnole': '/m/01c751',\n",
    "    #'Paracétamol': '/m/0lbt3',\n",
    "    #'Respiration': '/m/02gy9_',\n",
    "    #'Toux': '/m/01b_21'\n",
    "}\n",
    "\n",
    "# hospitalisations features given as input\n",
    "list_hosp_features = [\n",
    "    'NEW_HOSP',\n",
    "    #'TOT_HOSP',\n",
    "    #'TOT_HOSP_log',\n",
    "    #'TOT_HOSP_pct',\n",
    "]\n",
    "\n",
    "europe = False  # if True, use european countries. Otherwise, use french regions and belgium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional information: what is the target, should some features remain unscaled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target, should be one of the hosp features\n",
    "target = 'NEW_HOSP'\n",
    "\n",
    "cumsum = False  # if True, the target will be accumulated over each day\n",
    "\n",
    "# features that should not be scaled\n",
    "unscaled = [\n",
    "    #'NEW_HOSP',\n",
    "    #'TOT_HOSP',\n",
    "    #'TOT_HOSP_log',\n",
    "    #'TOT_HOSP_pct',\n",
    "    #'Fièvre',\n",
    "    #'Mal de gorge',\n",
    "    #'Dyspnée',\n",
    "    #'Agueusie',\n",
    "    #'Anosmie',\n",
    "    #'Coronavirus',\n",
    "    #'Virus',\n",
    "    #'Température corporelle humaine',\n",
    "    #'Épidémie',\n",
    "    #'Symptôme',\n",
    "    #'Thermomètre',\n",
    "    #'Grippe espagnole',\n",
    "    #'Paracétamol',\n",
    "    #'Respiration',\n",
    "    #'Toux',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of prediction: how many days as input sould be used to predict how many days as output? Should we give a prediction on all days or only on the last?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_begin = \"2020-02-01\"\n",
    "n_forecast = 20\n",
    "n_samples = 30\n",
    "predict_one = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_metrics = [metric_get(\"MeanSquaredError\"), metric_get('MeanAbsoluteError'), \n",
    "                      metric_get('RootMeanSquaredError')]\n",
    "\n",
    "url_world = \"../data/hospi/world.csv\"\n",
    "url_pop = \"../data/population.txt\"\n",
    "url_trends = \"../data/trends/model/\"\n",
    "url_hospi_belgium = \"../data/hospi/be-covid-hospi.csv\"\n",
    "url_department_france = \"france_departements.csv\"\n",
    "url_hospi_france_new = \"../data/hospi/fr-covid-hospi.csv\"\n",
    "url_hospi_france_tot = \"../data/hospi/fr-covid-hospi-total.csv\"\n",
    "if europe:\n",
    "    population = util.get_world_population(url_pop)\n",
    "    renaming = {v: k for k, v in util.european_geocodes.items()}\n",
    "    geocodes = {k: v for k, v in util.european_geocodes.items() if population[k] > 1_000_000}\n",
    "    df_hospi = util.hospi_world(url_world, geocodes, renaming, new_hosp=True, date_begin=date_begin)\n",
    "    augment_population = {k: v/1000 for k, v in population.items()}\n",
    "else:\n",
    "    geocodes = util.french_region_and_be\n",
    "    population = pd.read_csv(url_department_france).groupby('regionTrends').agg({'population': 'sum'})\n",
    "    augment_population = {k: pop['population'] / 100_000 for k, pop in population.iterrows()}  # pop per 100.000\n",
    "    df_hospi = util.hospi_french_region_and_be(url_hospi_france_tot, url_hospi_france_new, url_hospi_belgium, \n",
    "                                           url_department_france, util.french_region_and_be, new_hosp_in=True, \n",
    "                                           tot_hosp=True, date_begin=date_begin)\n",
    "df_trends = util.create_df_trends(url_trends, list_topics, geocodes)  # TODO deal with augmented data\n",
    "for k in df_hospi.keys(): # Rolling average of 7 days \n",
    "    df_hospi[k] = df_hospi[k].rolling(7, center=True).mean().dropna()\n",
    "    df_trends[k] = df_trends[k].rolling(7, center=True).mean().dropna()\n",
    "merged_df = {k: pd.merge(df_hospi[k], df_trends[k], left_index=True, right_index=True).dropna() for k,v in geocodes.items()}\n",
    "    \n",
    "scaler_generator = MinMaxScaler\n",
    "dg = util.DataGenerator(merged_df, n_samples, n_forecast, target, scaler_generator=scaler_generator, scaler_type='batch',\n",
    "                       augment_merge=3, augment_adjacency=util.france_region_adjacency, augment_population=augment_population,\n",
    "                       predict_one=predict_one, cumsum=cumsum, data_columns=list_hosp_features)\n",
    "n_features = dg.n_features\n",
    "target_idx = dg.target_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not predict_one:\n",
    "    weights_loss = np.array([(1/x) for x in range(1, n_forecast+1)])\n",
    "else:\n",
    "    weights_loss = 1\n",
    "\n",
    "def custom_loss_function(y_true, y_pred):\n",
    "    y_true = y_true * weights_loss\n",
    "    y_pred = y_pred * weights_loss\n",
    "    return tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def get_encoder_decoder(batch_input_shape):\n",
    "    model = Sequential()\n",
    "    #model.add(Bidirectional(LSTM(8, return_sequences=True, stateful=False), \n",
    "    #                        input_shape=(n_samples, n_features), merge_mode=\"ave\"))\n",
    "    model.add(LSTM(16, return_sequences=True, stateful=False, batch_input_shape=batch_input_shape, recurrent_dropout=0))\n",
    "    model.add(LSTM(8, return_sequences=False, stateful=False))\n",
    "    model.add(RepeatVector(n_forecast))  # repeat\n",
    "    model.add(LSTM(8, return_sequences=True, stateful=False))  # dec\n",
    "    if not predict_one:\n",
    "        model.add(LSTM(16, return_sequences=True, stateful=False))  # dec\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.add(Reshape((n_forecast,)))\n",
    "    else:\n",
    "        model.add(LSTM(16, return_sequences=False, stateful=False))  # dec\n",
    "        model.add(Dense(1))\n",
    "        model.add(Reshape((1,)))\n",
    "    model.compile(loss=custom_loss_function, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "    \n",
    "get_encoder_decoder((1, n_samples, n_features)).output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models to beat\n",
    "### MultiStepLastBaseline\n",
    "This model repeats the last value of hospitalisations `n_forecast` time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepLastLayer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    repeat the last hospitalisations given as input n_forecast time\n",
    "    \"\"\"\n",
    "    def call(self, inputs, *args, **kwargs):\n",
    "        a = inputs[:, -1:, target_idx:target_idx+1]  # target of the last days\n",
    "        # a = tf.where(tf.not_equal(a, 0), tf.zeros_like(a), a)\n",
    "        if not predict_one:\n",
    "            return tf.tile(\n",
    "                a,\n",
    "                [1, n_forecast, 1]   # repeat target n_forecast time\n",
    "            )\n",
    "        else:\n",
    "            return tf.tile(a, [1, 1, 1])\n",
    "        \n",
    "class MultiStepLastBaseline(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    repeat the last hospitalisations given as input n_forecast time\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_input_shape, *args, **kwargs):\n",
    "        super(MultiStepLastBaseline, self).__init__(name='')\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((1,)), trainable=False)\n",
    "        self.multi_step = MultiStepLastLayer()\n",
    "        self.reshape = Reshape((n_forecast,))\n",
    "    \n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.multi_step(input_tensor)\n",
    "        return self.reshape(x)\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return None\n",
    "    \n",
    "    def set_weights(self, *args, **kwargs):\n",
    "        return None\n",
    "\n",
    "def get_baseline(*args, **kwargs):\n",
    "    model = MultiStepLastBaseline(*args, **kwargs)\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "linear regression of the last `n_sample` days used to predict the next `n_forecast` days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionHospi(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    repeat the last hospitalisations given as input n_forecast time\n",
    "    \"\"\"\n",
    "    def predict(self, inputs, *args, **kwargs):\n",
    "        y = inputs[:, :, target_idx]  # target of the last days\n",
    "        length = len(inputs)\n",
    "        x = np.arange(n_samples).reshape(-1,1)  # dates of the target\n",
    "        if not predict_one:\n",
    "            result = np.zeros((length, n_forecast))\n",
    "            for i in range(length):\n",
    "                regr = LinearRegression().fit(x, y[i])  # linear regression of (days, target)\n",
    "                result[i] = regr.predict(np.arange(n_samples, n_samples+n_forecast).reshape(-1,1))\n",
    "        else:\n",
    "            result = np.zeros((length, 1))\n",
    "            for i in range(length):\n",
    "                regr = LinearRegression().fit(x, y[i])\n",
    "                result[i] = regre.predict([n_samples+n_forecast-1])\n",
    "        return result\n",
    "        \n",
    "def get_custom_linear_regression(*args, **kwargs):\n",
    "    model = LinearRegressionHospi()\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 fully connected layer (Dense model)\n",
    "Using only the target in the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseModel = Sequential()\n",
    "DenseModel.add(Lambda(lambda x: x[:,:,target_idx]))  # select only the target of the previous days\n",
    "if not predict_one:\n",
    "    DenseModel.add(Dense(n_forecast))   # predict the next target based on the previous ones\n",
    "else:\n",
    "    DenseModel.add(Dense(1))\n",
    "DenseModel.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "def get_dense_model(batch_input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x[:,:,target_idx], batch_input_shape=batch_input_shape))  # select only the target of the previous days\n",
    "    model.add(Dense(n_forecast))   # predict the next target based on the previous ones\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                          metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple prediction\n",
    "use a percentage of values for training and the remaining values for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X% for training, remaining for test\n",
    "ratio_training = 0.9\n",
    "nb_datapoints = dg.batch_size\n",
    "max_train = int(ratio_training * nb_datapoints)\n",
    "train_idx = np.array(range(max_train))\n",
    "test_idx = np.array(range(max_train, nb_datapoints))\n",
    "X_train = dg.get_x(train_idx, scaled=True)\n",
    "Y_train = dg.get_y(train_idx, scaled=True)\n",
    "X_test_unscaled = dg.get_x(test_idx, scaled=False)\n",
    "Y_test_unscaled = dg.get_y(test_idx, scaled=False)\n",
    "X_test = dg.get_x(test_idx, scaled=True, use_previous_scaler=True)\n",
    "Y_test = dg.get_y(test_idx, scaled=True, use_previous_scaler=True)\n",
    "Y_test_unpadded_unscaled = dg.remove_padded_y(Y_test_unscaled, idx=test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (344, 30, 16)             1216      \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (344, 8)                  800       \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (344, 20, 8)              0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (344, 20, 8)              544       \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (344, 20, 16)             1600      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (344, 20, 1)              17        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (344, 20)                 0         \n",
      "=================================================================\n",
      "Total params: 4,177\n",
      "Trainable params: 4,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 3s 6ms/step - loss: 0.0095 - mse: 0.1144 - mae: 0.2359 - root_mean_squared_error: 0.3374\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0067 - mse: 0.0809 - mae: 0.2421 - root_mean_squared_error: 0.2842\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0052 - mse: 0.0546 - mae: 0.1886 - root_mean_squared_error: 0.2337\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0032 - mse: 0.0483 - mae: 0.1479 - root_mean_squared_error: 0.2198\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0020 - mse: 0.0392 - mae: 0.1297 - root_mean_squared_error: 0.1980\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.0012 - mse: 0.0314 - mae: 0.1128 - root_mean_squared_error: 0.1771\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 7.9933e-04 - mse: 0.0277 - mae: 0.1052 - root_mean_squared_error: 0.1665\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.4719e-04 - mse: 0.0269 - mae: 0.1059 - root_mean_squared_error: 0.1640\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.4200e-04 - mse: 0.0257 - mae: 0.1051 - root_mean_squared_error: 0.1601\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 4.9811e-04 - mse: 0.0249 - mae: 0.1022 - root_mean_squared_error: 0.1577\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 4.6176e-04 - mse: 0.0235 - mae: 0.0985 - root_mean_squared_error: 0.1534\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 4.3149e-04 - mse: 0.0227 - mae: 0.0985 - root_mean_squared_error: 0.1506\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 4.1461e-04 - mse: 0.0219 - mae: 0.0958 - root_mean_squared_error: 0.1481\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.9476e-04 - mse: 0.0213 - mae: 0.0955 - root_mean_squared_error: 0.1458\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.4113e-04 - mse: 0.0204 - mae: 0.0931 - root_mean_squared_error: 0.1427\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 3.4758e-04 - mse: 0.0202 - mae: 0.0919 - root_mean_squared_error: 0.1423\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 3.0556e-04 - mse: 0.0192 - mae: 0.0901 - root_mean_squared_error: 0.1384\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.8746e-04 - mse: 0.0188 - mae: 0.0875 - root_mean_squared_error: 0.1372\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.7850e-04 - mse: 0.0185 - mae: 0.0870 - root_mean_squared_error: 0.1360\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.6211e-04 - mse: 0.0179 - mae: 0.0854 - root_mean_squared_error: 0.1338\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.4737e-04 - mse: 0.0174 - mae: 0.0840 - root_mean_squared_error: 0.1317\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.2887e-04 - mse: 0.0171 - mae: 0.0826 - root_mean_squared_error: 0.1308\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.1927e-04 - mse: 0.0168 - mae: 0.0826 - root_mean_squared_error: 0.1298\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 2.1259e-04 - mse: 0.0165 - mae: 0.0810 - root_mean_squared_error: 0.1284\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 2.0346e-04 - mse: 0.0162 - mae: 0.0806 - root_mean_squared_error: 0.1271\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.9812e-04 - mse: 0.0158 - mae: 0.0804 - root_mean_squared_error: 0.1257\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.9356e-04 - mse: 0.0161 - mae: 0.0801 - root_mean_squared_error: 0.1269\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.8600e-04 - mse: 0.0157 - mae: 0.0783 - root_mean_squared_error: 0.1254\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.8170e-04 - mse: 0.0151 - mae: 0.0781 - root_mean_squared_error: 0.1230\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.9534e-04 - mse: 0.0161 - mae: 0.0823 - root_mean_squared_error: 0.1268\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.7709e-04 - mse: 0.0157 - mae: 0.0786 - root_mean_squared_error: 0.1252\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.9032e-04 - mse: 0.0157 - mae: 0.0803 - root_mean_squared_error: 0.1253\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.8244e-04 - mse: 0.0150 - mae: 0.0784 - root_mean_squared_error: 0.1226\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.7491e-04 - mse: 0.0148 - mae: 0.0776 - root_mean_squared_error: 0.1216\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.7288e-04 - mse: 0.0150 - mae: 0.0775 - root_mean_squared_error: 0.1225\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.7461e-04 - mse: 0.0154 - mae: 0.0785 - root_mean_squared_error: 0.1239\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.6391e-04 - mse: 0.0143 - mae: 0.0760 - root_mean_squared_error: 0.1194\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.6211e-04 - mse: 0.0137 - mae: 0.0740 - root_mean_squared_error: 0.1171\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.6551e-04 - mse: 0.0139 - mae: 0.0752 - root_mean_squared_error: 0.1179\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.6823e-04 - mse: 0.0136 - mae: 0.0744 - root_mean_squared_error: 0.1168\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.6208e-04 - mse: 0.0132 - mae: 0.0745 - root_mean_squared_error: 0.1148\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.5554e-04 - mse: 0.0125 - mae: 0.0719 - root_mean_squared_error: 0.1120\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.6242e-04 - mse: 0.0128 - mae: 0.0716 - root_mean_squared_error: 0.1130\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.5317e-04 - mse: 0.0118 - mae: 0.0694 - root_mean_squared_error: 0.1087\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.4292e-04 - mse: 0.0114 - mae: 0.0679 - root_mean_squared_error: 0.1069\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.4436e-04 - mse: 0.0115 - mae: 0.0681 - root_mean_squared_error: 0.1075\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.4372e-04 - mse: 0.0113 - mae: 0.0682 - root_mean_squared_error: 0.1063\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.4919e-04 - mse: 0.0114 - mae: 0.0678 - root_mean_squared_error: 0.1068\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 1.3542e-04 - mse: 0.0109 - mae: 0.0657 - root_mean_squared_error: 0.1045\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 1.3749e-04 - mse: 0.0107 - mae: 0.0656 - root_mean_squared_error: 0.1034\n",
      "computed in 0:00:10.143686 s\n"
     ]
    }
   ],
   "source": [
    "batch_input = len(train_idx)\n",
    "encoder_decoder_train = get_encoder_decoder(batch_input_shape=(batch_input, n_samples, n_features))\n",
    "encoder_decoder_train.summary()\n",
    "start = time.time()\n",
    "encoder_decoder_train.fit(X_train, Y_train, epochs=50, verbose=1, batch_size=batch_input)\n",
    "end = time.time() - start\n",
    "print(f\"computed in {str(str(timedelta(seconds=end)))} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00399999 -0.00319381 -0.00462736 -0.00495593 -0.00505185 -0.0059531\n",
      " -0.00735401 -0.00921627 -0.01118677 -0.01355288 -0.01635245 -0.01905055\n",
      " -0.02181027 -0.02537839 -0.02906431 -0.03315539 -0.03714128 -0.0412641\n",
      " -0.04614591 -0.05104901]\n",
      "[-1.05436368  0.37067537  0.44653425  0.65145979  0.84681332  0.95984233\n",
      "  1.00716274  1.01253387  1.00897923  0.97270614  0.89913701  0.91584237\n",
      "  0.96087507  0.84870719  0.71453177  0.54581086  0.35548846  0.13264262\n",
      " -0.21607304 -0.63400138]\n",
      "[ 2.33457621  3.38391714  4.27078106  5.0814401   5.9187545   6.88740913\n",
      "  7.75030797  8.34271001  8.98803277  9.66452356 10.33478669 11.03699452\n",
      " 11.75513771 12.45663287 13.10032979 13.78414784 14.40715156 15.10399105\n",
      " 15.85113844 16.47614451]\n"
     ]
    }
   ],
   "source": [
    "batch_input = len(test_idx)\n",
    "encoder_decoder_prediction = get_encoder_decoder(batch_input_shape=(batch_input, n_samples, n_features) )\n",
    "encoder_decoder_prediction.set_weights(encoder_decoder_train.get_weights())\n",
    "\n",
    "Y_predicted = np.squeeze(encoder_decoder_prediction.predict(X_test, batch_size=batch_input))\n",
    "Y_predicted_unscaled = dg.inverse_transform_y(Y_predicted, idx=test_idx)\n",
    "\n",
    "print(np.mean(Y_predicted - Y_test, axis=0))\n",
    "print(np.mean(Y_predicted_unscaled - Y_test_unscaled, axis=0))\n",
    "\n",
    "Y_predicted_unpadded_unscaled =  dg.remove_padded_y(Y_predicted_unscaled, idx=test_idx)\n",
    "\n",
    "print(np.mean(abs(Y_predicted_unpadded_unscaled - Y_test_unpadded_unscaled), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.46074878  -0.15338165  -0.40157006   0.74939613  -2.39673914\n",
      "  -2.88345411   0.05736714  -0.38828503  -3.97644928  -6.80736716\n",
      "  -8.93417875 -12.83595009 -16.29206925 -14.67572464 -17.6491546\n",
      " -21.76751208 -27.00664252 -30.76871981 -36.3254831  -36.26026571]\n",
      "[ 97.23976213 121.77371275 121.59357122 122.7349443  128.44000302\n",
      " 105.06647095  67.53846734 110.9022132  134.43473352 129.85358327\n",
      " 136.07685939 141.34590485 126.69369166  95.88279133 132.96303825\n",
      " 152.38068353 148.63060826 152.68435713 156.96830775 143.07008432]\n"
     ]
    }
   ],
   "source": [
    "baseline_model= get_baseline()\n",
    "Y_predicted_unscaled = np.squeeze(baseline_model.predict(X_test_unscaled))\n",
    "\n",
    "print(np.mean(Y_predicted_unscaled - Y_test_unscaled, axis=0))\n",
    "\n",
    "Y_predicted_unpadded_unscaled =  dg.remove_padded_y(Y_predicted_unscaled, idx=test_idx)\n",
    "\n",
    "print(np.mean(abs(Y_predicted_unpadded_unscaled - Y_test_unpadded_unscaled), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (1, 30)                   0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (1, 20)                   620       \n",
      "=================================================================\n",
      "Total params: 620\n",
      "Trainable params: 620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 1.5908 - mse: 1.5908 - mae: 0.7549 - root_mean_squared_error: 1.2600\n",
      "Epoch 2/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 1.0971 - mse: 1.0971 - mae: 0.6273 - root_mean_squared_error: 1.0471\n",
      "Epoch 3/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.9083 - mse: 0.9083 - mae: 0.5514 - root_mean_squared_error: 0.9530\n",
      "Epoch 4/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8290 - mse: 0.8290 - mae: 0.5294 - root_mean_squared_error: 0.9104\n",
      "Epoch 5/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7829 - mse: 0.7829 - mae: 0.5089 - root_mean_squared_error: 0.8840\n",
      "Epoch 6/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7777 - mse: 0.7777 - mae: 0.5034 - root_mean_squared_error: 0.8817\n",
      "Epoch 7/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7990 - mse: 0.7990 - mae: 0.5138 - root_mean_squared_error: 0.8937\n",
      "Epoch 8/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8278 - mse: 0.8278 - mae: 0.5058 - root_mean_squared_error: 0.9093\n",
      "Epoch 9/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7990 - mse: 0.7990 - mae: 0.5016 - root_mean_squared_error: 0.8937\n",
      "Epoch 10/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7891 - mse: 0.7891 - mae: 0.5046 - root_mean_squared_error: 0.8882\n",
      "Epoch 11/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8128 - mse: 0.8128 - mae: 0.5141 - root_mean_squared_error: 0.9015\n",
      "Epoch 12/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8053 - mse: 0.8053 - mae: 0.5037 - root_mean_squared_error: 0.8971\n",
      "Epoch 13/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7967 - mse: 0.7967 - mae: 0.5048 - root_mean_squared_error: 0.8924\n",
      "Epoch 14/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7787 - mse: 0.7787 - mae: 0.5017 - root_mean_squared_error: 0.8823\n",
      "Epoch 15/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7989 - mse: 0.7989 - mae: 0.5052 - root_mean_squared_error: 0.8936\n",
      "Epoch 16/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7864 - mse: 0.7864 - mae: 0.4972 - root_mean_squared_error: 0.8865\n",
      "Epoch 17/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7686 - mse: 0.7686 - mae: 0.4961 - root_mean_squared_error: 0.8764\n",
      "Epoch 18/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8093 - mse: 0.8093 - mae: 0.5068 - root_mean_squared_error: 0.8994\n",
      "Epoch 19/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8094 - mse: 0.8094 - mae: 0.5115 - root_mean_squared_error: 0.8989\n",
      "Epoch 20/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8146 - mse: 0.8146 - mae: 0.5156 - root_mean_squared_error: 0.9024\n",
      "Epoch 21/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7942 - mse: 0.7942 - mae: 0.5088 - root_mean_squared_error: 0.8909\n",
      "Epoch 22/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7580 - mse: 0.7580 - mae: 0.4940 - root_mean_squared_error: 0.8703\n",
      "Epoch 23/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8113 - mse: 0.8113 - mae: 0.5112 - root_mean_squared_error: 0.9003\n",
      "Epoch 24/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7863 - mse: 0.7863 - mae: 0.5007 - root_mean_squared_error: 0.8866\n",
      "Epoch 25/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8132 - mse: 0.8132 - mae: 0.5090 - root_mean_squared_error: 0.9016\n",
      "Epoch 26/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7765 - mse: 0.7765 - mae: 0.5017 - root_mean_squared_error: 0.8811\n",
      "Epoch 27/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7990 - mse: 0.7990 - mae: 0.5075 - root_mean_squared_error: 0.8937\n",
      "Epoch 28/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8003 - mse: 0.8003 - mae: 0.5069 - root_mean_squared_error: 0.8945\n",
      "Epoch 29/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7958 - mse: 0.7958 - mae: 0.5007 - root_mean_squared_error: 0.8920\n",
      "Epoch 30/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8110 - mse: 0.8110 - mae: 0.5122 - root_mean_squared_error: 0.9004\n",
      "Epoch 31/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7910 - mse: 0.7910 - mae: 0.5105 - root_mean_squared_error: 0.8893\n",
      "Epoch 32/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7890 - mse: 0.7890 - mae: 0.5022 - root_mean_squared_error: 0.8882\n",
      "Epoch 33/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8114 - mse: 0.8114 - mae: 0.5130 - root_mean_squared_error: 0.9007\n",
      "Epoch 34/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8010 - mse: 0.8010 - mae: 0.5091 - root_mean_squared_error: 0.8949\n",
      "Epoch 35/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8018 - mse: 0.8018 - mae: 0.5076 - root_mean_squared_error: 0.8953\n",
      "Epoch 36/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8167 - mse: 0.8167 - mae: 0.5124 - root_mean_squared_error: 0.9033\n",
      "Epoch 37/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7856 - mse: 0.7856 - mae: 0.5051 - root_mean_squared_error: 0.8862\n",
      "Epoch 38/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7859 - mse: 0.7859 - mae: 0.5008 - root_mean_squared_error: 0.8864\n",
      "Epoch 39/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7857 - mse: 0.7857 - mae: 0.5060 - root_mean_squared_error: 0.8862\n",
      "Epoch 40/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8101 - mse: 0.8101 - mae: 0.5156 - root_mean_squared_error: 0.8999\n",
      "Epoch 41/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8048 - mse: 0.8048 - mae: 0.5109 - root_mean_squared_error: 0.8970\n",
      "Epoch 42/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7825 - mse: 0.7825 - mae: 0.5051 - root_mean_squared_error: 0.8840\n",
      "Epoch 43/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7675 - mse: 0.7675 - mae: 0.5024 - root_mean_squared_error: 0.8759\n",
      "Epoch 44/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8163 - mse: 0.8163 - mae: 0.5105 - root_mean_squared_error: 0.9032\n",
      "Epoch 45/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7890 - mse: 0.7890 - mae: 0.5047 - root_mean_squared_error: 0.8881\n",
      "Epoch 46/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7855 - mse: 0.7855 - mae: 0.5068 - root_mean_squared_error: 0.8862\n",
      "Epoch 47/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8004 - mse: 0.8004 - mae: 0.5112 - root_mean_squared_error: 0.8946\n",
      "Epoch 48/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7871 - mse: 0.7871 - mae: 0.4978 - root_mean_squared_error: 0.8869\n",
      "Epoch 49/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8010 - mse: 0.8010 - mae: 0.5071 - root_mean_squared_error: 0.8949\n",
      "Epoch 50/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7982 - mse: 0.7982 - mae: 0.5152 - root_mean_squared_error: 0.8933\n",
      "Epoch 51/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7880 - mse: 0.7880 - mae: 0.5013 - root_mean_squared_error: 0.8873\n",
      "Epoch 52/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7786 - mse: 0.7786 - mae: 0.5024 - root_mean_squared_error: 0.8820\n",
      "Epoch 53/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7998 - mse: 0.7998 - mae: 0.5082 - root_mean_squared_error: 0.8942\n",
      "Epoch 54/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8063 - mse: 0.8063 - mae: 0.5102 - root_mean_squared_error: 0.8978\n",
      "Epoch 55/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8024 - mse: 0.8024 - mae: 0.5059 - root_mean_squared_error: 0.8955\n",
      "Epoch 56/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8025 - mse: 0.8025 - mae: 0.5088 - root_mean_squared_error: 0.8957\n",
      "Epoch 57/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7873 - mse: 0.7873 - mae: 0.5030 - root_mean_squared_error: 0.8869\n",
      "Epoch 58/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7876 - mse: 0.7876 - mae: 0.5056 - root_mean_squared_error: 0.8869\n",
      "Epoch 59/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8158 - mse: 0.8158 - mae: 0.5128 - root_mean_squared_error: 0.9030\n",
      "Epoch 60/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7955 - mse: 0.7955 - mae: 0.5085 - root_mean_squared_error: 0.8918\n",
      "Epoch 61/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7936 - mse: 0.7936 - mae: 0.5072 - root_mean_squared_error: 0.8907\n",
      "Epoch 62/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7774 - mse: 0.7774 - mae: 0.5017 - root_mean_squared_error: 0.8813\n",
      "Epoch 63/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7638 - mse: 0.7638 - mae: 0.4982 - root_mean_squared_error: 0.8738\n",
      "Epoch 64/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8145 - mse: 0.8145 - mae: 0.5087 - root_mean_squared_error: 0.9024\n",
      "Epoch 65/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7863 - mse: 0.7863 - mae: 0.5031 - root_mean_squared_error: 0.8867\n",
      "Epoch 66/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7749 - mse: 0.7749 - mae: 0.4985 - root_mean_squared_error: 0.8801\n",
      "Epoch 67/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7931 - mse: 0.7931 - mae: 0.5002 - root_mean_squared_error: 0.8905\n",
      "Epoch 68/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8207 - mse: 0.8207 - mae: 0.5166 - root_mean_squared_error: 0.9058\n",
      "Epoch 69/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8054 - mse: 0.8054 - mae: 0.5083 - root_mean_squared_error: 0.8972\n",
      "Epoch 70/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7634 - mse: 0.7634 - mae: 0.4942 - root_mean_squared_error: 0.8730\n",
      "Epoch 71/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8029 - mse: 0.8029 - mae: 0.5190 - root_mean_squared_error: 0.8960\n",
      "Epoch 72/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8210 - mse: 0.8210 - mae: 0.5127 - root_mean_squared_error: 0.9059\n",
      "Epoch 73/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7871 - mse: 0.7871 - mae: 0.5013 - root_mean_squared_error: 0.8870\n",
      "Epoch 74/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8180 - mse: 0.8180 - mae: 0.5134 - root_mean_squared_error: 0.9038\n",
      "Epoch 75/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7475 - mse: 0.7475 - mae: 0.4934 - root_mean_squared_error: 0.8643\n",
      "Epoch 76/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7790 - mse: 0.7790 - mae: 0.5039 - root_mean_squared_error: 0.8825\n",
      "Epoch 77/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7526 - mse: 0.7526 - mae: 0.4957 - root_mean_squared_error: 0.8673\n",
      "Epoch 78/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8237 - mse: 0.8237 - mae: 0.5166 - root_mean_squared_error: 0.9073\n",
      "Epoch 79/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8093 - mse: 0.8093 - mae: 0.5135 - root_mean_squared_error: 0.8995\n",
      "Epoch 80/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7865 - mse: 0.7865 - mae: 0.5006 - root_mean_squared_error: 0.8868\n",
      "Epoch 81/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8172 - mse: 0.8172 - mae: 0.5111 - root_mean_squared_error: 0.9036\n",
      "Epoch 82/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8137 - mse: 0.8137 - mae: 0.5163 - root_mean_squared_error: 0.9020\n",
      "Epoch 83/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8241 - mse: 0.8241 - mae: 0.5095 - root_mean_squared_error: 0.9075\n",
      "Epoch 84/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7896 - mse: 0.7896 - mae: 0.5020 - root_mean_squared_error: 0.8883\n",
      "Epoch 85/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8021 - mse: 0.8021 - mae: 0.5093 - root_mean_squared_error: 0.8955\n",
      "Epoch 86/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7862 - mse: 0.7862 - mae: 0.5023 - root_mean_squared_error: 0.8865\n",
      "Epoch 87/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7766 - mse: 0.7766 - mae: 0.5015 - root_mean_squared_error: 0.8811\n",
      "Epoch 88/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8325 - mse: 0.8325 - mae: 0.5179 - root_mean_squared_error: 0.9121\n",
      "Epoch 89/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7887 - mse: 0.7887 - mae: 0.4974 - root_mean_squared_error: 0.8880\n",
      "Epoch 90/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7967 - mse: 0.7967 - mae: 0.5099 - root_mean_squared_error: 0.8925\n",
      "Epoch 91/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7962 - mse: 0.7962 - mae: 0.5067 - root_mean_squared_error: 0.8922\n",
      "Epoch 92/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8122 - mse: 0.8122 - mae: 0.5131 - root_mean_squared_error: 0.9011\n",
      "Epoch 93/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8054 - mse: 0.8054 - mae: 0.5132 - root_mean_squared_error: 0.8973\n",
      "Epoch 94/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7737 - mse: 0.7737 - mae: 0.4998 - root_mean_squared_error: 0.8794\n",
      "Epoch 95/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7849 - mse: 0.7849 - mae: 0.5061 - root_mean_squared_error: 0.8856\n",
      "Epoch 96/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7859 - mse: 0.7859 - mae: 0.5052 - root_mean_squared_error: 0.8863\n",
      "Epoch 97/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7675 - mse: 0.7675 - mae: 0.5010 - root_mean_squared_error: 0.8757\n",
      "Epoch 98/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7746 - mse: 0.7746 - mae: 0.4978 - root_mean_squared_error: 0.8800\n",
      "Epoch 99/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8233 - mse: 0.8233 - mae: 0.5189 - root_mean_squared_error: 0.9070\n",
      "Epoch 100/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8123 - mse: 0.8123 - mae: 0.5036 - root_mean_squared_error: 0.9006\n",
      "Epoch 101/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8064 - mse: 0.8064 - mae: 0.5085 - root_mean_squared_error: 0.8979\n",
      "Epoch 102/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8047 - mse: 0.8047 - mae: 0.5065 - root_mean_squared_error: 0.8969\n",
      "Epoch 103/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7878 - mse: 0.7878 - mae: 0.5082 - root_mean_squared_error: 0.8875\n",
      "Epoch 104/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8060 - mse: 0.8060 - mae: 0.5069 - root_mean_squared_error: 0.8977\n",
      "Epoch 105/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7929 - mse: 0.7929 - mae: 0.5072 - root_mean_squared_error: 0.8904\n",
      "Epoch 106/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7816 - mse: 0.7816 - mae: 0.5033 - root_mean_squared_error: 0.8839\n",
      "Epoch 107/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8031 - mse: 0.8031 - mae: 0.5040 - root_mean_squared_error: 0.8958\n",
      "Epoch 108/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7829 - mse: 0.7829 - mae: 0.5040 - root_mean_squared_error: 0.8846\n",
      "Epoch 109/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7915 - mse: 0.7915 - mae: 0.5078 - root_mean_squared_error: 0.8895\n",
      "Epoch 110/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8169 - mse: 0.8169 - mae: 0.5109 - root_mean_squared_error: 0.9037\n",
      "Epoch 111/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8016 - mse: 0.8016 - mae: 0.5088 - root_mean_squared_error: 0.8953\n",
      "Epoch 112/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7935 - mse: 0.7935 - mae: 0.5031 - root_mean_squared_error: 0.8907\n",
      "Epoch 113/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7977 - mse: 0.7977 - mae: 0.5078 - root_mean_squared_error: 0.8930\n",
      "Epoch 114/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7903 - mse: 0.7903 - mae: 0.5050 - root_mean_squared_error: 0.8889\n",
      "Epoch 115/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7931 - mse: 0.7931 - mae: 0.5090 - root_mean_squared_error: 0.8905\n",
      "Epoch 116/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8183 - mse: 0.8183 - mae: 0.5147 - root_mean_squared_error: 0.9043\n",
      "Epoch 117/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7999 - mse: 0.7999 - mae: 0.5066 - root_mean_squared_error: 0.8942\n",
      "Epoch 118/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7959 - mse: 0.7959 - mae: 0.5026 - root_mean_squared_error: 0.8920\n",
      "Epoch 119/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8041 - mse: 0.8041 - mae: 0.5046 - root_mean_squared_error: 0.8964\n",
      "Epoch 120/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7813 - mse: 0.7813 - mae: 0.4990 - root_mean_squared_error: 0.8838\n",
      "Epoch 121/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8172 - mse: 0.8172 - mae: 0.5121 - root_mean_squared_error: 0.9039\n",
      "Epoch 122/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8077 - mse: 0.8077 - mae: 0.5044 - root_mean_squared_error: 0.8983\n",
      "Epoch 123/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7855 - mse: 0.7855 - mae: 0.4999 - root_mean_squared_error: 0.8862\n",
      "Epoch 124/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8113 - mse: 0.8113 - mae: 0.5094 - root_mean_squared_error: 0.9006\n",
      "Epoch 125/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7737 - mse: 0.7737 - mae: 0.4942 - root_mean_squared_error: 0.8794\n",
      "Epoch 126/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8239 - mse: 0.8239 - mae: 0.5183 - root_mean_squared_error: 0.9075\n",
      "Epoch 127/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7991 - mse: 0.7991 - mae: 0.4998 - root_mean_squared_error: 0.8939\n",
      "Epoch 128/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7790 - mse: 0.7790 - mae: 0.5005 - root_mean_squared_error: 0.8824\n",
      "Epoch 129/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8097 - mse: 0.8097 - mae: 0.5087 - root_mean_squared_error: 0.8998\n",
      "Epoch 130/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8128 - mse: 0.8128 - mae: 0.5107 - root_mean_squared_error: 0.9012\n",
      "Epoch 131/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8105 - mse: 0.8105 - mae: 0.5120 - root_mean_squared_error: 0.9000\n",
      "Epoch 132/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8110 - mse: 0.8110 - mae: 0.5116 - root_mean_squared_error: 0.9002\n",
      "Epoch 133/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7846 - mse: 0.7846 - mae: 0.5053 - root_mean_squared_error: 0.8855\n",
      "Epoch 134/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7852 - mse: 0.7852 - mae: 0.5025 - root_mean_squared_error: 0.8861\n",
      "Epoch 135/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7966 - mse: 0.7966 - mae: 0.5035 - root_mean_squared_error: 0.8923\n",
      "Epoch 136/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8057 - mse: 0.8057 - mae: 0.5035 - root_mean_squared_error: 0.8974\n",
      "Epoch 137/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7687 - mse: 0.7687 - mae: 0.5007 - root_mean_squared_error: 0.8766\n",
      "Epoch 138/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8183 - mse: 0.8183 - mae: 0.5155 - root_mean_squared_error: 0.9044\n",
      "Epoch 139/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7808 - mse: 0.7808 - mae: 0.5036 - root_mean_squared_error: 0.8835\n",
      "Epoch 140/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8075 - mse: 0.8075 - mae: 0.5126 - root_mean_squared_error: 0.8985\n",
      "Epoch 141/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8301 - mse: 0.8301 - mae: 0.5132 - root_mean_squared_error: 0.9109\n",
      "Epoch 142/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7821 - mse: 0.7821 - mae: 0.4969 - root_mean_squared_error: 0.8842\n",
      "Epoch 143/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8009 - mse: 0.8009 - mae: 0.5053 - root_mean_squared_error: 0.8948\n",
      "Epoch 144/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7780 - mse: 0.7780 - mae: 0.5057 - root_mean_squared_error: 0.8818\n",
      "Epoch 145/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7859 - mse: 0.7859 - mae: 0.5033 - root_mean_squared_error: 0.8864\n",
      "Epoch 146/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7984 - mse: 0.7984 - mae: 0.5078 - root_mean_squared_error: 0.8935\n",
      "Epoch 147/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7808 - mse: 0.7808 - mae: 0.5016 - root_mean_squared_error: 0.8835\n",
      "Epoch 148/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7907 - mse: 0.7907 - mae: 0.5001 - root_mean_squared_error: 0.8888\n",
      "Epoch 149/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7943 - mse: 0.7943 - mae: 0.5069 - root_mean_squared_error: 0.8912\n",
      "Epoch 150/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7886 - mse: 0.7886 - mae: 0.5062 - root_mean_squared_error: 0.8879\n",
      "Epoch 151/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8069 - mse: 0.8069 - mae: 0.5077 - root_mean_squared_error: 0.8981\n",
      "Epoch 152/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7762 - mse: 0.7762 - mae: 0.4997 - root_mean_squared_error: 0.8808\n",
      "Epoch 153/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8214 - mse: 0.8214 - mae: 0.5076 - root_mean_squared_error: 0.9062\n",
      "Epoch 154/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7798 - mse: 0.7798 - mae: 0.5016 - root_mean_squared_error: 0.8829\n",
      "Epoch 155/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7958 - mse: 0.7958 - mae: 0.5085 - root_mean_squared_error: 0.8920\n",
      "Epoch 156/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7949 - mse: 0.7949 - mae: 0.5055 - root_mean_squared_error: 0.8914\n",
      "Epoch 157/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8079 - mse: 0.8079 - mae: 0.5123 - root_mean_squared_error: 0.8987\n",
      "Epoch 158/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8190 - mse: 0.8190 - mae: 0.5125 - root_mean_squared_error: 0.9049\n",
      "Epoch 159/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7963 - mse: 0.7963 - mae: 0.5008 - root_mean_squared_error: 0.8921\n",
      "Epoch 160/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7773 - mse: 0.7773 - mae: 0.5024 - root_mean_squared_error: 0.8815\n",
      "Epoch 161/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7941 - mse: 0.7941 - mae: 0.5058 - root_mean_squared_error: 0.8911\n",
      "Epoch 162/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7912 - mse: 0.7912 - mae: 0.5031 - root_mean_squared_error: 0.8894\n",
      "Epoch 163/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7920 - mse: 0.7920 - mae: 0.5028 - root_mean_squared_error: 0.8898\n",
      "Epoch 164/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8047 - mse: 0.8047 - mae: 0.5069 - root_mean_squared_error: 0.8968\n",
      "Epoch 165/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7862 - mse: 0.7862 - mae: 0.5057 - root_mean_squared_error: 0.8866\n",
      "Epoch 166/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7782 - mse: 0.7782 - mae: 0.5016 - root_mean_squared_error: 0.8818\n",
      "Epoch 167/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8052 - mse: 0.8052 - mae: 0.5078 - root_mean_squared_error: 0.8967\n",
      "Epoch 168/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7525 - mse: 0.7525 - mae: 0.4896 - root_mean_squared_error: 0.8670\n",
      "Epoch 169/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7877 - mse: 0.7877 - mae: 0.5028 - root_mean_squared_error: 0.8872\n",
      "Epoch 170/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7843 - mse: 0.7843 - mae: 0.5039 - root_mean_squared_error: 0.8855\n",
      "Epoch 171/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7883 - mse: 0.7883 - mae: 0.5039 - root_mean_squared_error: 0.8877\n",
      "Epoch 172/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8149 - mse: 0.8149 - mae: 0.5092 - root_mean_squared_error: 0.9021\n",
      "Epoch 173/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8100 - mse: 0.8100 - mae: 0.5139 - root_mean_squared_error: 0.8999\n",
      "Epoch 174/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7744 - mse: 0.7744 - mae: 0.5043 - root_mean_squared_error: 0.8798\n",
      "Epoch 175/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7932 - mse: 0.7932 - mae: 0.5091 - root_mean_squared_error: 0.8904\n",
      "Epoch 176/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7856 - mse: 0.7856 - mae: 0.5044 - root_mean_squared_error: 0.8863\n",
      "Epoch 177/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7953 - mse: 0.7953 - mae: 0.5063 - root_mean_squared_error: 0.8916\n",
      "Epoch 178/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8094 - mse: 0.8094 - mae: 0.5067 - root_mean_squared_error: 0.8996\n",
      "Epoch 179/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8137 - mse: 0.8137 - mae: 0.5121 - root_mean_squared_error: 0.9019\n",
      "Epoch 180/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7850 - mse: 0.7850 - mae: 0.4995 - root_mean_squared_error: 0.8859\n",
      "Epoch 181/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7999 - mse: 0.7999 - mae: 0.5046 - root_mean_squared_error: 0.8942\n",
      "Epoch 182/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7846 - mse: 0.7846 - mae: 0.5030 - root_mean_squared_error: 0.8857\n",
      "Epoch 183/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8117 - mse: 0.8117 - mae: 0.5135 - root_mean_squared_error: 0.9008\n",
      "Epoch 184/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7963 - mse: 0.7963 - mae: 0.5028 - root_mean_squared_error: 0.8921\n",
      "Epoch 185/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7920 - mse: 0.7920 - mae: 0.5047 - root_mean_squared_error: 0.8898\n",
      "Epoch 186/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7980 - mse: 0.7980 - mae: 0.5061 - root_mean_squared_error: 0.8932\n",
      "Epoch 187/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7684 - mse: 0.7684 - mae: 0.4971 - root_mean_squared_error: 0.8764\n",
      "Epoch 188/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7833 - mse: 0.7833 - mae: 0.5048 - root_mean_squared_error: 0.8850\n",
      "Epoch 189/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7719 - mse: 0.7719 - mae: 0.5021 - root_mean_squared_error: 0.8785\n",
      "Epoch 190/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7843 - mse: 0.7843 - mae: 0.5054 - root_mean_squared_error: 0.8855\n",
      "Epoch 191/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7946 - mse: 0.7946 - mae: 0.5081 - root_mean_squared_error: 0.8912\n",
      "Epoch 192/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8040 - mse: 0.8040 - mae: 0.5082 - root_mean_squared_error: 0.8966\n",
      "Epoch 193/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7766 - mse: 0.7766 - mae: 0.4989 - root_mean_squared_error: 0.8810\n",
      "Epoch 194/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8006 - mse: 0.8006 - mae: 0.5051 - root_mean_squared_error: 0.8945\n",
      "Epoch 195/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7875 - mse: 0.7875 - mae: 0.5011 - root_mean_squared_error: 0.8872\n",
      "Epoch 196/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7765 - mse: 0.7765 - mae: 0.4967 - root_mean_squared_error: 0.8809\n",
      "Epoch 197/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7916 - mse: 0.7916 - mae: 0.5033 - root_mean_squared_error: 0.8891\n",
      "Epoch 198/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8152 - mse: 0.8152 - mae: 0.5116 - root_mean_squared_error: 0.9026\n",
      "Epoch 199/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7737 - mse: 0.7737 - mae: 0.4970 - root_mean_squared_error: 0.8791\n",
      "Epoch 200/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7918 - mse: 0.7918 - mae: 0.5071 - root_mean_squared_error: 0.8898\n",
      "Epoch 201/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7744 - mse: 0.7744 - mae: 0.4977 - root_mean_squared_error: 0.8797\n",
      "Epoch 202/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7986 - mse: 0.7986 - mae: 0.5080 - root_mean_squared_error: 0.8936\n",
      "Epoch 203/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7821 - mse: 0.7821 - mae: 0.5022 - root_mean_squared_error: 0.8840\n",
      "Epoch 204/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8103 - mse: 0.8103 - mae: 0.5102 - root_mean_squared_error: 0.8999\n",
      "Epoch 205/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7860 - mse: 0.7860 - mae: 0.5024 - root_mean_squared_error: 0.8864\n",
      "Epoch 206/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7759 - mse: 0.7759 - mae: 0.4969 - root_mean_squared_error: 0.8808\n",
      "Epoch 207/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7999 - mse: 0.7999 - mae: 0.5160 - root_mean_squared_error: 0.8942\n",
      "Epoch 208/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8033 - mse: 0.8033 - mae: 0.5057 - root_mean_squared_error: 0.8961\n",
      "Epoch 209/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8137 - mse: 0.8137 - mae: 0.5092 - root_mean_squared_error: 0.9019\n",
      "Epoch 210/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8234 - mse: 0.8234 - mae: 0.5162 - root_mean_squared_error: 0.9072\n",
      "Epoch 211/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7698 - mse: 0.7698 - mae: 0.4973 - root_mean_squared_error: 0.8772\n",
      "Epoch 212/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8075 - mse: 0.8075 - mae: 0.5120 - root_mean_squared_error: 0.8983\n",
      "Epoch 213/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7817 - mse: 0.7817 - mae: 0.5006 - root_mean_squared_error: 0.8840\n",
      "Epoch 214/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7633 - mse: 0.7633 - mae: 0.4957 - root_mean_squared_error: 0.8734\n",
      "Epoch 215/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7955 - mse: 0.7955 - mae: 0.5065 - root_mean_squared_error: 0.8916\n",
      "Epoch 216/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7796 - mse: 0.7796 - mae: 0.4980 - root_mean_squared_error: 0.8826\n",
      "Epoch 217/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7783 - mse: 0.7783 - mae: 0.4943 - root_mean_squared_error: 0.8821\n",
      "Epoch 218/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7749 - mse: 0.7749 - mae: 0.5003 - root_mean_squared_error: 0.8797\n",
      "Epoch 219/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7633 - mse: 0.7633 - mae: 0.4977 - root_mean_squared_error: 0.8735\n",
      "Epoch 220/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7894 - mse: 0.7894 - mae: 0.5028 - root_mean_squared_error: 0.8884\n",
      "Epoch 221/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8074 - mse: 0.8074 - mae: 0.5105 - root_mean_squared_error: 0.8984\n",
      "Epoch 222/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7810 - mse: 0.7810 - mae: 0.5009 - root_mean_squared_error: 0.8834\n",
      "Epoch 223/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8222 - mse: 0.8222 - mae: 0.5133 - root_mean_squared_error: 0.9066\n",
      "Epoch 224/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7952 - mse: 0.7952 - mae: 0.5062 - root_mean_squared_error: 0.8917\n",
      "Epoch 225/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8022 - mse: 0.8022 - mae: 0.5108 - root_mean_squared_error: 0.8956\n",
      "Epoch 226/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8027 - mse: 0.8027 - mae: 0.5081 - root_mean_squared_error: 0.8957\n",
      "Epoch 227/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7823 - mse: 0.7823 - mae: 0.5039 - root_mean_squared_error: 0.8843\n",
      "Epoch 228/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8001 - mse: 0.8001 - mae: 0.5066 - root_mean_squared_error: 0.8944\n",
      "Epoch 229/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8208 - mse: 0.8208 - mae: 0.5096 - root_mean_squared_error: 0.9055\n",
      "Epoch 230/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7778 - mse: 0.7778 - mae: 0.4990 - root_mean_squared_error: 0.8818\n",
      "Epoch 231/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8003 - mse: 0.8003 - mae: 0.5094 - root_mean_squared_error: 0.8945\n",
      "Epoch 232/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8043 - mse: 0.8043 - mae: 0.5085 - root_mean_squared_error: 0.8968\n",
      "Epoch 233/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8227 - mse: 0.8227 - mae: 0.5189 - root_mean_squared_error: 0.9069\n",
      "Epoch 234/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8026 - mse: 0.8026 - mae: 0.5068 - root_mean_squared_error: 0.8957\n",
      "Epoch 235/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8139 - mse: 0.8139 - mae: 0.5118 - root_mean_squared_error: 0.9021\n",
      "Epoch 236/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8045 - mse: 0.8045 - mae: 0.5109 - root_mean_squared_error: 0.8967\n",
      "Epoch 237/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7978 - mse: 0.7978 - mae: 0.5018 - root_mean_squared_error: 0.8930\n",
      "Epoch 238/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7994 - mse: 0.7994 - mae: 0.5062 - root_mean_squared_error: 0.8938\n",
      "Epoch 239/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7992 - mse: 0.7992 - mae: 0.5023 - root_mean_squared_error: 0.8938\n",
      "Epoch 240/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7978 - mse: 0.7978 - mae: 0.5021 - root_mean_squared_error: 0.8931\n",
      "Epoch 241/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7699 - mse: 0.7699 - mae: 0.4972 - root_mean_squared_error: 0.8771\n",
      "Epoch 242/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7995 - mse: 0.7995 - mae: 0.5045 - root_mean_squared_error: 0.8939\n",
      "Epoch 243/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7891 - mse: 0.7891 - mae: 0.4975 - root_mean_squared_error: 0.8882\n",
      "Epoch 244/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7981 - mse: 0.7981 - mae: 0.5049 - root_mean_squared_error: 0.8933\n",
      "Epoch 245/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8001 - mse: 0.8001 - mae: 0.5041 - root_mean_squared_error: 0.8944\n",
      "Epoch 246/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7880 - mse: 0.7880 - mae: 0.5029 - root_mean_squared_error: 0.8876\n",
      "Epoch 247/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7702 - mse: 0.7702 - mae: 0.5028 - root_mean_squared_error: 0.8774\n",
      "Epoch 248/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7781 - mse: 0.7781 - mae: 0.5027 - root_mean_squared_error: 0.8819\n",
      "Epoch 249/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8039 - mse: 0.8039 - mae: 0.5129 - root_mean_squared_error: 0.8966\n",
      "Epoch 250/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7808 - mse: 0.7808 - mae: 0.5010 - root_mean_squared_error: 0.8834\n",
      "Epoch 251/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7728 - mse: 0.7728 - mae: 0.5012 - root_mean_squared_error: 0.8787\n",
      "Epoch 252/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8070 - mse: 0.8070 - mae: 0.5089 - root_mean_squared_error: 0.8981\n",
      "Epoch 253/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7614 - mse: 0.7614 - mae: 0.4974 - root_mean_squared_error: 0.8720\n",
      "Epoch 254/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8003 - mse: 0.8003 - mae: 0.5067 - root_mean_squared_error: 0.8944\n",
      "Epoch 255/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8058 - mse: 0.8058 - mae: 0.5096 - root_mean_squared_error: 0.8976\n",
      "Epoch 256/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7722 - mse: 0.7722 - mae: 0.4971 - root_mean_squared_error: 0.8786\n",
      "Epoch 257/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8036 - mse: 0.8036 - mae: 0.5105 - root_mean_squared_error: 0.8961\n",
      "Epoch 258/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7700 - mse: 0.7700 - mae: 0.4971 - root_mean_squared_error: 0.8774\n",
      "Epoch 259/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7816 - mse: 0.7816 - mae: 0.5036 - root_mean_squared_error: 0.8840\n",
      "Epoch 260/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8122 - mse: 0.8122 - mae: 0.5146 - root_mean_squared_error: 0.9012\n",
      "Epoch 261/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8002 - mse: 0.8002 - mae: 0.5094 - root_mean_squared_error: 0.8943\n",
      "Epoch 262/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7958 - mse: 0.7958 - mae: 0.5025 - root_mean_squared_error: 0.8917\n",
      "Epoch 263/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7969 - mse: 0.7969 - mae: 0.5131 - root_mean_squared_error: 0.8926\n",
      "Epoch 264/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7746 - mse: 0.7746 - mae: 0.4991 - root_mean_squared_error: 0.8799\n",
      "Epoch 265/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7786 - mse: 0.7786 - mae: 0.4979 - root_mean_squared_error: 0.8822\n",
      "Epoch 266/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8010 - mse: 0.8010 - mae: 0.5101 - root_mean_squared_error: 0.8949\n",
      "Epoch 267/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7752 - mse: 0.7752 - mae: 0.5009 - root_mean_squared_error: 0.8803\n",
      "Epoch 268/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7845 - mse: 0.7845 - mae: 0.5048 - root_mean_squared_error: 0.8857\n",
      "Epoch 269/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7932 - mse: 0.7932 - mae: 0.5072 - root_mean_squared_error: 0.8903\n",
      "Epoch 270/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7943 - mse: 0.7943 - mae: 0.5056 - root_mean_squared_error: 0.8911\n",
      "Epoch 271/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7557 - mse: 0.7557 - mae: 0.4969 - root_mean_squared_error: 0.8690\n",
      "Epoch 272/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8202 - mse: 0.8202 - mae: 0.5162 - root_mean_squared_error: 0.9055\n",
      "Epoch 273/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7776 - mse: 0.7776 - mae: 0.5006 - root_mean_squared_error: 0.8814\n",
      "Epoch 274/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8162 - mse: 0.8162 - mae: 0.5132 - root_mean_squared_error: 0.9031\n",
      "Epoch 275/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7588 - mse: 0.7588 - mae: 0.4904 - root_mean_squared_error: 0.8705\n",
      "Epoch 276/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8139 - mse: 0.8139 - mae: 0.5142 - root_mean_squared_error: 0.9018\n",
      "Epoch 277/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7971 - mse: 0.7971 - mae: 0.5052 - root_mean_squared_error: 0.8921\n",
      "Epoch 278/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7793 - mse: 0.7793 - mae: 0.4994 - root_mean_squared_error: 0.8826\n",
      "Epoch 279/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7927 - mse: 0.7927 - mae: 0.5030 - root_mean_squared_error: 0.8902\n",
      "Epoch 280/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7939 - mse: 0.7939 - mae: 0.5079 - root_mean_squared_error: 0.8910\n",
      "Epoch 281/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7866 - mse: 0.7866 - mae: 0.5025 - root_mean_squared_error: 0.8868\n",
      "Epoch 282/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7757 - mse: 0.7757 - mae: 0.4994 - root_mean_squared_error: 0.8802\n",
      "Epoch 283/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7611 - mse: 0.7611 - mae: 0.4997 - root_mean_squared_error: 0.8717\n",
      "Epoch 284/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7991 - mse: 0.7991 - mae: 0.5060 - root_mean_squared_error: 0.8938\n",
      "Epoch 285/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7895 - mse: 0.7895 - mae: 0.5042 - root_mean_squared_error: 0.8885\n",
      "Epoch 286/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7763 - mse: 0.7763 - mae: 0.5032 - root_mean_squared_error: 0.8807\n",
      "Epoch 287/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8005 - mse: 0.8005 - mae: 0.5103 - root_mean_squared_error: 0.8946\n",
      "Epoch 288/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8028 - mse: 0.8028 - mae: 0.5122 - root_mean_squared_error: 0.8959\n",
      "Epoch 289/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7829 - mse: 0.7829 - mae: 0.5033 - root_mean_squared_error: 0.8847\n",
      "Epoch 290/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8062 - mse: 0.8062 - mae: 0.5076 - root_mean_squared_error: 0.8976\n",
      "Epoch 291/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8000 - mse: 0.8000 - mae: 0.5111 - root_mean_squared_error: 0.8941\n",
      "Epoch 292/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7931 - mse: 0.7931 - mae: 0.5073 - root_mean_squared_error: 0.8905\n",
      "Epoch 293/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7939 - mse: 0.7939 - mae: 0.5034 - root_mean_squared_error: 0.8909\n",
      "Epoch 294/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8133 - mse: 0.8133 - mae: 0.5086 - root_mean_squared_error: 0.9016\n",
      "Epoch 295/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8067 - mse: 0.8067 - mae: 0.5092 - root_mean_squared_error: 0.8979\n",
      "Epoch 296/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7914 - mse: 0.7914 - mae: 0.5061 - root_mean_squared_error: 0.8895\n",
      "Epoch 297/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7818 - mse: 0.7818 - mae: 0.4995 - root_mean_squared_error: 0.8840\n",
      "Epoch 298/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8056 - mse: 0.8056 - mae: 0.5092 - root_mean_squared_error: 0.8974\n",
      "Epoch 299/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7634 - mse: 0.7634 - mae: 0.4952 - root_mean_squared_error: 0.8735\n",
      "Epoch 300/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7696 - mse: 0.7696 - mae: 0.4964 - root_mean_squared_error: 0.8770\n",
      "Epoch 301/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7982 - mse: 0.7982 - mae: 0.5061 - root_mean_squared_error: 0.8933\n",
      "Epoch 302/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7586 - mse: 0.7586 - mae: 0.4910 - root_mean_squared_error: 0.8702\n",
      "Epoch 303/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7746 - mse: 0.7746 - mae: 0.4974 - root_mean_squared_error: 0.8799\n",
      "Epoch 304/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8083 - mse: 0.8083 - mae: 0.5056 - root_mean_squared_error: 0.8990\n",
      "Epoch 305/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8265 - mse: 0.8265 - mae: 0.5119 - root_mean_squared_error: 0.9084\n",
      "Epoch 306/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7863 - mse: 0.7863 - mae: 0.5016 - root_mean_squared_error: 0.8864\n",
      "Epoch 307/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7726 - mse: 0.7726 - mae: 0.5039 - root_mean_squared_error: 0.8786\n",
      "Epoch 308/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7895 - mse: 0.7895 - mae: 0.5078 - root_mean_squared_error: 0.8884\n",
      "Epoch 309/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8008 - mse: 0.8008 - mae: 0.5040 - root_mean_squared_error: 0.8948\n",
      "Epoch 310/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7747 - mse: 0.7747 - mae: 0.4988 - root_mean_squared_error: 0.8801\n",
      "Epoch 311/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7771 - mse: 0.7771 - mae: 0.4972 - root_mean_squared_error: 0.8813\n",
      "Epoch 312/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7901 - mse: 0.7901 - mae: 0.5058 - root_mean_squared_error: 0.8886\n",
      "Epoch 313/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7625 - mse: 0.7625 - mae: 0.4978 - root_mean_squared_error: 0.8729\n",
      "Epoch 314/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7930 - mse: 0.7930 - mae: 0.5070 - root_mean_squared_error: 0.8904\n",
      "Epoch 315/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8059 - mse: 0.8059 - mae: 0.5082 - root_mean_squared_error: 0.8976\n",
      "Epoch 316/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7647 - mse: 0.7647 - mae: 0.4928 - root_mean_squared_error: 0.8737\n",
      "Epoch 317/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7783 - mse: 0.7783 - mae: 0.5062 - root_mean_squared_error: 0.8819\n",
      "Epoch 318/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7720 - mse: 0.7720 - mae: 0.4947 - root_mean_squared_error: 0.8784\n",
      "Epoch 319/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8023 - mse: 0.8023 - mae: 0.5051 - root_mean_squared_error: 0.8955\n",
      "Epoch 320/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7764 - mse: 0.7764 - mae: 0.4989 - root_mean_squared_error: 0.8808\n",
      "Epoch 321/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7931 - mse: 0.7931 - mae: 0.5085 - root_mean_squared_error: 0.8904\n",
      "Epoch 322/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7985 - mse: 0.7985 - mae: 0.5030 - root_mean_squared_error: 0.8935\n",
      "Epoch 323/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8008 - mse: 0.8008 - mae: 0.5048 - root_mean_squared_error: 0.8948\n",
      "Epoch 324/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8271 - mse: 0.8271 - mae: 0.5136 - root_mean_squared_error: 0.9093\n",
      "Epoch 325/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8054 - mse: 0.8054 - mae: 0.5100 - root_mean_squared_error: 0.8973\n",
      "Epoch 326/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7937 - mse: 0.7937 - mae: 0.5080 - root_mean_squared_error: 0.8908\n",
      "Epoch 327/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8137 - mse: 0.8137 - mae: 0.5096 - root_mean_squared_error: 0.9016\n",
      "Epoch 328/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7825 - mse: 0.7825 - mae: 0.4969 - root_mean_squared_error: 0.8845\n",
      "Epoch 329/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7759 - mse: 0.7759 - mae: 0.4945 - root_mean_squared_error: 0.8806\n",
      "Epoch 330/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7666 - mse: 0.7666 - mae: 0.4977 - root_mean_squared_error: 0.8745\n",
      "Epoch 331/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7902 - mse: 0.7902 - mae: 0.5028 - root_mean_squared_error: 0.8888\n",
      "Epoch 332/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7711 - mse: 0.7711 - mae: 0.4979 - root_mean_squared_error: 0.8780\n",
      "Epoch 333/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7821 - mse: 0.7821 - mae: 0.5055 - root_mean_squared_error: 0.8843\n",
      "Epoch 334/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7888 - mse: 0.7888 - mae: 0.5043 - root_mean_squared_error: 0.8880\n",
      "Epoch 335/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8080 - mse: 0.8080 - mae: 0.5086 - root_mean_squared_error: 0.8988\n",
      "Epoch 336/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7935 - mse: 0.7935 - mae: 0.5021 - root_mean_squared_error: 0.8905\n",
      "Epoch 337/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7803 - mse: 0.7803 - mae: 0.4942 - root_mean_squared_error: 0.8828\n",
      "Epoch 338/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7913 - mse: 0.7913 - mae: 0.5055 - root_mean_squared_error: 0.8895\n",
      "Epoch 339/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8074 - mse: 0.8074 - mae: 0.5021 - root_mean_squared_error: 0.8981\n",
      "Epoch 340/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7800 - mse: 0.7800 - mae: 0.5030 - root_mean_squared_error: 0.8830\n",
      "Epoch 341/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8043 - mse: 0.8043 - mae: 0.5100 - root_mean_squared_error: 0.8966\n",
      "Epoch 342/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8076 - mse: 0.8076 - mae: 0.5131 - root_mean_squared_error: 0.8985\n",
      "Epoch 343/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8013 - mse: 0.8013 - mae: 0.5003 - root_mean_squared_error: 0.8939\n",
      "Epoch 344/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7980 - mse: 0.7980 - mae: 0.5116 - root_mean_squared_error: 0.8932\n",
      "Epoch 345/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8078 - mse: 0.8078 - mae: 0.5077 - root_mean_squared_error: 0.8986\n",
      "Epoch 346/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8116 - mse: 0.8116 - mae: 0.5060 - root_mean_squared_error: 0.9005\n",
      "Epoch 347/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7969 - mse: 0.7969 - mae: 0.5078 - root_mean_squared_error: 0.8925\n",
      "Epoch 348/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7747 - mse: 0.7747 - mae: 0.4985 - root_mean_squared_error: 0.8800\n",
      "Epoch 349/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7751 - mse: 0.7751 - mae: 0.5046 - root_mean_squared_error: 0.8802\n",
      "Epoch 350/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7892 - mse: 0.7892 - mae: 0.5021 - root_mean_squared_error: 0.8883\n",
      "Epoch 351/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8116 - mse: 0.8116 - mae: 0.5076 - root_mean_squared_error: 0.9008\n",
      "Epoch 352/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7846 - mse: 0.7846 - mae: 0.5017 - root_mean_squared_error: 0.8855\n",
      "Epoch 353/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7579 - mse: 0.7579 - mae: 0.4934 - root_mean_squared_error: 0.8702\n",
      "Epoch 354/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8067 - mse: 0.8067 - mae: 0.5082 - root_mean_squared_error: 0.8980\n",
      "Epoch 355/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7907 - mse: 0.7907 - mae: 0.5069 - root_mean_squared_error: 0.8890\n",
      "Epoch 356/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7955 - mse: 0.7955 - mae: 0.5042 - root_mean_squared_error: 0.8917\n",
      "Epoch 357/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8252 - mse: 0.8252 - mae: 0.5134 - root_mean_squared_error: 0.9080\n",
      "Epoch 358/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7837 - mse: 0.7837 - mae: 0.5029 - root_mean_squared_error: 0.8850\n",
      "Epoch 359/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7901 - mse: 0.7901 - mae: 0.5033 - root_mean_squared_error: 0.8888\n",
      "Epoch 360/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8051 - mse: 0.8051 - mae: 0.5091 - root_mean_squared_error: 0.8970\n",
      "Epoch 361/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8091 - mse: 0.8091 - mae: 0.5033 - root_mean_squared_error: 0.8991\n",
      "Epoch 362/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7931 - mse: 0.7931 - mae: 0.5025 - root_mean_squared_error: 0.8905\n",
      "Epoch 363/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7866 - mse: 0.7866 - mae: 0.5049 - root_mean_squared_error: 0.8867\n",
      "Epoch 364/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7735 - mse: 0.7735 - mae: 0.4988 - root_mean_squared_error: 0.8793\n",
      "Epoch 365/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8207 - mse: 0.8207 - mae: 0.5147 - root_mean_squared_error: 0.9057\n",
      "Epoch 366/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8211 - mse: 0.8211 - mae: 0.5153 - root_mean_squared_error: 0.9060\n",
      "Epoch 367/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7746 - mse: 0.7746 - mae: 0.4979 - root_mean_squared_error: 0.8799\n",
      "Epoch 368/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7836 - mse: 0.7836 - mae: 0.5049 - root_mean_squared_error: 0.8850\n",
      "Epoch 369/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7892 - mse: 0.7892 - mae: 0.5038 - root_mean_squared_error: 0.8882\n",
      "Epoch 370/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7910 - mse: 0.7910 - mae: 0.5062 - root_mean_squared_error: 0.8892\n",
      "Epoch 371/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7709 - mse: 0.7709 - mae: 0.4957 - root_mean_squared_error: 0.8777\n",
      "Epoch 372/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7906 - mse: 0.7906 - mae: 0.5030 - root_mean_squared_error: 0.8889\n",
      "Epoch 373/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8359 - mse: 0.8359 - mae: 0.5171 - root_mean_squared_error: 0.9137\n",
      "Epoch 374/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7958 - mse: 0.7958 - mae: 0.5054 - root_mean_squared_error: 0.8917\n",
      "Epoch 375/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8132 - mse: 0.8132 - mae: 0.5116 - root_mean_squared_error: 0.9012\n",
      "Epoch 376/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8180 - mse: 0.8180 - mae: 0.5166 - root_mean_squared_error: 0.9042\n",
      "Epoch 377/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8002 - mse: 0.8002 - mae: 0.5101 - root_mean_squared_error: 0.8944\n",
      "Epoch 378/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7746 - mse: 0.7746 - mae: 0.5036 - root_mean_squared_error: 0.8800\n",
      "Epoch 379/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8078 - mse: 0.8078 - mae: 0.5083 - root_mean_squared_error: 0.8986\n",
      "Epoch 380/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7949 - mse: 0.7949 - mae: 0.5019 - root_mean_squared_error: 0.8915\n",
      "Epoch 381/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7777 - mse: 0.7777 - mae: 0.5034 - root_mean_squared_error: 0.8813\n",
      "Epoch 382/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8195 - mse: 0.8195 - mae: 0.5098 - root_mean_squared_error: 0.9044\n",
      "Epoch 383/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8289 - mse: 0.8289 - mae: 0.5166 - root_mean_squared_error: 0.9102\n",
      "Epoch 384/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7884 - mse: 0.7884 - mae: 0.5007 - root_mean_squared_error: 0.8878\n",
      "Epoch 385/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7783 - mse: 0.7783 - mae: 0.5009 - root_mean_squared_error: 0.8817\n",
      "Epoch 386/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8281 - mse: 0.8281 - mae: 0.5141 - root_mean_squared_error: 0.9099\n",
      "Epoch 387/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8113 - mse: 0.8113 - mae: 0.5116 - root_mean_squared_error: 0.9005\n",
      "Epoch 388/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7984 - mse: 0.7984 - mae: 0.5078 - root_mean_squared_error: 0.8934\n",
      "Epoch 389/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7897 - mse: 0.7897 - mae: 0.5028 - root_mean_squared_error: 0.8886\n",
      "Epoch 390/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8027 - mse: 0.8027 - mae: 0.5090 - root_mean_squared_error: 0.8958\n",
      "Epoch 391/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7925 - mse: 0.7925 - mae: 0.5098 - root_mean_squared_error: 0.8902\n",
      "Epoch 392/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7652 - mse: 0.7652 - mae: 0.4966 - root_mean_squared_error: 0.8745\n",
      "Epoch 393/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8080 - mse: 0.8080 - mae: 0.5109 - root_mean_squared_error: 0.8988\n",
      "Epoch 394/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8236 - mse: 0.8236 - mae: 0.5141 - root_mean_squared_error: 0.9073\n",
      "Epoch 395/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8074 - mse: 0.8074 - mae: 0.5086 - root_mean_squared_error: 0.8984\n",
      "Epoch 396/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7789 - mse: 0.7789 - mae: 0.4991 - root_mean_squared_error: 0.8824\n",
      "Epoch 397/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7940 - mse: 0.7940 - mae: 0.5068 - root_mean_squared_error: 0.8909\n",
      "Epoch 398/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7960 - mse: 0.7960 - mae: 0.5074 - root_mean_squared_error: 0.8921\n",
      "Epoch 399/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8120 - mse: 0.8120 - mae: 0.5090 - root_mean_squared_error: 0.9009\n",
      "Epoch 400/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7838 - mse: 0.7838 - mae: 0.5043 - root_mean_squared_error: 0.8852\n",
      "computed in 0:01:51.917585 s\n"
     ]
    }
   ],
   "source": [
    "batch_input_shape = (1, n_samples, n_features)\n",
    "dense_model = get_dense_model(batch_input_shape)\n",
    "dense_model.summary()\n",
    "start = time.time()\n",
    "dense_model.fit(X_train, Y_train, epochs=400, verbose=1)\n",
    "end = time.time() - start\n",
    "print(f\"computed in {str(str(timedelta(seconds=end)))} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dense_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b0757c7eb5d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mY_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mY_predicted_unscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_predicted\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_predicted_unscaled\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY_test_unscaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dense_model' is not defined"
     ]
    }
   ],
   "source": [
    "Y_predicted = np.squeeze(dense_model.predict(X_test, batch_size=1))\n",
    "Y_predicted_unscaled = dg.inverse_transform_y(Y_predicted, idx=test_idx)\n",
    "\n",
    "print(np.mean(Y_predicted - Y_test, axis=0))\n",
    "print(np.mean(Y_predicted_unscaled - Y_test_unscaled, axis=0))\n",
    "\n",
    "Y_predicted_unpadded_unscaled =  dg.remove_padded_y(Y_predicted_unscaled, idx=test_idx)\n",
    "\n",
    "print(mean_absolute_error(Y_predicted_unpadded_unscaled, Y_test_unpadded_unscaled, multioutput=\"raw_values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_vs_actual(df_y_real, df_y_predicted, prediction_dates, cur_loc, horizon=1, mode=0):\n",
    "    \"\"\"\n",
    "    plot the prediction done on a specific horizon along with the last points of data before\n",
    "    \"\"\"\n",
    "    if not predict_one:\n",
    "        if mode == 0:\n",
    "            horizon_range = range(horizon, horizon+1)\n",
    "        elif mode == 1:\n",
    "            horizon_range = range(1, horizon+1)\n",
    "    else:\n",
    "        horizon_range = [1]\n",
    "\n",
    "    df_real = df_y_real[cur_loc]\n",
    "    df_pred = df_y_predicted[cur_loc]\n",
    "    prediction_dates = prediction_dates[:, horizon]\n",
    "    \n",
    "    for horizon in horizon_range:\n",
    "        fig = plt.figure(figsize=(6,3))\n",
    "        expected = df_real[f\"{target}(t+{horizon})\"].values\n",
    "        plt.plot(prediction_dates, expected, marker=\".\", label=\"True value\")\n",
    "        prediction = df_pred[f\"{target}(t+{horizon})\"].values\n",
    "\n",
    "        if not predict_one:\n",
    "            plot_label = f\"Prediction horizon {horizon}\"\n",
    "        else:\n",
    "            plot_label = f\"Prediction horizon {n_forecast}\"\n",
    "        plt.plot(prediction_dates, prediction, marker='.', label=plot_label)\n",
    "\n",
    "        ax = fig.axes[0]\n",
    "        # set locator\n",
    "        ax.xaxis.set_major_locator(mdates.DayLocator(interval=7))\n",
    "        # set formatter\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y'))\n",
    "        # set font and rotation for date tick labels\n",
    "        plt.gcf().autofmt_xdate()\n",
    "        plt.title(f\"Plot true and predicted values for {cur_loc}\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationLogger(ProgbarLogger):\n",
    "    \"\"\"\n",
    "    compute metrics on the validation set, using a different batch size than the training set\n",
    "    at the end of each epoch, the weights of the training model are used to set the validation model\n",
    "    the error is computed based on the unscaled and unpadded values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, validation_model, val_batch_size, X_val, Y_val, val_idx, val_geo):\n",
    "        \"\"\"\n",
    "        :param validation_model: validation model ready to be used\n",
    "        :param X_val: X validation set\n",
    "        :param Y_val: Y validation set, already unscaled and unpadded\n",
    "        :param val_idx: validation idx\n",
    "        :param val_geo: validation geoloc\n",
    "        \"\"\"\n",
    "        super(ValidationLogger, self).__init__(count_mode='steps')\n",
    "        self.validation_model = validation_model\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.X_val = X_val\n",
    "        self.Y_val = Y_val\n",
    "        self.val_idx = val_idx\n",
    "        self.val_geo = val_geo\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        at the end of each epoch, compute the metrics asked on the validation set\n",
    "        \"\"\"\n",
    "        self.validation_model.set_weights(self.model.get_weights())\n",
    "        Y_pred = self.validation_model.predict(self.X_val, batch_size=self.val_batch_size)\n",
    "        Y_pred = dg.inverse_transform_y(Y_pred, geo=self.val_geo, idx=self.val_idx)\n",
    "        Y_pred = dg.remove_padded_y(Y_pred, geo=self.val_geo, idx=self.val_idx)\n",
    "        for metric in validation_metrics:\n",
    "            metric.reset_states()\n",
    "            metric.update_state(Y_pred, self.Y_val)\n",
    "            logs[f\"val_{metric.name}\"] = metric.result().numpy()  # prepend name for validation set\n",
    "        super(ValidationLogger, self).on_epoch_end(epoch, logs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(Y_expected, Y_actual) -> pd.DataFrame:\n",
    "    MAE = mean_absolute_error(Y_expected, Y_actual, multioutput=\"raw_values\")\n",
    "    MSE = mean_squared_error(Y_expected, Y_actual, multioutput=\"raw_values\")\n",
    "    values = {}\n",
    "    for t in range(n_forecast):\n",
    "        values[f'MAE(t+{t+1})'] = [MAE[t]]\n",
    "        values[f'MSE(t+{t+1})'] = [MSE[t]]\n",
    "    return pd.DataFrame(values)\n",
    "\n",
    "\n",
    "def walk_forward_evaluation_untrainable(model_generator: callable, nb_fit_first: int, nb_validation: int, \n",
    "                                        nb_test: int, verbose: int = 1, *args, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    walk forward evaluation for an untrainable model\n",
    "    \"\"\"\n",
    "    # initial index used\n",
    "    max_len = dg.batch_size\n",
    "    train_idx = np.arange(nb_fit_first)\n",
    "    valid_idx = np.arange(nb_fit_first, nb_fit_first + nb_validation)  # validation set is simply ignored\n",
    "    if nb_fit_first + nb_validation >= max_len:\n",
    "        finished = True  # no test can be done\n",
    "    else:\n",
    "        finished = False  # a test set can be created\n",
    "        test_idx = np.arange(nb_fit_first + nb_validation, min(nb_fit_first + nb_validation + nb_test, max_len))\n",
    "    df_error = pd.DataFrame()\n",
    "    walk = 0\n",
    "    last_iter = False  # True when the last iteration is reached\n",
    "    while not finished:\n",
    "        X_train = dg.get_x(train_idx)  # set the scalers\n",
    "        Y_train = dg.get_y(train_idx)\n",
    "        X_test = dg.get_x(test_idx, geo=dg.loc_init, use_previous_scaler=True)\n",
    "        Y_test_real = dg.get_y(test_idx, geo=dg.loc_init, scaled=False)\n",
    "        Y_test_real = dg.remove_padded_y(Y_test_real, geo=dg.loc_init, idx=test_idx)\n",
    "        \n",
    "        batch_size = len(X_test)\n",
    "        model_pred = model_generator(batch_input_shape=(batch_size, n_samples, n_features))\n",
    "        Y_predicted = model_pred.predict(X_test, batch_size=batch_size)\n",
    "        Y_predicted_real = dg.inverse_transform_y(Y_predicted, geo=dg.loc_init, idx=test_idx)\n",
    "        Y_predicted_real = dg.remove_padded_y(Y_predicted_real, geo=dg.loc_init, idx=test_idx)\n",
    "        \n",
    "        \n",
    "        # compute the error using the unpadded and unscaled data\n",
    "        error = compute_error(Y_test_real, Y_predicted_real)\n",
    "        name = f'walk {walk + 1}'\n",
    "        error['walk'] = name\n",
    "        error['nb_test_datapoints'] = len(Y_test_real)\n",
    "        error['days_train'] = len(train_idx)\n",
    "        error['days_valid'] = len(valid_idx)\n",
    "        error['days_test'] = len(test_idx)\n",
    "        error = error.set_index('walk')\n",
    "        df_error = df_error.append(error)\n",
    "\n",
    "        if verbose != 0:\n",
    "            display(df_error)\n",
    "        if last_iter:\n",
    "            finished = True\n",
    "        # indexes for next fit\n",
    "        train_idx = np.arange(train_idx[-1] + 1 + nb_test)\n",
    "        valid_idx += nb_test\n",
    "        if test_idx[-1] + nb_test >= max_len:  # last iteration, less points can be used for the test set\n",
    "            last_iter = True  # last iteration to be done\n",
    "            test_idx = np.arange(test_idx[-1], max_len)\n",
    "        else:\n",
    "            test_idx += nb_test\n",
    "        walk += 1\n",
    "    return df_error\n",
    "\n",
    "\n",
    "def walk_forward_evaluation(model_generator: callable, nb_fit_first: int, nb_validation: int, nb_test: int, \n",
    "                            epochs: int = 400, plot=False, verbose: int = 1,\n",
    "                            return_history: bool = False, batch_size_fun: callable = None,\n",
    "                            es_stop_val: bool = False, *args, **kwargs\n",
    "                           ) -> Union[pd.DataFrame, List[History]]:\n",
    "    \"\"\"\n",
    "    evaluates a model using a walk forward evaluation: multiples fit are done, each followed by at most nb_test\n",
    "    to evaluate the model\n",
    "    :param model_generator: function returning the model to evaluate\n",
    "    :param nb_fit_first: number of datapoints used for the first fit\n",
    "    :param nb_validation: number of datapoints used for each evaluation set\n",
    "    :param nb_test: number of datapoints used for the test set (at most)\n",
    "    :param epochs: number of epochs used on each fit\n",
    "    :param callbacks: list of callbacks to use\n",
    "    :param verbose: verbose level. Passed to fit and used to display the error dataframe\n",
    "    :param return_history: if True, returns the list of history of each walk\n",
    "    :param batch_size_fun: function used to compute the batch size based on the X_train tensor\n",
    "        if not specified, default to batch_size = len(train_idx)\n",
    "    \"\"\"\n",
    "    # initial index used\n",
    "    max_len = dg.batch_size\n",
    "    train_idx = np.arange(nb_fit_first)\n",
    "    valid_idx = np.arange(nb_fit_first, nb_fit_first + nb_validation)\n",
    "    if nb_fit_first + nb_validation >= max_len:\n",
    "        finished = True  # no test can be done\n",
    "    else:\n",
    "        finished = False  # a test set can be created\n",
    "        test_idx = np.arange(nb_fit_first + nb_validation, min(nb_fit_first + nb_validation + nb_test, max_len))\n",
    "    df_error = pd.DataFrame()\n",
    "    walk = 0\n",
    "    last_iter = False  # True when the last iteration is reached\n",
    "    all_history = []\n",
    "    \n",
    "    while not finished:\n",
    "        X_train = dg.get_x(train_idx)\n",
    "        Y_train = dg.get_y(train_idx)\n",
    "        if batch_size_fun is None:\n",
    "            batch_size = len(X_train)\n",
    "        else:\n",
    "            batch_size = batch_size_fun(X_train)\n",
    "            \n",
    "        if len(valid_idx) > 0:\n",
    "            X_val = dg.get_x(valid_idx, geo=dg.loc_init, use_previous_scaler=True)\n",
    "            Y_val_real = dg.get_y(valid_idx, geo=dg.loc_init, scaled=False)\n",
    "            Y_val_real = dg.remove_padded_y(Y_val_real, geo=dg.loc_init, idx=valid_idx)\n",
    "            batch_size_val = len(X_val)\n",
    "            model_validation = model_generator(batch_input_shape=(batch_size_val, n_samples, n_features))\n",
    "            val_log = ValidationLogger(model_validation, len(X_val), X_val, Y_val_real, valid_idx, dg.loc_init)\n",
    "            if es_stop_val:\n",
    "                callbacks = [val_log, EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=25)]\n",
    "            else:\n",
    "                callbacks = [val_log]\n",
    "        else:\n",
    "            callbacks = None\n",
    "            \n",
    "        model_train = model_generator(batch_input_shape=(batch_size, n_samples, n_features))\n",
    "\n",
    "        history = model_train.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=verbose)\n",
    "        \n",
    "        # test only on the unaugmented data without padding\n",
    "        batch_size = len(test_idx)\n",
    "        # construct a new model and use the previous weights to set it\n",
    "        model_pred = model_generator(batch_input_shape=(batch_size, n_samples, n_features))\n",
    "        model_pred.set_weights(model_train.get_weights())\n",
    "        X_test = dg.get_x(test_idx, geo=dg.loc_init, use_previous_scaler=True)\n",
    "        Y_test = dg.get_y(test_idx, geo=dg.loc_init, use_previous_scaler=True)\n",
    "        Y_test_real = dg.get_y(test_idx, geo=dg.loc_init, scaled=False)\n",
    "        Y_test_real = dg.remove_padded_y(Y_test_real, geo=dg.loc_init, idx=test_idx)\n",
    "        Y_predicted = model_pred.predict(X_test, batch_size=batch_size)\n",
    "        Y_predicted_real = dg.inverse_transform_y(Y_predicted, geo=dg.loc_init, idx=test_idx)\n",
    "        Y_predicted_real = dg.remove_padded_y(Y_predicted_real, geo=dg.loc_init, idx=test_idx)        \n",
    "        if return_history:\n",
    "            # add test metrics to the history for this walk, based on the unpadded unscaled data \n",
    "            # except for the loss\n",
    "            for metric in model_train.metrics:\n",
    "                metric.reset_states()\n",
    "                if metric.name == 'loss':\n",
    "                    metric.update_state(Y_predicted_real, Y_test_real)\n",
    "                else:\n",
    "                    # compute metric accross each horizon\n",
    "                    for i in range(n_forecast):\n",
    "                        metric.update_state(Y_predicted_real[:, i], Y_test_real[:, i])\n",
    "                        history.history[f\"test_{metric.name}(t+{i+1})\"] = [metric.result().numpy()]\n",
    "                        metric.reset_states()\n",
    "                    # compute mean of metric on all horizon\n",
    "                    metric.update_state(Y_predicted_real, Y_test_real)\n",
    "                # prepend name for test set\n",
    "                history.history[f\"test_{metric.name}\"] = [metric.result().numpy()]\n",
    "            # add number of unpadded datapoints\n",
    "            history.history['nb_test_datapoints'] = [len(Y_test_real)]\n",
    "            all_history.append(history)\n",
    "        if not return_history or verbose != 0:\n",
    "            # compute the error using the unpadded and unscaled data\n",
    "            error = compute_error(Y_test_real, Y_predicted_real)\n",
    "            name = f'walk {walk + 1}'\n",
    "            error['walk'] = name\n",
    "            error['nb_test_datapoints'] = len(Y_test_real)\n",
    "            error['days_train'] = len(train_idx)\n",
    "            error['days_valid'] = len(valid_idx)\n",
    "            error['days_test'] = len(test_idx)\n",
    "            error = error.set_index('walk')\n",
    "            df_error = df_error.append(error)\n",
    "        if verbose != 0:\n",
    "            display(df_error)\n",
    "        if last_iter:\n",
    "            finished = True\n",
    "        # indexes for next fit\n",
    "        train_idx = np.arange(train_idx[-1] + 1 + nb_test)\n",
    "        valid_idx += nb_test\n",
    "        if test_idx[-1] + nb_test >= max_len:  # last iteration, less points can be used for the test set\n",
    "            last_iter = True  # last iteration to be done\n",
    "            test_idx = np.arange(test_idx[-1], max_len)\n",
    "        else:\n",
    "            test_idx += nb_test\n",
    "        walk += 1\n",
    "    if not return_history:\n",
    "        df_error.loc['mean'] = df_error.mean()\n",
    "        \n",
    "    return df_error if not return_history else all_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_div = lambda x : len(x)\n",
    "df_errors = walk_forward_evaluation(get_dense_model, 250, 10, 30, epochs=300, verbose=0, batch_size_fun=no_div,\n",
    "                                   es_stop_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 27 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f13e429bb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 28 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f1334717700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "df_errors = walk_forward_evaluation_untrainable(get_baseline, 250, 10, 30, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE(t+1)</th>\n",
       "      <th>MSE(t+1)</th>\n",
       "      <th>MAE(t+2)</th>\n",
       "      <th>MSE(t+2)</th>\n",
       "      <th>MAE(t+3)</th>\n",
       "      <th>MSE(t+3)</th>\n",
       "      <th>MAE(t+4)</th>\n",
       "      <th>MSE(t+4)</th>\n",
       "      <th>MAE(t+5)</th>\n",
       "      <th>MSE(t+5)</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE(t+18)</th>\n",
       "      <th>MSE(t+18)</th>\n",
       "      <th>MAE(t+19)</th>\n",
       "      <th>MSE(t+19)</th>\n",
       "      <th>MAE(t+20)</th>\n",
       "      <th>MSE(t+20)</th>\n",
       "      <th>nb_test_datapoints</th>\n",
       "      <th>days_train</th>\n",
       "      <th>days_valid</th>\n",
       "      <th>days_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>walk 1</th>\n",
       "      <td>2.653414</td>\n",
       "      <td>18.376976</td>\n",
       "      <td>4.322945</td>\n",
       "      <td>52.678029</td>\n",
       "      <td>5.744572</td>\n",
       "      <td>99.592669</td>\n",
       "      <td>7.075787</td>\n",
       "      <td>152.626038</td>\n",
       "      <td>8.228779</td>\n",
       "      <td>206.738369</td>\n",
       "      <td>...</td>\n",
       "      <td>13.139682</td>\n",
       "      <td>526.430186</td>\n",
       "      <td>13.107798</td>\n",
       "      <td>537.771202</td>\n",
       "      <td>13.087922</td>\n",
       "      <td>544.791437</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2</th>\n",
       "      <td>2.172878</td>\n",
       "      <td>10.873261</td>\n",
       "      <td>3.472671</td>\n",
       "      <td>26.068706</td>\n",
       "      <td>4.598137</td>\n",
       "      <td>43.123129</td>\n",
       "      <td>5.665217</td>\n",
       "      <td>61.351936</td>\n",
       "      <td>6.546998</td>\n",
       "      <td>80.249630</td>\n",
       "      <td>...</td>\n",
       "      <td>15.076191</td>\n",
       "      <td>496.024723</td>\n",
       "      <td>15.651139</td>\n",
       "      <td>539.153116</td>\n",
       "      <td>16.183644</td>\n",
       "      <td>579.313899</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3</th>\n",
       "      <td>2.168530</td>\n",
       "      <td>9.121090</td>\n",
       "      <td>3.131470</td>\n",
       "      <td>19.516919</td>\n",
       "      <td>4.084058</td>\n",
       "      <td>32.435851</td>\n",
       "      <td>5.006418</td>\n",
       "      <td>48.270193</td>\n",
       "      <td>5.833955</td>\n",
       "      <td>66.968301</td>\n",
       "      <td>...</td>\n",
       "      <td>12.543685</td>\n",
       "      <td>312.345581</td>\n",
       "      <td>12.952381</td>\n",
       "      <td>335.639696</td>\n",
       "      <td>13.394824</td>\n",
       "      <td>358.485215</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4</th>\n",
       "      <td>1.947412</td>\n",
       "      <td>9.317302</td>\n",
       "      <td>3.106625</td>\n",
       "      <td>25.314141</td>\n",
       "      <td>4.145549</td>\n",
       "      <td>50.678769</td>\n",
       "      <td>5.137888</td>\n",
       "      <td>86.681698</td>\n",
       "      <td>6.157143</td>\n",
       "      <td>130.161822</td>\n",
       "      <td>...</td>\n",
       "      <td>17.865632</td>\n",
       "      <td>1171.578426</td>\n",
       "      <td>18.690683</td>\n",
       "      <td>1268.008037</td>\n",
       "      <td>19.469151</td>\n",
       "      <td>1366.899668</td>\n",
       "      <td>690.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5</th>\n",
       "      <td>3.149070</td>\n",
       "      <td>26.206336</td>\n",
       "      <td>5.512423</td>\n",
       "      <td>88.594118</td>\n",
       "      <td>7.495343</td>\n",
       "      <td>170.564193</td>\n",
       "      <td>9.111803</td>\n",
       "      <td>248.506310</td>\n",
       "      <td>10.316772</td>\n",
       "      <td>320.849713</td>\n",
       "      <td>...</td>\n",
       "      <td>18.110250</td>\n",
       "      <td>673.431590</td>\n",
       "      <td>19.579194</td>\n",
       "      <td>772.061152</td>\n",
       "      <td>21.017082</td>\n",
       "      <td>920.087802</td>\n",
       "      <td>92.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.418261</td>\n",
       "      <td>14.778993</td>\n",
       "      <td>3.909227</td>\n",
       "      <td>42.434383</td>\n",
       "      <td>5.213532</td>\n",
       "      <td>79.278922</td>\n",
       "      <td>6.399423</td>\n",
       "      <td>119.487235</td>\n",
       "      <td>7.416729</td>\n",
       "      <td>160.993567</td>\n",
       "      <td>...</td>\n",
       "      <td>15.347088</td>\n",
       "      <td>635.962101</td>\n",
       "      <td>15.996239</td>\n",
       "      <td>690.526641</td>\n",
       "      <td>16.630525</td>\n",
       "      <td>753.915604</td>\n",
       "      <td>570.4</td>\n",
       "      <td>310.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MAE(t+1)   MSE(t+1)  MAE(t+2)   MSE(t+2)  MAE(t+3)    MSE(t+3)  \\\n",
       "walk                                                                     \n",
       "walk 1  2.653414  18.376976  4.322945  52.678029  5.744572   99.592669   \n",
       "walk 2  2.172878  10.873261  3.472671  26.068706  4.598137   43.123129   \n",
       "walk 3  2.168530   9.121090  3.131470  19.516919  4.084058   32.435851   \n",
       "walk 4  1.947412   9.317302  3.106625  25.314141  4.145549   50.678769   \n",
       "walk 5  3.149070  26.206336  5.512423  88.594118  7.495343  170.564193   \n",
       "mean    2.418261  14.778993  3.909227  42.434383  5.213532   79.278922   \n",
       "\n",
       "        MAE(t+4)    MSE(t+4)   MAE(t+5)    MSE(t+5)  ...  MAE(t+18)  \\\n",
       "walk                                                 ...              \n",
       "walk 1  7.075787  152.626038   8.228779  206.738369  ...  13.139682   \n",
       "walk 2  5.665217   61.351936   6.546998   80.249630  ...  15.076191   \n",
       "walk 3  5.006418   48.270193   5.833955   66.968301  ...  12.543685   \n",
       "walk 4  5.137888   86.681698   6.157143  130.161822  ...  17.865632   \n",
       "walk 5  9.111803  248.506310  10.316772  320.849713  ...  18.110250   \n",
       "mean    6.399423  119.487235   7.416729  160.993567  ...  15.347088   \n",
       "\n",
       "          MSE(t+18)  MAE(t+19)    MSE(t+19)  MAE(t+20)    MSE(t+20)  \\\n",
       "walk                                                                  \n",
       "walk 1   526.430186  13.107798   537.771202  13.087922   544.791437   \n",
       "walk 2   496.024723  15.651139   539.153116  16.183644   579.313899   \n",
       "walk 3   312.345581  12.952381   335.639696  13.394824   358.485215   \n",
       "walk 4  1171.578426  18.690683  1268.008037  19.469151  1366.899668   \n",
       "walk 5   673.431590  19.579194   772.061152  21.017082   920.087802   \n",
       "mean     635.962101  15.996239   690.526641  16.630525   753.915604   \n",
       "\n",
       "        nb_test_datapoints  days_train  days_valid  days_test  \n",
       "walk                                                           \n",
       "walk 1               690.0       250.0        10.0       30.0  \n",
       "walk 2               690.0       280.0        10.0       30.0  \n",
       "walk 3               690.0       310.0        10.0       30.0  \n",
       "walk 4               690.0       340.0        10.0       30.0  \n",
       "walk 5                92.0       370.0        10.0        4.0  \n",
       "mean                 570.4       310.0        10.0       24.8  \n",
       "\n",
       "[6 rows x 44 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "66/66 [==============================] - 1s 2ms/step - loss: 0.3480 - mse: 0.3480 - mae: 0.3775 - root_mean_squared_error: 0.5843 - val_mean_squared_error: 493404.6562 - val_mean_absolute_error: 311.0318 - val_root_mean_squared_error: 702.4277\n",
      "Epoch 2/200\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0654 - mse: 0.0654 - mae: 0.1591 - root_mean_squared_error: 0.2550 - val_mean_squared_error: 63255.7500 - val_mean_absolute_error: 152.8721 - val_root_mean_squared_error: 251.5070\n",
      "Epoch 3/200\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0299 - mse: 0.0299 - mae: 0.1195 - root_mean_squared_error: 0.1729 - val_mean_squared_error: 54219.2266 - val_mean_absolute_error: 148.2310 - val_root_mean_squared_error: 232.8502\n",
      "Epoch 4/200\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0222 - mse: 0.0222 - mae: 0.1068 - root_mean_squared_error: 0.1489 - val_mean_squared_error: 37672.4297 - val_mean_absolute_error: 118.3708 - val_root_mean_squared_error: 194.0939\n",
      "Epoch 5/200\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0979 - root_mean_squared_error: 0.1375 - val_mean_squared_error: 43032.0195 - val_mean_absolute_error: 130.3975 - val_root_mean_squared_error: 207.4416\n",
      "Epoch 00005: early stopping\n",
      "computed in 0:00:00.931670 s\n"
     ]
    }
   ],
   "source": [
    "# X% for training, remaining for test\n",
    "ratio_training = 0.8\n",
    "nb_datapoints = dg.batch_size\n",
    "max_train = int(ratio_training * nb_datapoints)\n",
    "train_idx = np.array(range(max_train))\n",
    "val_idx = np.array(range(max_train, nb_datapoints))\n",
    "X_train = dg.get_x(train_idx, scaled=True)\n",
    "Y_train = dg.get_y(train_idx, scaled=True)\n",
    "X_val = dg.get_x(val_idx, scaled=True, use_previous_scaler=True)\n",
    "Y_val_unscaled = dg.get_y(val_idx, geo=dg.loc_init, scaled=False, use_previous_scaler=True)\n",
    "Y_val_unpadded_unscaled = dg.remove_padded_y(Y_val_unscaled, idx=val_idx, geo=dg.loc_init)\n",
    "\n",
    "model_generator = get_dense_model\n",
    "\n",
    "batch_size_train = len(train_idx)\n",
    "batch_size_val = len(X_val)\n",
    "model = model_generator(batch_input_shape=(batch_size_train, n_samples, n_features))\n",
    "model_validation = model_generator(batch_input_shape=(batch_size_val, n_samples, n_features))\n",
    "val_log = ValidationLogger(model_validation, len(X_val), X_val, Y_val_unpadded_unscaled, val_idx, dg.loc_init)\n",
    "callbacks = [val_log, EarlyStopping(monitor='val_root_mean_squared_error', mode='min', verbose=1, patience=1)]\n",
    "    \n",
    "start = time.time()\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size_train, epochs=200, callbacks=callbacks)\n",
    "end = time.time() - start\n",
    "print(f\"computed in {str(str(timedelta(seconds=end)))} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8c3e63bab394>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m scan_object = talos.Scan(\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/talos/scan/Scan.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, params, model, experiment_name, x_val, y_val, val_split, random_method, seed, performance_target, fraction_limit, round_limit, time_limit, boolean_limit, reduction_method, reduction_interval, reduction_window, reduction_threshold, reduction_metric, minimize_loss, disable_progress_bar, print_params, clear_session, save_weights)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;31m# start runtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mscan_run\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mscan_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/talos/scan/scan_run.py\u001b[0m in \u001b[0;36mscan_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# otherwise proceed with next permutation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mscan_round\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/talos/scan/scan_round.py\u001b[0m in \u001b[0;36mscan_round\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mingest_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mingest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/talos/model/ingest_model.py\u001b[0m in \u001b[0;36mingest_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m     through Scan() model paramater.'''\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     return self.model(self.x_train,\n\u001b[0m\u001b[1;32m      7\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-8c3e63bab394>\u001b[0m in \u001b[0;36mtalos_walk\u001b[0;34m(x_train, y_train, x_val, y_val, p)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mbatch_size_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_div'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     history_list = walk_forward_evaluation(model_generator, nb_fit_first=250, nb_validation=0, nb_test=30, \n\u001b[0m\u001b[1;32m     46\u001b[0m                                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                                       batch_size_fun=batch_size_fun, return_history=True, es_stop_val=True)\n",
      "\u001b[0;32m<ipython-input-12-93302560fc4f>\u001b[0m in \u001b[0;36mwalk_forward_evaluation\u001b[0;34m(model_generator, nb_fit_first, nb_validation, nb_test, epochs, plot, verbose, return_history, batch_size_fun, es_stop_val)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mmodel_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# test only on the unaugmented data without padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyper_parameter = {\n",
    "    'hidden_1': [4, 8, 16],\n",
    "    'hidden_2': [0, 4, 8, 16],\n",
    "    'lr': [0.001, 0.01],\n",
    "    'activation': ['relu', 'elu'],\n",
    "    'reg': [lambda x: regularizers.l1(l=x), lambda x: None],\n",
    "    'regw': [5e-4, 1e-3],\n",
    "    'optimizer': ['Adam', 'RMSprop'],\n",
    "    'losses': ['mse', 'mae', custom_loss_function],\n",
    "    'scaling': [MinMaxScaler, StandardScaler],\n",
    "    'batch_size_div': [1],\n",
    "    'epochs': [600]\n",
    "}\n",
    "\n",
    "def get_encoder_decoder(batch_input_shape, p):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(p['hidden_1'], return_sequences=(p['hidden_2'] != 0), \n",
    "                   batch_input_shape=batch_input_shape, kernel_regularizer=p['reg'](p['regw'])))\n",
    "    if p['hidden_2'] != 0:\n",
    "        model.add(LSTM(p['hidden_2'], return_sequences=False, kernel_regularizer=p['reg'](p['regw'])))\n",
    "    model.add(RepeatVector(n_forecast))  # repeat\n",
    "    if p['hidden_2'] != 0:\n",
    "        model.add(LSTM(p['hidden_2'], return_sequences=True, kernel_regularizer=p['reg'](p['regw'])))  # dec\n",
    "    if not predict_one:\n",
    "        model.add(LSTM(p['hidden_1'], return_sequences=True, kernel_regularizer=p['reg'](p['regw'])))  # dec\n",
    "        model.add(TimeDistributed(Dense(1, kernel_regularizer=p['reg'](p['regw']), activation=p['activation'])))\n",
    "        model.add(Reshape((n_forecast,)))\n",
    "    else:\n",
    "        model.add(LSTM(p['hidden_1'], return_sequences=False, kernel_regularizer=p['reg'](p['regw'])))  # dec\n",
    "        model.add(Dense(1, kernel_regularizer=p['reg'](p['regw']), activation=p['activation']))\n",
    "        model.add(Reshape((1,)))\n",
    "    model.compile(loss=p[\"losses\"], optimizer=p[\"optimizer\"], metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    K.set_value(model.optimizer.learning_rate, p['lr'])\n",
    "    return model\n",
    "    \n",
    "\n",
    "def talos_walk(x_train, y_train, x_val, y_val, p):\n",
    "    \"\"\"\n",
    "    walk forward evaluation used by talos to determine the best model to keep\n",
    "    \"\"\"\n",
    "    dg.set_scaler(p['scaling'])\n",
    "    model_generator = lambda batch_input_shape: get_encoder_decoder(batch_input_shape, p)\n",
    "\n",
    "    batch_size_fun = lambda x : int(x.shape[0] / p['batch_size_div'])\n",
    "    history_list = walk_forward_evaluation(model_generator, nb_fit_first=250, nb_validation=0, nb_test=30, \n",
    "                                      epochs=p['epochs'], plot=False, verbose=0, \n",
    "                                      batch_size_fun=batch_size_fun, return_history=True, es_stop_val=True)\n",
    "    # compute history to return, based on the list of history\n",
    "    history = History()\n",
    "    # used to trick talos to compute the right amount of round_epochs. needs to be the first entry of the dict\n",
    "    # create an array of len == mean of number of epoch of each history\n",
    "    history.history['ep'] = np.arange(int(np.mean([len(hist.history['loss']) for hist in history_list]))) + 1\n",
    "    for log in history_list[0].history.keys():\n",
    "        history.history[log] = [np.mean([hist.history[log][-1] for hist in history_list])]\n",
    "    return history, model_generator((1, n_samples, n_features))\n",
    "\n",
    "scan_object = talos.Scan(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    x_val=[],\n",
    "    y_val=[],\n",
    "    params=hyper_parameter,\n",
    "    model=talos_walk,\n",
    "    experiment_name='trends1', \n",
    "    fraction_limit=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_object = talos.Analyze(scan_object)\n",
    "print(\"MAE\", analyze_object.low('mae'))\n",
    "print(\"MSE\", analyze_object.low('mse'))\n",
    "print(\"RMSE\", analyze_object.low('root_mean_squared_error'))\n",
    "print(\"TEST MAE\", analyze_object.low('test_mae'))\n",
    "print(\"TEST MSE\", analyze_object.low('test_mse'))\n",
    "print(\"TEST RMSE\", analyze_object.low('test_root_mean_squared_error'))\n",
    "df = analyze_object.table('test_root_mean_squared_error', ascending=True)\n",
    "print(df.columns)\n",
    "columns = [f\"test_mae(t+{i})\" for i in range(1, n_forecast+1)]\n",
    "df[['hidden_1', 'hidden_2', 'lr', 'batch_size_div', 'scaling', 'test_mse', 'test_mae'] + columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 estimators combined\n",
    "The first estimator should be simple and output an estimate $ \\hat{y}_{(1)}(t) $ of $ y(t) $\n",
    "The correction factor ($ C(t) $) between the estimation and the real value is then computed:\n",
    "\n",
    "$$ y(t) = \\hat{y}_{(1)}(t) * ( 1 + C(t)) $$\n",
    "\n",
    "$$ C(t) = \\frac{y(t) - \\hat{y}_{(1)}(t)}{\\hat{y}_{(1)}(t)} $$\n",
    "\n",
    "This formula is used such that $C(t)$ should be bounded in [-1, 1] if the target is a positive value and the first estimator is relatively closed to the expected value.\n",
    "\n",
    "Anoter estimator is then trained based on the first one. The goal is to be able to predict the correction $ C(t) $ that should be done on the first estimator, in order to reduce the estimation error and get a better prediction. The prediction of $ C(t) $ is written $ \\hat{C}(t) $. Once this estimator is computed, the final prediction can be constructed:\n",
    "\n",
    "$$ \\hat{y}(t) = \\hat{y}_{(1)}(t) * ( 1 + \\hat{C}(t)) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First estimator\n",
    "Construct the predition $\\hat{y}_{(1)}(t)$ and compute $C(t)$ based on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 608ms/step - loss: 0.4331 - mse: 0.4331 - mae: 0.4510 - root_mean_squared_error: 0.6581\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3930 - mse: 0.3930 - mae: 0.4266 - root_mean_squared_error: 0.6269\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3669 - mse: 0.3669 - mae: 0.4106 - root_mean_squared_error: 0.6057\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3467 - mse: 0.3467 - mae: 0.3980 - root_mean_squared_error: 0.5888\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3299 - mse: 0.3299 - mae: 0.3873 - root_mean_squared_error: 0.5743\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3152 - mse: 0.3152 - mae: 0.3779 - root_mean_squared_error: 0.5614\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3022 - mse: 0.3022 - mae: 0.3694 - root_mean_squared_error: 0.5497\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2904 - mse: 0.2904 - mae: 0.3616 - root_mean_squared_error: 0.5389\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2797 - mse: 0.2797 - mae: 0.3543 - root_mean_squared_error: 0.5288\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2697 - mse: 0.2697 - mae: 0.3474 - root_mean_squared_error: 0.5193\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2604 - mse: 0.2604 - mae: 0.3410 - root_mean_squared_error: 0.5103\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2517 - mse: 0.2517 - mae: 0.3349 - root_mean_squared_error: 0.5017\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2435 - mse: 0.2435 - mae: 0.3291 - root_mean_squared_error: 0.4934\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2357 - mse: 0.2357 - mae: 0.3236 - root_mean_squared_error: 0.4855\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2283 - mse: 0.2283 - mae: 0.3182 - root_mean_squared_error: 0.4779\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2213 - mse: 0.2213 - mae: 0.3132 - root_mean_squared_error: 0.4705\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2147 - mse: 0.2147 - mae: 0.3082 - root_mean_squared_error: 0.4633\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2083 - mse: 0.2083 - mae: 0.3035 - root_mean_squared_error: 0.4564\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2021 - mse: 0.2021 - mae: 0.2989 - root_mean_squared_error: 0.4496\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1963 - mse: 0.1963 - mae: 0.2944 - root_mean_squared_error: 0.4430\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1906 - mse: 0.1906 - mae: 0.2901 - root_mean_squared_error: 0.4366\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1852 - mse: 0.1852 - mae: 0.2858 - root_mean_squared_error: 0.4304\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.1800 - mse: 0.1800 - mae: 0.2816 - root_mean_squared_error: 0.4242\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1749 - mse: 0.1749 - mae: 0.2775 - root_mean_squared_error: 0.4183\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1701 - mse: 0.1701 - mae: 0.2735 - root_mean_squared_error: 0.4124\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1654 - mse: 0.1654 - mae: 0.2696 - root_mean_squared_error: 0.4067\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1608 - mse: 0.1608 - mae: 0.2657 - root_mean_squared_error: 0.4011\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1565 - mse: 0.1565 - mae: 0.2619 - root_mean_squared_error: 0.3956\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1522 - mse: 0.1522 - mae: 0.2582 - root_mean_squared_error: 0.3902\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1481 - mse: 0.1481 - mae: 0.2545 - root_mean_squared_error: 0.3849\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1442 - mse: 0.1442 - mae: 0.2509 - root_mean_squared_error: 0.3797\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1403 - mse: 0.1403 - mae: 0.2474 - root_mean_squared_error: 0.3746\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1366 - mse: 0.1366 - mae: 0.2439 - root_mean_squared_error: 0.3696\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1330 - mse: 0.1330 - mae: 0.2405 - root_mean_squared_error: 0.3647\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1295 - mse: 0.1295 - mae: 0.2372 - root_mean_squared_error: 0.3599\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1262 - mse: 0.1262 - mae: 0.2339 - root_mean_squared_error: 0.3552\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1229 - mse: 0.1229 - mae: 0.2307 - root_mean_squared_error: 0.3506\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1197 - mse: 0.1197 - mae: 0.2276 - root_mean_squared_error: 0.3460\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1166 - mse: 0.1166 - mae: 0.2245 - root_mean_squared_error: 0.3415\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1137 - mse: 0.1137 - mae: 0.2215 - root_mean_squared_error: 0.3372\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1108 - mse: 0.1108 - mae: 0.2185 - root_mean_squared_error: 0.3328\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1080 - mse: 0.1080 - mae: 0.2155 - root_mean_squared_error: 0.3286\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1053 - mse: 0.1053 - mae: 0.2126 - root_mean_squared_error: 0.3245\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1027 - mse: 0.1027 - mae: 0.2098 - root_mean_squared_error: 0.3204\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1001 - mse: 0.1001 - mae: 0.2070 - root_mean_squared_error: 0.3164\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0976 - mse: 0.0976 - mae: 0.2043 - root_mean_squared_error: 0.3125\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0952 - mse: 0.0952 - mae: 0.2017 - root_mean_squared_error: 0.3086\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0929 - mse: 0.0929 - mae: 0.1991 - root_mean_squared_error: 0.3048\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0907 - mse: 0.0907 - mae: 0.1965 - root_mean_squared_error: 0.3011\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0885 - mse: 0.0885 - mae: 0.1941 - root_mean_squared_error: 0.2975\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0864 - mse: 0.0864 - mae: 0.1916 - root_mean_squared_error: 0.2940\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0844 - mse: 0.0844 - mae: 0.1893 - root_mean_squared_error: 0.2905\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0824 - mse: 0.0824 - mae: 0.1870 - root_mean_squared_error: 0.2871\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0805 - mse: 0.0805 - mae: 0.1848 - root_mean_squared_error: 0.2837\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0787 - mse: 0.0787 - mae: 0.1825 - root_mean_squared_error: 0.2805\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0769 - mse: 0.0769 - mae: 0.1806 - root_mean_squared_error: 0.2773\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0752 - mse: 0.0752 - mae: 0.1784 - root_mean_squared_error: 0.2742\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0735 - mse: 0.0735 - mae: 0.1765 - root_mean_squared_error: 0.2711\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0719 - mse: 0.0719 - mae: 0.1745 - root_mean_squared_error: 0.2681\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0704 - mse: 0.0704 - mae: 0.1727 - root_mean_squared_error: 0.2652\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0689 - mse: 0.0689 - mae: 0.1709 - root_mean_squared_error: 0.2624\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0674 - mse: 0.0674 - mae: 0.1692 - root_mean_squared_error: 0.2597\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0660 - mse: 0.0660 - mae: 0.1675 - root_mean_squared_error: 0.2570\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0647 - mse: 0.0647 - mae: 0.1660 - root_mean_squared_error: 0.2544\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0634 - mse: 0.0634 - mae: 0.1644 - root_mean_squared_error: 0.2518\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0622 - mse: 0.0622 - mae: 0.1629 - root_mean_squared_error: 0.2493\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0610 - mse: 0.0610 - mae: 0.1614 - root_mean_squared_error: 0.2469\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0598 - mse: 0.0598 - mae: 0.1601 - root_mean_squared_error: 0.2446\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0587 - mse: 0.0587 - mae: 0.1586 - root_mean_squared_error: 0.2423\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0577 - mse: 0.0577 - mae: 0.1574 - root_mean_squared_error: 0.2401\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0566 - mse: 0.0566 - mae: 0.1559 - root_mean_squared_error: 0.2380\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0556 - mse: 0.0556 - mae: 0.1548 - root_mean_squared_error: 0.2359\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0547 - mse: 0.0547 - mae: 0.1535 - root_mean_squared_error: 0.2339\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0538 - mse: 0.0538 - mae: 0.1525 - root_mean_squared_error: 0.2319\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0529 - mse: 0.0529 - mae: 0.1513 - root_mean_squared_error: 0.2300\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0521 - mse: 0.0521 - mae: 0.1503 - root_mean_squared_error: 0.2282\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0512 - mse: 0.0512 - mae: 0.1492 - root_mean_squared_error: 0.2264\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0505 - mse: 0.0505 - mae: 0.1483 - root_mean_squared_error: 0.2246\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0497 - mse: 0.0497 - mae: 0.1473 - root_mean_squared_error: 0.2229\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0490 - mse: 0.0490 - mae: 0.1466 - root_mean_squared_error: 0.2213\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0483 - mse: 0.0483 - mae: 0.1456 - root_mean_squared_error: 0.2197\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0476 - mse: 0.0476 - mae: 0.1450 - root_mean_squared_error: 0.2182\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0470 - mse: 0.0470 - mae: 0.1439 - root_mean_squared_error: 0.2167\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0463 - mse: 0.0463 - mae: 0.1434 - root_mean_squared_error: 0.2152\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0457 - mse: 0.0457 - mae: 0.1424 - root_mean_squared_error: 0.2138\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0451 - mse: 0.0451 - mae: 0.1419 - root_mean_squared_error: 0.2124\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0446 - mse: 0.0446 - mae: 0.1410 - root_mean_squared_error: 0.2111\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0440 - mse: 0.0440 - mae: 0.1405 - root_mean_squared_error: 0.2098\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0435 - mse: 0.0435 - mae: 0.1397 - root_mean_squared_error: 0.2085\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0430 - mse: 0.0430 - mae: 0.1391 - root_mean_squared_error: 0.2073\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0425 - mse: 0.0425 - mae: 0.1384 - root_mean_squared_error: 0.2061\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0420 - mse: 0.0420 - mae: 0.1378 - root_mean_squared_error: 0.2049\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0415 - mse: 0.0415 - mae: 0.1372 - root_mean_squared_error: 0.2037\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0410 - mse: 0.0410 - mae: 0.1365 - root_mean_squared_error: 0.2026\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0406 - mse: 0.0406 - mae: 0.1359 - root_mean_squared_error: 0.2015\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0402 - mse: 0.0402 - mae: 0.1353 - root_mean_squared_error: 0.2004\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0397 - mse: 0.0397 - mae: 0.1347 - root_mean_squared_error: 0.1993\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0393 - mse: 0.0393 - mae: 0.1342 - root_mean_squared_error: 0.1983\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0389 - mse: 0.0389 - mae: 0.1334 - root_mean_squared_error: 0.1972\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0385 - mse: 0.0385 - mae: 0.1331 - root_mean_squared_error: 0.1962\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0381 - mse: 0.0381 - mae: 0.1322 - root_mean_squared_error: 0.1952\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0377 - mse: 0.0377 - mae: 0.1319 - root_mean_squared_error: 0.1942\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0373 - mse: 0.0373 - mae: 0.1311 - root_mean_squared_error: 0.1932\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0370 - mse: 0.0370 - mae: 0.1307 - root_mean_squared_error: 0.1922\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0366 - mse: 0.0366 - mae: 0.1300 - root_mean_squared_error: 0.1913\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0362 - mse: 0.0362 - mae: 0.1295 - root_mean_squared_error: 0.1903\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0359 - mse: 0.0359 - mae: 0.1290 - root_mean_squared_error: 0.1894\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0355 - mse: 0.0355 - mae: 0.1283 - root_mean_squared_error: 0.1885\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0352 - mse: 0.0352 - mae: 0.1278 - root_mean_squared_error: 0.1875\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0348 - mse: 0.0348 - mae: 0.1272 - root_mean_squared_error: 0.1866\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0345 - mse: 0.0345 - mae: 0.1266 - root_mean_squared_error: 0.1856\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0341 - mse: 0.0341 - mae: 0.1261 - root_mean_squared_error: 0.1847\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0338 - mse: 0.0338 - mae: 0.1254 - root_mean_squared_error: 0.1838\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0335 - mse: 0.0335 - mae: 0.1250 - root_mean_squared_error: 0.1829\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0331 - mse: 0.0331 - mae: 0.1243 - root_mean_squared_error: 0.1820\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0328 - mse: 0.0328 - mae: 0.1239 - root_mean_squared_error: 0.1812\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0325 - mse: 0.0325 - mae: 0.1232 - root_mean_squared_error: 0.1803\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0322 - mse: 0.0322 - mae: 0.1228 - root_mean_squared_error: 0.1794\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0319 - mse: 0.0319 - mae: 0.1221 - root_mean_squared_error: 0.1785\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0316 - mse: 0.0316 - mae: 0.1217 - root_mean_squared_error: 0.1777\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0313 - mse: 0.0313 - mae: 0.1211 - root_mean_squared_error: 0.1768\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0310 - mse: 0.0310 - mae: 0.1206 - root_mean_squared_error: 0.1760\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0307 - mse: 0.0307 - mae: 0.1201 - root_mean_squared_error: 0.1752\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0304 - mse: 0.0304 - mae: 0.1195 - root_mean_squared_error: 0.1744\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0301 - mse: 0.0301 - mae: 0.1191 - root_mean_squared_error: 0.1735\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0298 - mse: 0.0298 - mae: 0.1185 - root_mean_squared_error: 0.1727\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0296 - mse: 0.0296 - mae: 0.1181 - root_mean_squared_error: 0.1720\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0293 - mse: 0.0293 - mae: 0.1175 - root_mean_squared_error: 0.1712\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0291 - mse: 0.0291 - mae: 0.1173 - root_mean_squared_error: 0.1704\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0288 - mse: 0.0288 - mae: 0.1165 - root_mean_squared_error: 0.1697\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0285 - mse: 0.0285 - mae: 0.1162 - root_mean_squared_error: 0.1689\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0283 - mse: 0.0283 - mae: 0.1156 - root_mean_squared_error: 0.1682\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0280 - mse: 0.0280 - mae: 0.1152 - root_mean_squared_error: 0.1674\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0278 - mse: 0.0278 - mae: 0.1147 - root_mean_squared_error: 0.1667\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0276 - mse: 0.0276 - mae: 0.1144 - root_mean_squared_error: 0.1660\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0273 - mse: 0.0273 - mae: 0.1139 - root_mean_squared_error: 0.1653\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0271 - mse: 0.0271 - mae: 0.1136 - root_mean_squared_error: 0.1646\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0269 - mse: 0.0269 - mae: 0.1130 - root_mean_squared_error: 0.1640\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0267 - mse: 0.0267 - mae: 0.1128 - root_mean_squared_error: 0.1633\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0265 - mse: 0.0265 - mae: 0.1122 - root_mean_squared_error: 0.1627\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0263 - mse: 0.0263 - mae: 0.1121 - root_mean_squared_error: 0.1621\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0261 - mse: 0.0261 - mae: 0.1114 - root_mean_squared_error: 0.1614\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0259 - mse: 0.0259 - mae: 0.1114 - root_mean_squared_error: 0.1608\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0257 - mse: 0.0257 - mae: 0.1108 - root_mean_squared_error: 0.1602\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0255 - mse: 0.0255 - mae: 0.1106 - root_mean_squared_error: 0.1596\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0253 - mse: 0.0253 - mae: 0.1102 - root_mean_squared_error: 0.1591\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0251 - mse: 0.0251 - mae: 0.1099 - root_mean_squared_error: 0.1585\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0249 - mse: 0.0249 - mae: 0.1095 - root_mean_squared_error: 0.1579\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0248 - mse: 0.0248 - mae: 0.1093 - root_mean_squared_error: 0.1574\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0246 - mse: 0.0246 - mae: 0.1089 - root_mean_squared_error: 0.1569\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0244 - mse: 0.0244 - mae: 0.1087 - root_mean_squared_error: 0.1563\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0243 - mse: 0.0243 - mae: 0.1083 - root_mean_squared_error: 0.1558\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0241 - mse: 0.0241 - mae: 0.1082 - root_mean_squared_error: 0.1553\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0240 - mse: 0.0240 - mae: 0.1077 - root_mean_squared_error: 0.1548\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0238 - mse: 0.0238 - mae: 0.1076 - root_mean_squared_error: 0.1543\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0237 - mse: 0.0237 - mae: 0.1073 - root_mean_squared_error: 0.1539\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0235 - mse: 0.0235 - mae: 0.1071 - root_mean_squared_error: 0.1534\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0234 - mse: 0.0234 - mae: 0.1068 - root_mean_squared_error: 0.1529\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0233 - mse: 0.0233 - mae: 0.1067 - root_mean_squared_error: 0.1525\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1063 - root_mean_squared_error: 0.1521\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0230 - mse: 0.0230 - mae: 0.1062 - root_mean_squared_error: 0.1516\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0229 - mse: 0.0229 - mae: 0.1059 - root_mean_squared_error: 0.1512\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.1057 - root_mean_squared_error: 0.1508\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0226 - mse: 0.0226 - mae: 0.1056 - root_mean_squared_error: 0.1504\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0225 - mse: 0.0225 - mae: 0.1053 - root_mean_squared_error: 0.1500\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0224 - mse: 0.0224 - mae: 0.1052 - root_mean_squared_error: 0.1497\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0223 - mse: 0.0223 - mae: 0.1049 - root_mean_squared_error: 0.1493\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0222 - mse: 0.0222 - mae: 0.1047 - root_mean_squared_error: 0.1489\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0221 - mse: 0.0221 - mae: 0.1046 - root_mean_squared_error: 0.1485\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0219 - mse: 0.0219 - mae: 0.1043 - root_mean_squared_error: 0.1482\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0218 - mse: 0.0218 - mae: 0.1043 - root_mean_squared_error: 0.1478\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0217 - mse: 0.0217 - mae: 0.1039 - root_mean_squared_error: 0.1475\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0217 - mse: 0.0217 - mae: 0.1040 - root_mean_squared_error: 0.1471\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0216 - mse: 0.0216 - mae: 0.1035 - root_mean_squared_error: 0.1468\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0215 - mse: 0.0215 - mae: 0.1038 - root_mean_squared_error: 0.1465\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0214 - mse: 0.0214 - mae: 0.1032 - root_mean_squared_error: 0.1462\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0213 - mse: 0.0213 - mae: 0.1033 - root_mean_squared_error: 0.1459\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.1029 - root_mean_squared_error: 0.1456\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0211 - mse: 0.0211 - mae: 0.1029 - root_mean_squared_error: 0.1453\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0210 - mse: 0.0210 - mae: 0.1028 - root_mean_squared_error: 0.1450\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0210 - mse: 0.0210 - mae: 0.1026 - root_mean_squared_error: 0.1448\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0209 - mse: 0.0209 - mae: 0.1024 - root_mean_squared_error: 0.1445\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0208 - mse: 0.0208 - mae: 0.1023 - root_mean_squared_error: 0.1442\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0207 - mse: 0.0207 - mae: 0.1020 - root_mean_squared_error: 0.1439\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0206 - mse: 0.0206 - mae: 0.1021 - root_mean_squared_error: 0.1437\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0206 - mse: 0.0206 - mae: 0.1017 - root_mean_squared_error: 0.1434\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1019 - root_mean_squared_error: 0.1432\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1014 - root_mean_squared_error: 0.1429\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1016 - root_mean_squared_error: 0.1427\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1011 - root_mean_squared_error: 0.1425\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1013 - root_mean_squared_error: 0.1423\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1008 - root_mean_squared_error: 0.1420\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1010 - root_mean_squared_error: 0.1418\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1006 - root_mean_squared_error: 0.1416\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1006 - root_mean_squared_error: 0.1414\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1004 - root_mean_squared_error: 0.1412\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1003 - root_mean_squared_error: 0.1410\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1002 - root_mean_squared_error: 0.1408\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.0999 - root_mean_squared_error: 0.1406\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.1001 - root_mean_squared_error: 0.1404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndf_predicted_1 = dg.inverse_transform_y(Y_train_val_pred, idx=train_val_idx, return_type='dict_df')\\ndf_predicted_1\\n\\n# predictions on the training set\\nmodel_prediction = model_generator(batch_input_shape=(batch_size_train, n_samples, dg.n_features))\\nmodel_prediction.set_weights(model.get_weights())\\nY_train_pred = model_prediction.predict(X_train, batch_size=batch_size_train)\\n\\n# predictions on the validation without augmented regions\\nmodel_prediction = model_generator(batch_input_shape=(batch_size_val, n_samples, dg.n_features))\\nmodel_prediction.set_weights(model.get_weights())\\nY_val_pred = model_prediction.predict(X_val, batch_size=batch_size_val)\\ndf_Y_real = {loc: dg.df[loc][dg.target_columns].iloc[train_val_idx] for loc in dg.df}\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X% for training, remaining for test\n",
    "ratio_training = 0.8\n",
    "epochs = 200\n",
    "\n",
    "nb_datapoints = dg.batch_size\n",
    "max_train = int(ratio_training * nb_datapoints)\n",
    "train_idx = np.array(range(max_train))\n",
    "valid_idx = np.array([])\n",
    "test_idx = np.array(range(max_train, nb_datapoints))\n",
    "\n",
    "X_train_1 = dg.get_x(train_idx, scaled=True)\n",
    "Y_train = dg.get_y(train_idx, scaled=True)\n",
    "\n",
    "if len(valid_idx) > 0:\n",
    "    X_val_1 = dg.get_x(val_idx, geo=dg.loc_init, scaled=True, use_previous_scaler=True)\n",
    "    Y_val_unscaled = dg.get_y(val_idx, geo=dg.loc_init, scaled=False)\n",
    "    Y_val_real = dg.remove_padded_y(Y_val_unscaled, idx=val_idx, geo=dg.loc_init)\n",
    "\n",
    "X_test_1 = dg.get_x(test_idx, scaled=True, geo=dg.loc_init, use_previous_scaler=True)\n",
    "Y_test_unscaled = dg.get_y(test_idx, scaled=False, geo=dg.loc_init)\n",
    "Y_test_real = dg.remove_padded_y(Y_test_unscaled, idx=test_idx, geo=dg.loc_init)\n",
    "    \n",
    "\"\"\"\n",
    "X_train_val = dg.get_x(train_val_idx, scaled=True)\n",
    "df_Y_real = {loc: dg.df[loc][dg.target_columns].iloc[train_val_idx] for loc in dg.df}\n",
    "Y_real_train_val = dg.get_y(train_val_idx, scaled=True, use_previous_scaler=True)\n",
    "\"\"\"\n",
    "\n",
    "model_generator = get_dense_model\n",
    "\n",
    "batch_size_train = len(X_train_1)\n",
    "batch_size_test = len(X_test_1)\n",
    "\n",
    "if len(valid_idx) > 0:  # use validation set for an early stop\n",
    "    batch_size_val = len(X_val_1)\n",
    "    model_validation = model_generator(batch_input_shape=(batch_size_val, n_samples, dg.n_features))\n",
    "    val_log = ValidationLogger(model_validation, len(X_val), X_val, Y_val_real, valid_idx, dg.loc_init)\n",
    "    callbacks = [val_log, EarlyStopping(monitor='val_root_mean_squared_error', mode='min', verbose=1, patience=20)]\n",
    "else:\n",
    "    callbacks = None\n",
    "\n",
    "model = model_generator(batch_input_shape=(batch_size_train, n_samples, dg.n_features))\n",
    "history = model.fit(X_train_1, Y_train, batch_size=batch_size_train, epochs=epochs, callbacks=callbacks)\n",
    "# compute the predictions on both the training and the validation\n",
    "Y_train_pred_1 = model.predict(X_train_1, batch_size=batch_size_train)\n",
    "\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_test, n_samples, dg.n_features))\n",
    "model_prediction.set_weights(model.get_weights())\n",
    "Y_test_pred_1 = model_prediction.predict(X_test_1, batch_size=batch_size_test)\n",
    "# unscale and unpad the data\n",
    "Y_test_pred_unscaled_1 = dg.inverse_transform_y(Y_test_pred_1, idx=test_idx, geo=dg.loc_init)\n",
    "Y_test_pred_real_1 = dg.remove_padded_y(Y_test_pred_unscaled_1, idx=test_idx, geo=dg.loc_init)\n",
    "\n",
    "\"\"\"\n",
    "df_predicted_1 = dg.inverse_transform_y(Y_train_val_pred, idx=train_val_idx, return_type='dict_df')\n",
    "df_predicted_1\n",
    "\n",
    "# predictions on the training set\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_train, n_samples, dg.n_features))\n",
    "model_prediction.set_weights(model.get_weights())\n",
    "Y_train_pred = model_prediction.predict(X_train, batch_size=batch_size_train)\n",
    "\n",
    "# predictions on the validation without augmented regions\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_val, n_samples, dg.n_features))\n",
    "model_prediction.set_weights(model.get_weights())\n",
    "Y_val_pred = model_prediction.predict(X_val, batch_size=batch_size_val)\n",
    "df_Y_real = {loc: dg.df[loc][dg.target_columns].iloc[train_val_idx] for loc in dg.df}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor k in dg.df:\\n    # add the data columns to the dataframe\\n    df_c[k] = dg.df[k][data_dg_2]\\n    for t in range(1, n_forecast+1):  # add the target columns\\n        target_t = f'{target}(t+{t})'\\n        df_c[k][f'C(t+{t})'] = (df_Y_real[k][target_t] - df_predicted_1[k][target_t]) / df_predicted_1[k][target_t]\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the corrections and add them to a dataframe\n",
    "data_dg_2 = [f'{topic}(t{i})' for i in range(-n_samples+1, 0, 1) for topic in list_topics] + [topic for topic in list_topics]\n",
    "target_df_2 = [f'{target}(t+{i})' for i in range(1, n_forecast+1)]\n",
    "df_c = {loc : dg.df[loc][data_dg_2 + target_df_2] for loc in dg.df}\n",
    "\"\"\"\n",
    "for k in dg.df:\n",
    "    # add the data columns to the dataframe\n",
    "    df_c[k] = dg.df[k][data_dg_2]\n",
    "    for t in range(1, n_forecast+1):  # add the target columns\n",
    "        target_t = f'{target}(t+{t})'\n",
    "        df_c[k][f'C(t+{t})'] = (df_Y_real[k][target_t] - df_predicted_1[k][target_t]) / df_predicted_1[k][target_t]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Fièvre(t-29)</th>\n",
       "      <th>Mal de gorge(t-29)</th>\n",
       "      <th>Symptôme(t-29)</th>\n",
       "      <th>Fièvre(t-28)</th>\n",
       "      <th>Mal de gorge(t-28)</th>\n",
       "      <th>Symptôme(t-28)</th>\n",
       "      <th>Fièvre(t-27)</th>\n",
       "      <th>Mal de gorge(t-27)</th>\n",
       "      <th>Symptôme(t-27)</th>\n",
       "      <th>Fièvre(t-26)</th>\n",
       "      <th>...</th>\n",
       "      <th>NEW_HOSP(t+11)</th>\n",
       "      <th>NEW_HOSP(t+12)</th>\n",
       "      <th>NEW_HOSP(t+13)</th>\n",
       "      <th>NEW_HOSP(t+14)</th>\n",
       "      <th>NEW_HOSP(t+15)</th>\n",
       "      <th>NEW_HOSP(t+16)</th>\n",
       "      <th>NEW_HOSP(t+17)</th>\n",
       "      <th>NEW_HOSP(t+18)</th>\n",
       "      <th>NEW_HOSP(t+19)</th>\n",
       "      <th>NEW_HOSP(t+20)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">BE</th>\n",
       "      <th>2020-03-04</th>\n",
       "      <td>25.402051</td>\n",
       "      <td>25.829284</td>\n",
       "      <td>22.034428</td>\n",
       "      <td>25.245307</td>\n",
       "      <td>25.878495</td>\n",
       "      <td>21.941446</td>\n",
       "      <td>24.616242</td>\n",
       "      <td>25.042280</td>\n",
       "      <td>21.586268</td>\n",
       "      <td>24.280325</td>\n",
       "      <td>...</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>96.428571</td>\n",
       "      <td>138.428571</td>\n",
       "      <td>184.428571</td>\n",
       "      <td>214.142857</td>\n",
       "      <td>239.857143</td>\n",
       "      <td>282.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>374.142857</td>\n",
       "      <td>416.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>25.245307</td>\n",
       "      <td>25.878495</td>\n",
       "      <td>21.941446</td>\n",
       "      <td>24.616242</td>\n",
       "      <td>25.042280</td>\n",
       "      <td>21.586268</td>\n",
       "      <td>24.280325</td>\n",
       "      <td>24.475947</td>\n",
       "      <td>21.198360</td>\n",
       "      <td>24.268387</td>\n",
       "      <td>...</td>\n",
       "      <td>96.428571</td>\n",
       "      <td>138.428571</td>\n",
       "      <td>184.428571</td>\n",
       "      <td>214.142857</td>\n",
       "      <td>239.857143</td>\n",
       "      <td>282.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>374.142857</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>461.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-06</th>\n",
       "      <td>24.616242</td>\n",
       "      <td>25.042280</td>\n",
       "      <td>21.586268</td>\n",
       "      <td>24.280325</td>\n",
       "      <td>24.475947</td>\n",
       "      <td>21.198360</td>\n",
       "      <td>24.268387</td>\n",
       "      <td>24.651690</td>\n",
       "      <td>20.792376</td>\n",
       "      <td>24.279004</td>\n",
       "      <td>...</td>\n",
       "      <td>138.428571</td>\n",
       "      <td>184.428571</td>\n",
       "      <td>214.142857</td>\n",
       "      <td>239.857143</td>\n",
       "      <td>282.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>374.142857</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>461.714286</td>\n",
       "      <td>499.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-07</th>\n",
       "      <td>24.280325</td>\n",
       "      <td>24.475947</td>\n",
       "      <td>21.198360</td>\n",
       "      <td>24.268387</td>\n",
       "      <td>24.651690</td>\n",
       "      <td>20.792376</td>\n",
       "      <td>24.279004</td>\n",
       "      <td>24.994527</td>\n",
       "      <td>20.590845</td>\n",
       "      <td>24.030306</td>\n",
       "      <td>...</td>\n",
       "      <td>184.428571</td>\n",
       "      <td>214.142857</td>\n",
       "      <td>239.857143</td>\n",
       "      <td>282.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>374.142857</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>461.714286</td>\n",
       "      <td>499.857143</td>\n",
       "      <td>531.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-08</th>\n",
       "      <td>24.268387</td>\n",
       "      <td>24.651690</td>\n",
       "      <td>20.792376</td>\n",
       "      <td>24.279004</td>\n",
       "      <td>24.994527</td>\n",
       "      <td>20.590845</td>\n",
       "      <td>24.030306</td>\n",
       "      <td>24.884379</td>\n",
       "      <td>20.461919</td>\n",
       "      <td>24.089472</td>\n",
       "      <td>...</td>\n",
       "      <td>214.142857</td>\n",
       "      <td>239.857143</td>\n",
       "      <td>282.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>374.142857</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>461.714286</td>\n",
       "      <td>499.857143</td>\n",
       "      <td>531.000000</td>\n",
       "      <td>549.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-17</th>\n",
       "      <td>22.274262</td>\n",
       "      <td>35.131666</td>\n",
       "      <td>22.239621</td>\n",
       "      <td>22.250779</td>\n",
       "      <td>35.052849</td>\n",
       "      <td>22.831868</td>\n",
       "      <td>22.082581</td>\n",
       "      <td>34.776027</td>\n",
       "      <td>23.138793</td>\n",
       "      <td>21.799723</td>\n",
       "      <td>...</td>\n",
       "      <td>254.714286</td>\n",
       "      <td>261.571429</td>\n",
       "      <td>274.571429</td>\n",
       "      <td>269.285714</td>\n",
       "      <td>263.285714</td>\n",
       "      <td>267.714286</td>\n",
       "      <td>255.714286</td>\n",
       "      <td>259.000000</td>\n",
       "      <td>253.285714</td>\n",
       "      <td>250.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-18</th>\n",
       "      <td>22.250779</td>\n",
       "      <td>35.052849</td>\n",
       "      <td>22.831868</td>\n",
       "      <td>22.082581</td>\n",
       "      <td>34.776027</td>\n",
       "      <td>23.138793</td>\n",
       "      <td>21.799723</td>\n",
       "      <td>33.634505</td>\n",
       "      <td>23.251944</td>\n",
       "      <td>21.560343</td>\n",
       "      <td>...</td>\n",
       "      <td>261.571429</td>\n",
       "      <td>274.571429</td>\n",
       "      <td>269.285714</td>\n",
       "      <td>263.285714</td>\n",
       "      <td>267.714286</td>\n",
       "      <td>255.714286</td>\n",
       "      <td>259.000000</td>\n",
       "      <td>253.285714</td>\n",
       "      <td>250.285714</td>\n",
       "      <td>249.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-19</th>\n",
       "      <td>22.082581</td>\n",
       "      <td>34.776027</td>\n",
       "      <td>23.138793</td>\n",
       "      <td>21.799723</td>\n",
       "      <td>33.634505</td>\n",
       "      <td>23.251944</td>\n",
       "      <td>21.560343</td>\n",
       "      <td>32.425871</td>\n",
       "      <td>23.347154</td>\n",
       "      <td>21.668993</td>\n",
       "      <td>...</td>\n",
       "      <td>274.571429</td>\n",
       "      <td>269.285714</td>\n",
       "      <td>263.285714</td>\n",
       "      <td>267.714286</td>\n",
       "      <td>255.714286</td>\n",
       "      <td>259.000000</td>\n",
       "      <td>253.285714</td>\n",
       "      <td>250.285714</td>\n",
       "      <td>249.714286</td>\n",
       "      <td>254.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-20</th>\n",
       "      <td>21.799723</td>\n",
       "      <td>33.634505</td>\n",
       "      <td>23.251944</td>\n",
       "      <td>21.560343</td>\n",
       "      <td>32.425871</td>\n",
       "      <td>23.347154</td>\n",
       "      <td>21.668993</td>\n",
       "      <td>31.181405</td>\n",
       "      <td>23.458170</td>\n",
       "      <td>21.891241</td>\n",
       "      <td>...</td>\n",
       "      <td>269.285714</td>\n",
       "      <td>263.285714</td>\n",
       "      <td>267.714286</td>\n",
       "      <td>255.714286</td>\n",
       "      <td>259.000000</td>\n",
       "      <td>253.285714</td>\n",
       "      <td>250.285714</td>\n",
       "      <td>249.714286</td>\n",
       "      <td>254.142857</td>\n",
       "      <td>253.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-21</th>\n",
       "      <td>21.560343</td>\n",
       "      <td>32.425871</td>\n",
       "      <td>23.347154</td>\n",
       "      <td>21.668993</td>\n",
       "      <td>31.181405</td>\n",
       "      <td>23.458170</td>\n",
       "      <td>21.891241</td>\n",
       "      <td>29.244671</td>\n",
       "      <td>23.651876</td>\n",
       "      <td>21.963463</td>\n",
       "      <td>...</td>\n",
       "      <td>263.285714</td>\n",
       "      <td>267.714286</td>\n",
       "      <td>255.714286</td>\n",
       "      <td>259.000000</td>\n",
       "      <td>253.285714</td>\n",
       "      <td>250.285714</td>\n",
       "      <td>249.714286</td>\n",
       "      <td>254.142857</td>\n",
       "      <td>253.428571</td>\n",
       "      <td>256.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>383 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Fièvre(t-29)  Mal de gorge(t-29)  Symptôme(t-29)  \\\n",
       "LOC DATE                                                           \n",
       "BE  2020-03-04     25.402051           25.829284       22.034428   \n",
       "    2020-03-05     25.245307           25.878495       21.941446   \n",
       "    2020-03-06     24.616242           25.042280       21.586268   \n",
       "    2020-03-07     24.280325           24.475947       21.198360   \n",
       "    2020-03-08     24.268387           24.651690       20.792376   \n",
       "...                      ...                 ...             ...   \n",
       "    2021-03-17     22.274262           35.131666       22.239621   \n",
       "    2021-03-18     22.250779           35.052849       22.831868   \n",
       "    2021-03-19     22.082581           34.776027       23.138793   \n",
       "    2021-03-20     21.799723           33.634505       23.251944   \n",
       "    2021-03-21     21.560343           32.425871       23.347154   \n",
       "\n",
       "                Fièvre(t-28)  Mal de gorge(t-28)  Symptôme(t-28)  \\\n",
       "LOC DATE                                                           \n",
       "BE  2020-03-04     25.245307           25.878495       21.941446   \n",
       "    2020-03-05     24.616242           25.042280       21.586268   \n",
       "    2020-03-06     24.280325           24.475947       21.198360   \n",
       "    2020-03-07     24.268387           24.651690       20.792376   \n",
       "    2020-03-08     24.279004           24.994527       20.590845   \n",
       "...                      ...                 ...             ...   \n",
       "    2021-03-17     22.250779           35.052849       22.831868   \n",
       "    2021-03-18     22.082581           34.776027       23.138793   \n",
       "    2021-03-19     21.799723           33.634505       23.251944   \n",
       "    2021-03-20     21.560343           32.425871       23.347154   \n",
       "    2021-03-21     21.668993           31.181405       23.458170   \n",
       "\n",
       "                Fièvre(t-27)  Mal de gorge(t-27)  Symptôme(t-27)  \\\n",
       "LOC DATE                                                           \n",
       "BE  2020-03-04     24.616242           25.042280       21.586268   \n",
       "    2020-03-05     24.280325           24.475947       21.198360   \n",
       "    2020-03-06     24.268387           24.651690       20.792376   \n",
       "    2020-03-07     24.279004           24.994527       20.590845   \n",
       "    2020-03-08     24.030306           24.884379       20.461919   \n",
       "...                      ...                 ...             ...   \n",
       "    2021-03-17     22.082581           34.776027       23.138793   \n",
       "    2021-03-18     21.799723           33.634505       23.251944   \n",
       "    2021-03-19     21.560343           32.425871       23.347154   \n",
       "    2021-03-20     21.668993           31.181405       23.458170   \n",
       "    2021-03-21     21.891241           29.244671       23.651876   \n",
       "\n",
       "                Fièvre(t-26)  ...  NEW_HOSP(t+11)  NEW_HOSP(t+12)  \\\n",
       "LOC DATE                      ...                                   \n",
       "BE  2020-03-04     24.280325  ...       66.000000       96.428571   \n",
       "    2020-03-05     24.268387  ...       96.428571      138.428571   \n",
       "    2020-03-06     24.279004  ...      138.428571      184.428571   \n",
       "    2020-03-07     24.030306  ...      184.428571      214.142857   \n",
       "    2020-03-08     24.089472  ...      214.142857      239.857143   \n",
       "...                      ...  ...             ...             ...   \n",
       "    2021-03-17     21.799723  ...      254.714286      261.571429   \n",
       "    2021-03-18     21.560343  ...      261.571429      274.571429   \n",
       "    2021-03-19     21.668993  ...      274.571429      269.285714   \n",
       "    2021-03-20     21.891241  ...      269.285714      263.285714   \n",
       "    2021-03-21     21.963463  ...      263.285714      267.714286   \n",
       "\n",
       "                NEW_HOSP(t+13)  NEW_HOSP(t+14)  NEW_HOSP(t+15)  \\\n",
       "LOC DATE                                                         \n",
       "BE  2020-03-04      138.428571      184.428571      214.142857   \n",
       "    2020-03-05      184.428571      214.142857      239.857143   \n",
       "    2020-03-06      214.142857      239.857143      282.000000   \n",
       "    2020-03-07      239.857143      282.000000      334.000000   \n",
       "    2020-03-08      282.000000      334.000000      374.142857   \n",
       "...                        ...             ...             ...   \n",
       "    2021-03-17      274.571429      269.285714      263.285714   \n",
       "    2021-03-18      269.285714      263.285714      267.714286   \n",
       "    2021-03-19      263.285714      267.714286      255.714286   \n",
       "    2021-03-20      267.714286      255.714286      259.000000   \n",
       "    2021-03-21      255.714286      259.000000      253.285714   \n",
       "\n",
       "                NEW_HOSP(t+16)  NEW_HOSP(t+17)  NEW_HOSP(t+18)  \\\n",
       "LOC DATE                                                         \n",
       "BE  2020-03-04      239.857143      282.000000      334.000000   \n",
       "    2020-03-05      282.000000      334.000000      374.142857   \n",
       "    2020-03-06      334.000000      374.142857      416.000000   \n",
       "    2020-03-07      374.142857      416.000000      461.714286   \n",
       "    2020-03-08      416.000000      461.714286      499.857143   \n",
       "...                        ...             ...             ...   \n",
       "    2021-03-17      267.714286      255.714286      259.000000   \n",
       "    2021-03-18      255.714286      259.000000      253.285714   \n",
       "    2021-03-19      259.000000      253.285714      250.285714   \n",
       "    2021-03-20      253.285714      250.285714      249.714286   \n",
       "    2021-03-21      250.285714      249.714286      254.142857   \n",
       "\n",
       "                NEW_HOSP(t+19)  NEW_HOSP(t+20)  \n",
       "LOC DATE                                        \n",
       "BE  2020-03-04      374.142857      416.000000  \n",
       "    2020-03-05      416.000000      461.714286  \n",
       "    2020-03-06      461.714286      499.857143  \n",
       "    2020-03-07      499.857143      531.000000  \n",
       "    2020-03-08      531.000000      549.571429  \n",
       "...                        ...             ...  \n",
       "    2021-03-17      253.285714      250.285714  \n",
       "    2021-03-18      250.285714      249.714286  \n",
       "    2021-03-19      249.714286      254.142857  \n",
       "    2021-03-20      254.142857      253.428571  \n",
       "    2021-03-21      253.428571      256.714286  \n",
       "\n",
       "[383 rows x 110 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c['BE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_generator = MinMaxScaler\n",
    "dg_2 = util.DataGenerator(df_c, n_samples, n_forecast, target=target, scaler_generator=scaler_generator, \n",
    "                          scaler_type='batch', augment_merge=0, predict_one=False, cumsum=False,\n",
    "                          data_columns=[k for k in list_topics], no_lag=True)\n",
    "dg_2.set_loc_init(dg.loc_init)  # consider the other localisations as being augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the second estimator to compute $Y_2(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.5092 - mse: 0.5092 - mae: 0.5694 - root_mean_squared_error: 0.7136\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4733 - mse: 0.4733 - mae: 0.5465 - root_mean_squared_error: 0.6879\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4491 - mse: 0.4491 - mae: 0.5306 - root_mean_squared_error: 0.6702\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4300 - mse: 0.4300 - mae: 0.5177 - root_mean_squared_error: 0.6557\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4137 - mse: 0.4137 - mae: 0.5065 - root_mean_squared_error: 0.6432\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3994 - mse: 0.3994 - mae: 0.4965 - root_mean_squared_error: 0.6320\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3865 - mse: 0.3865 - mae: 0.4874 - root_mean_squared_error: 0.6217\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3747 - mse: 0.3747 - mae: 0.4789 - root_mean_squared_error: 0.6121\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3637 - mse: 0.3637 - mae: 0.4709 - root_mean_squared_error: 0.6031\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3535 - mse: 0.3535 - mae: 0.4633 - root_mean_squared_error: 0.5945\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3439 - mse: 0.3439 - mae: 0.4561 - root_mean_squared_error: 0.5864\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3348 - mse: 0.3348 - mae: 0.4492 - root_mean_squared_error: 0.5786\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3261 - mse: 0.3261 - mae: 0.4425 - root_mean_squared_error: 0.5711\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3179 - mse: 0.3179 - mae: 0.4362 - root_mean_squared_error: 0.5638\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3100 - mse: 0.3100 - mae: 0.4300 - root_mean_squared_error: 0.5568\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3024 - mse: 0.3024 - mae: 0.4241 - root_mean_squared_error: 0.5499\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2952 - mse: 0.2952 - mae: 0.4184 - root_mean_squared_error: 0.5433\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2882 - mse: 0.2882 - mae: 0.4128 - root_mean_squared_error: 0.5368\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2815 - mse: 0.2815 - mae: 0.4074 - root_mean_squared_error: 0.5305\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2750 - mse: 0.2750 - mae: 0.4022 - root_mean_squared_error: 0.5244\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2687 - mse: 0.2687 - mae: 0.3972 - root_mean_squared_error: 0.5184\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2626 - mse: 0.2626 - mae: 0.3922 - root_mean_squared_error: 0.5125\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2568 - mse: 0.2568 - mae: 0.3875 - root_mean_squared_error: 0.5067\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2511 - mse: 0.2511 - mae: 0.3828 - root_mean_squared_error: 0.5011\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2456 - mse: 0.2456 - mae: 0.3783 - root_mean_squared_error: 0.4956\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2403 - mse: 0.2403 - mae: 0.3739 - root_mean_squared_error: 0.4902\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2351 - mse: 0.2351 - mae: 0.3697 - root_mean_squared_error: 0.4849\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2301 - mse: 0.2301 - mae: 0.3655 - root_mean_squared_error: 0.4797\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2252 - mse: 0.2252 - mae: 0.3614 - root_mean_squared_error: 0.4746\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2205 - mse: 0.2205 - mae: 0.3575 - root_mean_squared_error: 0.4696\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2159 - mse: 0.2159 - mae: 0.3537 - root_mean_squared_error: 0.4647\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2115 - mse: 0.2115 - mae: 0.3499 - root_mean_squared_error: 0.4599\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2072 - mse: 0.2072 - mae: 0.3463 - root_mean_squared_error: 0.4551\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2030 - mse: 0.2030 - mae: 0.3428 - root_mean_squared_error: 0.4505\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1989 - mse: 0.1989 - mae: 0.3393 - root_mean_squared_error: 0.4460\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1950 - mse: 0.1950 - mae: 0.3360 - root_mean_squared_error: 0.4415\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1911 - mse: 0.1911 - mae: 0.3328 - root_mean_squared_error: 0.4372\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1874 - mse: 0.1874 - mae: 0.3296 - root_mean_squared_error: 0.4329\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1838 - mse: 0.1838 - mae: 0.3266 - root_mean_squared_error: 0.4287\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1803 - mse: 0.1803 - mae: 0.3236 - root_mean_squared_error: 0.4246\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1769 - mse: 0.1769 - mae: 0.3207 - root_mean_squared_error: 0.4206\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1737 - mse: 0.1737 - mae: 0.3179 - root_mean_squared_error: 0.4167\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1705 - mse: 0.1705 - mae: 0.3152 - root_mean_squared_error: 0.4129\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1674 - mse: 0.1674 - mae: 0.3125 - root_mean_squared_error: 0.4091\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1644 - mse: 0.1644 - mae: 0.3100 - root_mean_squared_error: 0.4055\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1615 - mse: 0.1615 - mae: 0.3075 - root_mean_squared_error: 0.4019\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1587 - mse: 0.1587 - mae: 0.3051 - root_mean_squared_error: 0.3984\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1560 - mse: 0.1560 - mae: 0.3028 - root_mean_squared_error: 0.3950\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1534 - mse: 0.1534 - mae: 0.3006 - root_mean_squared_error: 0.3917\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1509 - mse: 0.1509 - mae: 0.2984 - root_mean_squared_error: 0.3885\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1485 - mse: 0.1485 - mae: 0.2963 - root_mean_squared_error: 0.3853\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1461 - mse: 0.1461 - mae: 0.2943 - root_mean_squared_error: 0.3823\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1439 - mse: 0.1439 - mae: 0.2924 - root_mean_squared_error: 0.3793\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1417 - mse: 0.1417 - mae: 0.2905 - root_mean_squared_error: 0.3764\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1396 - mse: 0.1396 - mae: 0.2887 - root_mean_squared_error: 0.3736\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1375 - mse: 0.1375 - mae: 0.2870 - root_mean_squared_error: 0.3709\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1356 - mse: 0.1356 - mae: 0.2853 - root_mean_squared_error: 0.3682\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1337 - mse: 0.1337 - mae: 0.2837 - root_mean_squared_error: 0.3657\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1319 - mse: 0.1319 - mae: 0.2822 - root_mean_squared_error: 0.3632\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1301 - mse: 0.1301 - mae: 0.2807 - root_mean_squared_error: 0.3607\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1285 - mse: 0.1285 - mae: 0.2793 - root_mean_squared_error: 0.3584\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1268 - mse: 0.1268 - mae: 0.2780 - root_mean_squared_error: 0.3562\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1253 - mse: 0.1253 - mae: 0.2767 - root_mean_squared_error: 0.3540\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1238 - mse: 0.1238 - mae: 0.2755 - root_mean_squared_error: 0.3518\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1224 - mse: 0.1224 - mae: 0.2743 - root_mean_squared_error: 0.3498\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1210 - mse: 0.1210 - mae: 0.2732 - root_mean_squared_error: 0.3478\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1196 - mse: 0.1196 - mae: 0.2721 - root_mean_squared_error: 0.3459\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1184 - mse: 0.1184 - mae: 0.2711 - root_mean_squared_error: 0.3440\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1171 - mse: 0.1171 - mae: 0.2700 - root_mean_squared_error: 0.3422\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1159 - mse: 0.1159 - mae: 0.2691 - root_mean_squared_error: 0.3405\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1148 - mse: 0.1148 - mae: 0.2681 - root_mean_squared_error: 0.3388\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1137 - mse: 0.1137 - mae: 0.2673 - root_mean_squared_error: 0.3372\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1127 - mse: 0.1127 - mae: 0.2664 - root_mean_squared_error: 0.3356\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1116 - mse: 0.1116 - mae: 0.2656 - root_mean_squared_error: 0.3341\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1107 - mse: 0.1107 - mae: 0.2648 - root_mean_squared_error: 0.3327\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1097 - mse: 0.1097 - mae: 0.2640 - root_mean_squared_error: 0.3313\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1088 - mse: 0.1088 - mae: 0.2633 - root_mean_squared_error: 0.3299\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1080 - mse: 0.1080 - mae: 0.2626 - root_mean_squared_error: 0.3286\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1071 - mse: 0.1071 - mae: 0.2619 - root_mean_squared_error: 0.3273\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1063 - mse: 0.1063 - mae: 0.2612 - root_mean_squared_error: 0.3261\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1056 - mse: 0.1056 - mae: 0.2606 - root_mean_squared_error: 0.3249\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1048 - mse: 0.1048 - mae: 0.2600 - root_mean_squared_error: 0.3238\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1041 - mse: 0.1041 - mae: 0.2594 - root_mean_squared_error: 0.3227\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1035 - mse: 0.1035 - mae: 0.2589 - root_mean_squared_error: 0.3216\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1028 - mse: 0.1028 - mae: 0.2583 - root_mean_squared_error: 0.3206\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1022 - mse: 0.1022 - mae: 0.2579 - root_mean_squared_error: 0.3197\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1016 - mse: 0.1016 - mae: 0.2572 - root_mean_squared_error: 0.3187\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1010 - mse: 0.1010 - mae: 0.2568 - root_mean_squared_error: 0.3178\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1004 - mse: 0.1004 - mae: 0.2563 - root_mean_squared_error: 0.3169\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0999 - mse: 0.0999 - mae: 0.2559 - root_mean_squared_error: 0.3161\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0994 - mse: 0.0994 - mae: 0.2554 - root_mean_squared_error: 0.3153\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0989 - mse: 0.0989 - mae: 0.2550 - root_mean_squared_error: 0.3145\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0984 - mse: 0.0984 - mae: 0.2545 - root_mean_squared_error: 0.3137\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0980 - mse: 0.0980 - mae: 0.2542 - root_mean_squared_error: 0.3130\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0975 - mse: 0.0975 - mae: 0.2538 - root_mean_squared_error: 0.3123\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0971 - mse: 0.0971 - mae: 0.2534 - root_mean_squared_error: 0.3116\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0967 - mse: 0.0967 - mae: 0.2530 - root_mean_squared_error: 0.3110\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0963 - mse: 0.0963 - mae: 0.2527 - root_mean_squared_error: 0.3103\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0959 - mse: 0.0959 - mae: 0.2523 - root_mean_squared_error: 0.3097\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0956 - mse: 0.0956 - mae: 0.2520 - root_mean_squared_error: 0.3091\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0952 - mse: 0.0952 - mae: 0.2516 - root_mean_squared_error: 0.3086\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0949 - mse: 0.0949 - mae: 0.2513 - root_mean_squared_error: 0.3080\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0945 - mse: 0.0945 - mae: 0.2509 - root_mean_squared_error: 0.3075\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0942 - mse: 0.0942 - mae: 0.2507 - root_mean_squared_error: 0.3069\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0939 - mse: 0.0939 - mae: 0.2503 - root_mean_squared_error: 0.3064\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0936 - mse: 0.0936 - mae: 0.2501 - root_mean_squared_error: 0.3059\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0933 - mse: 0.0933 - mae: 0.2497 - root_mean_squared_error: 0.3054\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0930 - mse: 0.0930 - mae: 0.2495 - root_mean_squared_error: 0.3050\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0927 - mse: 0.0927 - mae: 0.2491 - root_mean_squared_error: 0.3045\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0924 - mse: 0.0924 - mae: 0.2488 - root_mean_squared_error: 0.3040\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0922 - mse: 0.0922 - mae: 0.2486 - root_mean_squared_error: 0.3036\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0919 - mse: 0.0919 - mae: 0.2483 - root_mean_squared_error: 0.3032\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0916 - mse: 0.0916 - mae: 0.2480 - root_mean_squared_error: 0.3027\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0914 - mse: 0.0914 - mae: 0.2477 - root_mean_squared_error: 0.3023\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0911 - mse: 0.0911 - mae: 0.2474 - root_mean_squared_error: 0.3019\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0909 - mse: 0.0909 - mae: 0.2472 - root_mean_squared_error: 0.3015\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0906 - mse: 0.0906 - mae: 0.2469 - root_mean_squared_error: 0.3010\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0904 - mse: 0.0904 - mae: 0.2467 - root_mean_squared_error: 0.3006\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0901 - mse: 0.0901 - mae: 0.2463 - root_mean_squared_error: 0.3002\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0899 - mse: 0.0899 - mae: 0.2461 - root_mean_squared_error: 0.2998\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0897 - mse: 0.0897 - mae: 0.2458 - root_mean_squared_error: 0.2994\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0894 - mse: 0.0894 - mae: 0.2456 - root_mean_squared_error: 0.2991\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0892 - mse: 0.0892 - mae: 0.2453 - root_mean_squared_error: 0.2987\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0890 - mse: 0.0890 - mae: 0.2451 - root_mean_squared_error: 0.2983\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0887 - mse: 0.0887 - mae: 0.2448 - root_mean_squared_error: 0.2979\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0885 - mse: 0.0885 - mae: 0.2446 - root_mean_squared_error: 0.2975\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0883 - mse: 0.0883 - mae: 0.2443 - root_mean_squared_error: 0.2972\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0881 - mse: 0.0881 - mae: 0.2441 - root_mean_squared_error: 0.2968\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0879 - mse: 0.0879 - mae: 0.2438 - root_mean_squared_error: 0.2964\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0877 - mse: 0.0877 - mae: 0.2436 - root_mean_squared_error: 0.2961\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0874 - mse: 0.0874 - mae: 0.2433 - root_mean_squared_error: 0.2957\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0872 - mse: 0.0872 - mae: 0.2431 - root_mean_squared_error: 0.2954\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0870 - mse: 0.0870 - mae: 0.2429 - root_mean_squared_error: 0.2950\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0868 - mse: 0.0868 - mae: 0.2427 - root_mean_squared_error: 0.2947\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0866 - mse: 0.0866 - mae: 0.2424 - root_mean_squared_error: 0.2943\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0864 - mse: 0.0864 - mae: 0.2422 - root_mean_squared_error: 0.2940\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0862 - mse: 0.0862 - mae: 0.2419 - root_mean_squared_error: 0.2937\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0860 - mse: 0.0860 - mae: 0.2418 - root_mean_squared_error: 0.2933\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0859 - mse: 0.0859 - mae: 0.2415 - root_mean_squared_error: 0.2930\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0857 - mse: 0.0857 - mae: 0.2414 - root_mean_squared_error: 0.2927\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0855 - mse: 0.0855 - mae: 0.2411 - root_mean_squared_error: 0.2924\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0853 - mse: 0.0853 - mae: 0.2409 - root_mean_squared_error: 0.2921\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0851 - mse: 0.0851 - mae: 0.2407 - root_mean_squared_error: 0.2918\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0849 - mse: 0.0849 - mae: 0.2405 - root_mean_squared_error: 0.2915\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0848 - mse: 0.0848 - mae: 0.2403 - root_mean_squared_error: 0.2912\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0846 - mse: 0.0846 - mae: 0.2401 - root_mean_squared_error: 0.2909\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0844 - mse: 0.0844 - mae: 0.2400 - root_mean_squared_error: 0.2906\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0843 - mse: 0.0843 - mae: 0.2397 - root_mean_squared_error: 0.2903\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0841 - mse: 0.0841 - mae: 0.2396 - root_mean_squared_error: 0.2900\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0840 - mse: 0.0840 - mae: 0.2394 - root_mean_squared_error: 0.2898\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0838 - mse: 0.0838 - mae: 0.2392 - root_mean_squared_error: 0.2895\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0836 - mse: 0.0836 - mae: 0.2391 - root_mean_squared_error: 0.2892\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0835 - mse: 0.0835 - mae: 0.2389 - root_mean_squared_error: 0.2889\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0833 - mse: 0.0833 - mae: 0.2387 - root_mean_squared_error: 0.2887\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0832 - mse: 0.0832 - mae: 0.2386 - root_mean_squared_error: 0.2884\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0831 - mse: 0.0831 - mae: 0.2384 - root_mean_squared_error: 0.2882\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0829 - mse: 0.0829 - mae: 0.2382 - root_mean_squared_error: 0.2879\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0828 - mse: 0.0828 - mae: 0.2381 - root_mean_squared_error: 0.2877\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0826 - mse: 0.0826 - mae: 0.2379 - root_mean_squared_error: 0.2875\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0825 - mse: 0.0825 - mae: 0.2379 - root_mean_squared_error: 0.2872\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0824 - mse: 0.0824 - mae: 0.2376 - root_mean_squared_error: 0.2870\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0822 - mse: 0.0822 - mae: 0.2375 - root_mean_squared_error: 0.2868\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0821 - mse: 0.0821 - mae: 0.2374 - root_mean_squared_error: 0.2865\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0820 - mse: 0.0820 - mae: 0.2371 - root_mean_squared_error: 0.2863\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0819 - mse: 0.0819 - mae: 0.2373 - root_mean_squared_error: 0.2861\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0817 - mse: 0.0817 - mae: 0.2369 - root_mean_squared_error: 0.2859\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0816 - mse: 0.0816 - mae: 0.2370 - root_mean_squared_error: 0.2857\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0815 - mse: 0.0815 - mae: 0.2367 - root_mean_squared_error: 0.2855\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0814 - mse: 0.0814 - mae: 0.2367 - root_mean_squared_error: 0.2853\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0813 - mse: 0.0813 - mae: 0.2365 - root_mean_squared_error: 0.2851\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0812 - mse: 0.0812 - mae: 0.2364 - root_mean_squared_error: 0.2849\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0811 - mse: 0.0811 - mae: 0.2363 - root_mean_squared_error: 0.2847\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0810 - mse: 0.0810 - mae: 0.2362 - root_mean_squared_error: 0.2845\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0809 - mse: 0.0809 - mae: 0.2361 - root_mean_squared_error: 0.2843\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0807 - mse: 0.0807 - mae: 0.2360 - root_mean_squared_error: 0.2842\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0806 - mse: 0.0806 - mae: 0.2359 - root_mean_squared_error: 0.2840\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0805 - mse: 0.0805 - mae: 0.2358 - root_mean_squared_error: 0.2838\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0805 - mse: 0.0805 - mae: 0.2357 - root_mean_squared_error: 0.2836\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0804 - mse: 0.0804 - mae: 0.2356 - root_mean_squared_error: 0.2835\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0803 - mse: 0.0803 - mae: 0.2355 - root_mean_squared_error: 0.2833\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0802 - mse: 0.0802 - mae: 0.2355 - root_mean_squared_error: 0.2831\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0801 - mse: 0.0801 - mae: 0.2352 - root_mean_squared_error: 0.2830\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0800 - mse: 0.0800 - mae: 0.2353 - root_mean_squared_error: 0.2828\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0799 - mse: 0.0799 - mae: 0.2351 - root_mean_squared_error: 0.2827\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0798 - mse: 0.0798 - mae: 0.2351 - root_mean_squared_error: 0.2825\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0797 - mse: 0.0797 - mae: 0.2349 - root_mean_squared_error: 0.2824\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0796 - mse: 0.0796 - mae: 0.2349 - root_mean_squared_error: 0.2822\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0796 - mse: 0.0796 - mae: 0.2348 - root_mean_squared_error: 0.2821\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0795 - mse: 0.0795 - mae: 0.2348 - root_mean_squared_error: 0.2819\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0794 - mse: 0.0794 - mae: 0.2347 - root_mean_squared_error: 0.2818\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0793 - mse: 0.0793 - mae: 0.2346 - root_mean_squared_error: 0.2816\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0792 - mse: 0.0792 - mae: 0.2346 - root_mean_squared_error: 0.2815\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0792 - mse: 0.0792 - mae: 0.2344 - root_mean_squared_error: 0.2814\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0791 - mse: 0.0791 - mae: 0.2344 - root_mean_squared_error: 0.2812\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0790 - mse: 0.0790 - mae: 0.2343 - root_mean_squared_error: 0.2811\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0789 - mse: 0.0789 - mae: 0.2343 - root_mean_squared_error: 0.2810\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0789 - mse: 0.0789 - mae: 0.2342 - root_mean_squared_error: 0.2808\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0788 - mse: 0.0788 - mae: 0.2341 - root_mean_squared_error: 0.2807\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0787 - mse: 0.0787 - mae: 0.2341 - root_mean_squared_error: 0.2806\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0787 - mse: 0.0787 - mae: 0.2340 - root_mean_squared_error: 0.2805\n"
     ]
    }
   ],
   "source": [
    "X_train_2 = dg_2.get_x(train_idx, scaled=True)\n",
    "Y_train_2 = dg_2.get_y(train_idx, scaled=True)\n",
    "\n",
    "if len(valid_idx) > 0:\n",
    "    X_val_2 = dg_2.get_x(valid_idx, scaled=True, use_previous_scaler=True, geo=dg_2.loc_init)\n",
    "    Y_val_unscaled_2 = dg.get_y(valid_idx, geo=dg.loc_init, scaled=False)\n",
    "    Y_val_real_2 = dg.remove_padded_y(Y_val_unscaled, idx=val_idx, geo=dg.loc_init)\n",
    "    batch_size_val = len(X_val_2)\n",
    "    \n",
    "X_test_2 = dg_2.get_x(test_idx, scaled=True, use_previous_scaler=True, geo=dg_2.loc_init)\n",
    "Y_test_unscaled_2 = dg.get_y(test_idx, scaled=False, geo=dg_2.loc_init)\n",
    "Y_test_real_2 = dg.remove_padded_y(Y_test_unscaled_2, idx=test_idx, geo=dg_2.loc_init)\n",
    "\n",
    "model_generator = get_dense_model\n",
    "\n",
    "batch_size_train = len(X_train_2)\n",
    "batch_size_test = len(X_test_2)\n",
    "model = model_generator(batch_input_shape=(batch_size_train, n_samples, dg_2.n_features))\n",
    "    \n",
    "history = model.fit(X_train_2, Y_train_2, batch_size=batch_size_train, epochs=epochs, callbacks=None)\n",
    "# compute the predictions on both the training and the validation\n",
    "Y_train_pred_2 = model.predict(X_train_2, batch_size=batch_size_train)\n",
    "\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_test, n_samples, dg_2.n_features))\n",
    "model_prediction.set_weights(model.get_weights())\n",
    "Y_test_pred_2 = model_prediction.predict(X_test_2, batch_size=batch_size_test)\n",
    "# unscale and unpad the data\n",
    "Y_test_pred_unscaled_2 = dg_2.inverse_transform_y(Y_test_pred_2, idx=test_idx, geo=dg_2.loc_init)\n",
    "Y_test_pred_real_2 = dg.remove_padded_y(Y_test_pred_unscaled_2, idx=test_idx, geo=dg_2.loc_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the final prediction by combining the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssembleLayerTimeDist(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    assemble the prediction from the trends and the predictions from another model\n",
    "    the same weight is used at every timestep (1 trainable parameter)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape):\n",
    "        super(AssembleLayer, self).__init__(name='')\n",
    "        self.dense_trends = tf.keras.layers.Dense(1, use_bias=False, \n",
    "                                                 kernel_constraint=tf.keras.constraints.MinMaxNorm(0.001, 0.2))\n",
    "        batch_input_shape = (input_shape[0], input_shape[1], 1)\n",
    "        self.time_dist = TimeDistributed(self.dense_trends, batch_input_shape=batch_input_shape)\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x_trends = input_tensor[:, :, 1:]\n",
    "        x_hosp = input_tensor[:, :, :1]\n",
    "        x_trends = tf.keras.layers.Subtract()([x_trends, x_hosp])  # x_trends - x_hosp\n",
    "        x_trends = self.time_dist(x_trends, training=training)  # apply simple weight\n",
    "        return tf.keras.layers.Add()([x_trends, x_hosp])  # final prediction = x_hosp + (x_trends - x_hosp) * c\n",
    "\n",
    "class AssembleLayer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    assemble the prediction from the trends and the predictions from another model\n",
    "    different weight are used at every timestep (n_forecast trainable parameter)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_input_shape):\n",
    "        super(AssembleLayer, self).__init__(name='')\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[1,batch_input_shape[1]], \n",
    "                                      constraint=tf.keras.constraints.MinMaxNorm(0.001, 0.2))\n",
    "\n",
    "    \"\"\"\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                              shape=[1,input_shape[1]], constraint=tf.keras.constraints.MinMaxNorm(0.001, 0.2))\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x_trends = input_tensor[:, :, 1]\n",
    "        x_hosp = input_tensor[:, :, 0]\n",
    "        x_trends = tf.keras.layers.Subtract()([x_trends, x_hosp])  # x_trends - x_hosp\n",
    "        x_trends = tf.multiply(x_trends, self.kernel)  # apply simple weight: (x_trends - x_hosp) * c\n",
    "        # x_trends = tf.reshape(x_trends, x_hosp.shape)\n",
    "        return tf.keras.layers.Add()([x_hosp, x_trends])  # final prediction = x_hosp + (x_trends - x_hosp) * c\n",
    "\n",
    "    \n",
    "def get_assemble(batch_input_shape):\n",
    "    model = AssembleLayer(batch_input_shape)\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.0253 - mse: 0.0253 - mae: 0.1184 - root_mean_squared_error: 0.1590\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0214 - mse: 0.0214 - mae: 0.1060 - root_mean_squared_error: 0.1464\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0214 - mse: 0.0214 - mae: 0.1059 - root_mean_squared_error: 0.1463\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0214 - mse: 0.0214 - mae: 0.1058 - root_mean_squared_error: 0.1462\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0213 - mse: 0.0213 - mae: 0.1057 - root_mean_squared_error: 0.1461\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0213 - mse: 0.0213 - mae: 0.1056 - root_mean_squared_error: 0.1460\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0213 - mse: 0.0213 - mae: 0.1055 - root_mean_squared_error: 0.1459\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0213 - mse: 0.0213 - mae: 0.1054 - root_mean_squared_error: 0.1458\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.1053 - root_mean_squared_error: 0.1458\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.1052 - root_mean_squared_error: 0.1457\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.1052 - root_mean_squared_error: 0.1456\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.1051 - root_mean_squared_error: 0.1455\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.1050 - root_mean_squared_error: 0.1455\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0211 - mse: 0.0211 - mae: 0.1049 - root_mean_squared_error: 0.1454\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0211 - mse: 0.0211 - mae: 0.1049 - root_mean_squared_error: 0.1453\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0211 - mse: 0.0211 - mae: 0.1048 - root_mean_squared_error: 0.1453\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0211 - mse: 0.0211 - mae: 0.1047 - root_mean_squared_error: 0.1452\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0211 - mse: 0.0211 - mae: 0.1047 - root_mean_squared_error: 0.1451\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0210 - mse: 0.0210 - mae: 0.1046 - root_mean_squared_error: 0.1451\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0210 - mse: 0.0210 - mae: 0.1045 - root_mean_squared_error: 0.1450\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0210 - mse: 0.0210 - mae: 0.1045 - root_mean_squared_error: 0.1449\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0210 - mse: 0.0210 - mae: 0.1044 - root_mean_squared_error: 0.1449\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0210 - mse: 0.0210 - mae: 0.1044 - root_mean_squared_error: 0.1448\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0210 - mse: 0.0210 - mae: 0.1043 - root_mean_squared_error: 0.1448\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0209 - mse: 0.0209 - mae: 0.1042 - root_mean_squared_error: 0.1447\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0209 - mse: 0.0209 - mae: 0.1042 - root_mean_squared_error: 0.1446\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0209 - mse: 0.0209 - mae: 0.1041 - root_mean_squared_error: 0.1446\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0209 - mse: 0.0209 - mae: 0.1041 - root_mean_squared_error: 0.1445\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0209 - mse: 0.0209 - mae: 0.1040 - root_mean_squared_error: 0.1445\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0209 - mse: 0.0209 - mae: 0.1039 - root_mean_squared_error: 0.1444\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0208 - mse: 0.0208 - mae: 0.1039 - root_mean_squared_error: 0.1444\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0208 - mse: 0.0208 - mae: 0.1038 - root_mean_squared_error: 0.1443\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0208 - mse: 0.0208 - mae: 0.1038 - root_mean_squared_error: 0.1443\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0208 - mse: 0.0208 - mae: 0.1037 - root_mean_squared_error: 0.1442\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0208 - mse: 0.0208 - mae: 0.1037 - root_mean_squared_error: 0.1442\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0208 - mse: 0.0208 - mae: 0.1036 - root_mean_squared_error: 0.1441\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0208 - mse: 0.0208 - mae: 0.1035 - root_mean_squared_error: 0.1441\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0207 - mse: 0.0207 - mae: 0.1035 - root_mean_squared_error: 0.1440\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0207 - mse: 0.0207 - mae: 0.1034 - root_mean_squared_error: 0.1439\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0207 - mse: 0.0207 - mae: 0.1034 - root_mean_squared_error: 0.1439\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0207 - mse: 0.0207 - mae: 0.1033 - root_mean_squared_error: 0.1438\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0207 - mse: 0.0207 - mae: 0.1033 - root_mean_squared_error: 0.1438\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0207 - mse: 0.0207 - mae: 0.1032 - root_mean_squared_error: 0.1437\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0206 - mse: 0.0206 - mae: 0.1032 - root_mean_squared_error: 0.1437\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0206 - mse: 0.0206 - mae: 0.1031 - root_mean_squared_error: 0.1436\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0206 - mse: 0.0206 - mae: 0.1031 - root_mean_squared_error: 0.1436\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0206 - mse: 0.0206 - mae: 0.1030 - root_mean_squared_error: 0.1436\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0206 - mse: 0.0206 - mae: 0.1030 - root_mean_squared_error: 0.1435\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0206 - mse: 0.0206 - mae: 0.1029 - root_mean_squared_error: 0.1435\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0206 - mse: 0.0206 - mae: 0.1029 - root_mean_squared_error: 0.1434\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0206 - mse: 0.0206 - mae: 0.1028 - root_mean_squared_error: 0.1434\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1028 - root_mean_squared_error: 0.1433\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1027 - root_mean_squared_error: 0.1433\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1027 - root_mean_squared_error: 0.1432\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1027 - root_mean_squared_error: 0.1432\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1026 - root_mean_squared_error: 0.1431\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1026 - root_mean_squared_error: 0.1431\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1025 - root_mean_squared_error: 0.1431\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.1025 - root_mean_squared_error: 0.1430\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1024 - root_mean_squared_error: 0.1430\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1024 - root_mean_squared_error: 0.1429\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1024 - root_mean_squared_error: 0.1429\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1023 - root_mean_squared_error: 0.1429\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1023 - root_mean_squared_error: 0.1428\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1022 - root_mean_squared_error: 0.1428\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1022 - root_mean_squared_error: 0.1427\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.1022 - root_mean_squared_error: 0.1427\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1021 - root_mean_squared_error: 0.1427\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1021 - root_mean_squared_error: 0.1426\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1020 - root_mean_squared_error: 0.1426\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1020 - root_mean_squared_error: 0.1425\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1020 - root_mean_squared_error: 0.1425\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1019 - root_mean_squared_error: 0.1425\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1019 - root_mean_squared_error: 0.1424\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1018 - root_mean_squared_error: 0.1424\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1018 - root_mean_squared_error: 0.1423\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.1018 - root_mean_squared_error: 0.1423\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1017 - root_mean_squared_error: 0.1423\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1017 - root_mean_squared_error: 0.1422\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1017 - root_mean_squared_error: 0.1422\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1016 - root_mean_squared_error: 0.1422\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1016 - root_mean_squared_error: 0.1421\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1016 - root_mean_squared_error: 0.1421\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1015 - root_mean_squared_error: 0.1421\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1015 - root_mean_squared_error: 0.1420\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1014 - root_mean_squared_error: 0.1420\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.1014 - root_mean_squared_error: 0.1420\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1014 - root_mean_squared_error: 0.1419\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1013 - root_mean_squared_error: 0.1419\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1013 - root_mean_squared_error: 0.1419\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1013 - root_mean_squared_error: 0.1418\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1012 - root_mean_squared_error: 0.1418\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1012 - root_mean_squared_error: 0.1418\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1012 - root_mean_squared_error: 0.1417\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1012 - root_mean_squared_error: 0.1417\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1011 - root_mean_squared_error: 0.1417\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.1011 - root_mean_squared_error: 0.1416\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1011 - root_mean_squared_error: 0.1416\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1010 - root_mean_squared_error: 0.1416\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1010 - root_mean_squared_error: 0.1415\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1010 - root_mean_squared_error: 0.1415\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1009 - root_mean_squared_error: 0.1415\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1009 - root_mean_squared_error: 0.1414\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1009 - root_mean_squared_error: 0.1414\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1008 - root_mean_squared_error: 0.1414\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1008 - root_mean_squared_error: 0.1413\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1008 - root_mean_squared_error: 0.1413\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1008 - root_mean_squared_error: 0.1413\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.1007 - root_mean_squared_error: 0.1413\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1007 - root_mean_squared_error: 0.1412\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1007 - root_mean_squared_error: 0.1412\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1006 - root_mean_squared_error: 0.1412\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1006 - root_mean_squared_error: 0.1411\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1006 - root_mean_squared_error: 0.1411\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1006 - root_mean_squared_error: 0.1411\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1005 - root_mean_squared_error: 0.1411\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1005 - root_mean_squared_error: 0.1410\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1005 - root_mean_squared_error: 0.1410\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1005 - root_mean_squared_error: 0.1410\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1004 - root_mean_squared_error: 0.1410\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1004 - root_mean_squared_error: 0.1409\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.1004 - root_mean_squared_error: 0.1409\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1004 - root_mean_squared_error: 0.1409\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1004 - root_mean_squared_error: 0.1409\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1003 - root_mean_squared_error: 0.1408\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1003 - root_mean_squared_error: 0.1408\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1003 - root_mean_squared_error: 0.1408\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1003 - root_mean_squared_error: 0.1408\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1003 - root_mean_squared_error: 0.1407\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1002 - root_mean_squared_error: 0.1407\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1002 - root_mean_squared_error: 0.1407\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1002 - root_mean_squared_error: 0.1407\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1002 - root_mean_squared_error: 0.1407\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1002 - root_mean_squared_error: 0.1406\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1001 - root_mean_squared_error: 0.1406\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1001 - root_mean_squared_error: 0.1406\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1001 - root_mean_squared_error: 0.1406\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1001 - root_mean_squared_error: 0.1406\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.1001 - root_mean_squared_error: 0.1406\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.1000 - root_mean_squared_error: 0.1405\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.1000 - root_mean_squared_error: 0.1405\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.1000 - root_mean_squared_error: 0.1405\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.1000 - root_mean_squared_error: 0.1405\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.1000 - root_mean_squared_error: 0.1405\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.1000 - root_mean_squared_error: 0.1404\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.1000 - root_mean_squared_error: 0.1404\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0999 - root_mean_squared_error: 0.1404\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0999 - root_mean_squared_error: 0.1404\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0999 - root_mean_squared_error: 0.1404\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0999 - root_mean_squared_error: 0.1403\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0999 - root_mean_squared_error: 0.1403\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0999 - root_mean_squared_error: 0.1403\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0999 - root_mean_squared_error: 0.1403\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0998 - root_mean_squared_error: 0.1403\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0998 - root_mean_squared_error: 0.1403\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0998 - root_mean_squared_error: 0.1402\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0998 - root_mean_squared_error: 0.1402\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0998 - root_mean_squared_error: 0.1402\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0998 - root_mean_squared_error: 0.1402\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0998 - root_mean_squared_error: 0.1402\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0998 - root_mean_squared_error: 0.1402\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0997 - root_mean_squared_error: 0.1401\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0997 - root_mean_squared_error: 0.1401\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0997 - root_mean_squared_error: 0.1401\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0997 - root_mean_squared_error: 0.1401\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0997 - root_mean_squared_error: 0.1401\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0997 - root_mean_squared_error: 0.1401\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0997 - root_mean_squared_error: 0.1401\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0997 - root_mean_squared_error: 0.1400\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0997 - root_mean_squared_error: 0.1400\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0997 - root_mean_squared_error: 0.1400\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0997 - root_mean_squared_error: 0.1400\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1400\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1400\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1400\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1400\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1399\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1399\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1399\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1399\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1399\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1399\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1399\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1399\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1399\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1398\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0996 - root_mean_squared_error: 0.1398\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0996 - root_mean_squared_error: 0.1398\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0995 - root_mean_squared_error: 0.1398\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0995 - root_mean_squared_error: 0.1398\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0995 - root_mean_squared_error: 0.1398\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0995 - root_mean_squared_error: 0.1398\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0995 - root_mean_squared_error: 0.1398\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0995 - root_mean_squared_error: 0.1398\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0995 - root_mean_squared_error: 0.1398\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0995 - root_mean_squared_error: 0.1397\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0995 - root_mean_squared_error: 0.1397\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0995 - root_mean_squared_error: 0.1397\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0995 - root_mean_squared_error: 0.1397\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0995 - root_mean_squared_error: 0.1397\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6cc3030d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6cc2803a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "X_train_assembled = np.stack([Y_train_pred_1, Y_train_pred_2], axis=2)\n",
    "Y_train_assembled = Y_train  # output the target of the first model\n",
    "X_test_assembled = np.stack([Y_test_pred_1, Y_test_pred_2], axis=2)\n",
    "Y_test_assembled_real = Y_test_real\n",
    "\n",
    "batch_size_train = len(X_train_assembled)\n",
    "batch_size_test = len(X_test_assembled)\n",
    "\n",
    "model_generator = get_assemble\n",
    "\n",
    "model_train = model_generator(batch_input_shape=(batch_size_train, n_forecast, 2))\n",
    "model_train.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model_train.fit(X_train_assembled, Y_train_assembled, batch_size=batch_size_train, epochs=epochs)\n",
    "Y_train_assembled_pred = model_train.predict(X_train_assembled, batch_size=batch_size_train)\n",
    "\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_test, n_forecast, 2))\n",
    "model_prediction.set_weights(model_train.get_weights())\n",
    "Y_test_assembled_pred = model_prediction.predict(X_test_assembled, batch_size=batch_size_test)\n",
    "# unscale and unpad the data\n",
    "Y_test_assembled_pred_unscaled = dg_2.inverse_transform_y(Y_test_assembled_pred, idx=test_idx, geo=dg.loc_init)\n",
    "Y_test_assembled_pred_real = dg.remove_padded_y(Y_test_assembled_pred_unscaled, idx=test_idx, geo=dg.loc_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factor t+1 = -0.0369\n",
      "factor t+2 = 0.0094\n",
      "factor t+3 = -0.0201\n",
      "factor t+4 = -0.0503\n",
      "factor t+5 = -0.0222\n",
      "factor t+6 = 0.0086\n",
      "factor t+7 = 0.0610\n",
      "factor t+8 = -0.0460\n",
      "factor t+9 = 0.0251\n",
      "factor t+10 = 0.0206\n",
      "factor t+11 = -0.0086\n",
      "factor t+12 = 0.0465\n",
      "factor t+13 = 0.0380\n",
      "factor t+14 = -0.0035\n",
      "factor t+15 = -0.0059\n",
      "factor t+16 = 0.0511\n",
      "factor t+17 = 0.0799\n",
      "factor t+18 = -0.0015\n",
      "factor t+19 = 0.1458\n",
      "factor t+20 = 0.0549\n"
     ]
    }
   ],
   "source": [
    "factors = model_prediction.weights[0].numpy().reshape(n_forecast)\n",
    "for t in range(n_forecast):\n",
    "    print(f'factor t+{t+1} = {factors[t]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE(t+1)</th>\n",
       "      <th>MSE(t+1)</th>\n",
       "      <th>MAE(t+2)</th>\n",
       "      <th>MSE(t+2)</th>\n",
       "      <th>MAE(t+3)</th>\n",
       "      <th>MSE(t+3)</th>\n",
       "      <th>MAE(t+4)</th>\n",
       "      <th>MSE(t+4)</th>\n",
       "      <th>MAE(t+5)</th>\n",
       "      <th>MSE(t+5)</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE(t+16)</th>\n",
       "      <th>MSE(t+16)</th>\n",
       "      <th>MAE(t+17)</th>\n",
       "      <th>MSE(t+17)</th>\n",
       "      <th>MAE(t+18)</th>\n",
       "      <th>MSE(t+18)</th>\n",
       "      <th>MAE(t+19)</th>\n",
       "      <th>MSE(t+19)</th>\n",
       "      <th>MAE(t+20)</th>\n",
       "      <th>MSE(t+20)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model 1</th>\n",
       "      <td>64.776166</td>\n",
       "      <td>8177.744198</td>\n",
       "      <td>85.544188</td>\n",
       "      <td>14000.047469</td>\n",
       "      <td>56.282541</td>\n",
       "      <td>6775.733298</td>\n",
       "      <td>92.281441</td>\n",
       "      <td>16304.983330</td>\n",
       "      <td>73.014681</td>\n",
       "      <td>12421.447863</td>\n",
       "      <td>...</td>\n",
       "      <td>171.245261</td>\n",
       "      <td>68173.437018</td>\n",
       "      <td>189.017537</td>\n",
       "      <td>79470.457678</td>\n",
       "      <td>216.303038</td>\n",
       "      <td>105515.005546</td>\n",
       "      <td>175.501428</td>\n",
       "      <td>73015.696985</td>\n",
       "      <td>199.017932</td>\n",
       "      <td>91250.922392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model trends</th>\n",
       "      <td>500.168623</td>\n",
       "      <td>448179.411747</td>\n",
       "      <td>490.743285</td>\n",
       "      <td>423056.173580</td>\n",
       "      <td>488.237584</td>\n",
       "      <td>428791.133248</td>\n",
       "      <td>481.568161</td>\n",
       "      <td>427597.078281</td>\n",
       "      <td>463.950663</td>\n",
       "      <td>402762.104273</td>\n",
       "      <td>...</td>\n",
       "      <td>462.392660</td>\n",
       "      <td>442370.553714</td>\n",
       "      <td>462.200601</td>\n",
       "      <td>440222.909615</td>\n",
       "      <td>478.446685</td>\n",
       "      <td>497911.997312</td>\n",
       "      <td>467.308769</td>\n",
       "      <td>461839.989367</td>\n",
       "      <td>468.023170</td>\n",
       "      <td>465973.809583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assembled</th>\n",
       "      <td>53.343115</td>\n",
       "      <td>5873.652453</td>\n",
       "      <td>88.448106</td>\n",
       "      <td>14906.556168</td>\n",
       "      <td>51.802158</td>\n",
       "      <td>5967.674274</td>\n",
       "      <td>78.317268</td>\n",
       "      <td>11706.270929</td>\n",
       "      <td>68.515821</td>\n",
       "      <td>11198.143498</td>\n",
       "      <td>...</td>\n",
       "      <td>181.193902</td>\n",
       "      <td>76003.897253</td>\n",
       "      <td>203.806071</td>\n",
       "      <td>91230.733219</td>\n",
       "      <td>216.146575</td>\n",
       "      <td>105285.687699</td>\n",
       "      <td>203.392486</td>\n",
       "      <td>94678.490116</td>\n",
       "      <td>208.180587</td>\n",
       "      <td>99320.488036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                MAE(t+1)       MSE(t+1)    MAE(t+2)       MSE(t+2)  \\\n",
       "model                                                                \n",
       "model 1        64.776166    8177.744198   85.544188   14000.047469   \n",
       "model trends  500.168623  448179.411747  490.743285  423056.173580   \n",
       "assembled      53.343115    5873.652453   88.448106   14906.556168   \n",
       "\n",
       "                MAE(t+3)       MSE(t+3)    MAE(t+4)       MSE(t+4)  \\\n",
       "model                                                                \n",
       "model 1        56.282541    6775.733298   92.281441   16304.983330   \n",
       "model trends  488.237584  428791.133248  481.568161  427597.078281   \n",
       "assembled      51.802158    5967.674274   78.317268   11706.270929   \n",
       "\n",
       "                MAE(t+5)       MSE(t+5)  ...   MAE(t+16)      MSE(t+16)  \\\n",
       "model                                    ...                              \n",
       "model 1        73.014681   12421.447863  ...  171.245261   68173.437018   \n",
       "model trends  463.950663  402762.104273  ...  462.392660  442370.553714   \n",
       "assembled      68.515821   11198.143498  ...  181.193902   76003.897253   \n",
       "\n",
       "               MAE(t+17)      MSE(t+17)   MAE(t+18)      MSE(t+18)  \\\n",
       "model                                                                \n",
       "model 1       189.017537   79470.457678  216.303038  105515.005546   \n",
       "model trends  462.200601  440222.909615  478.446685  497911.997312   \n",
       "assembled     203.806071   91230.733219  216.146575  105285.687699   \n",
       "\n",
       "               MAE(t+19)      MSE(t+19)   MAE(t+20)      MSE(t+20)  \n",
       "model                                                               \n",
       "model 1       175.501428   73015.696985  199.017932   91250.922392  \n",
       "model trends  467.308769  461839.989367  468.023170  465973.809583  \n",
       "assembled     203.392486   94678.490116  208.180587   99320.488036  \n",
       "\n",
       "[3 rows x 40 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_1 = compute_error(Y_test_pred_real_1, Y_test_real)\n",
    "error_1['model'] = 'model 1'\n",
    "error_2 = compute_error(Y_test_pred_real_2, Y_test_real_2)\n",
    "error_2['model'] = 'model trends'\n",
    "error_assembled = compute_error(Y_test_assembled_pred_real, Y_test_assembled_real)\n",
    "error_assembled['model'] = 'assembled'\n",
    "pd.concat([error_1, error_2, error_assembled]).set_index('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_assembly(model_generator_1: callable, dg_1: util.DataGenerator,\n",
    "                          model_generator_2: callable, dg_2: util.DataGenerator,\n",
    "                          model_generator_as: callable, \n",
    "                          nb_fit_first: int, nb_valid: int, nb_test: int, verbose: int = 1,\n",
    "                          return_history: bool = False, batch_size_fun: callable = None,\n",
    "                          es_stop_val: bool = False, epochs: int = 200) -> Union[pd.DataFrame, List[History]]:\n",
    "    \"\"\"\n",
    "    evaluates a model using a walk forward evaluation: multiples fit are done, each followed by at most nb_test\n",
    "    to evaluate the model\n",
    "    :param model_generator_1: function returning the first model to evaluate\n",
    "    :param target_1: target for the first model\n",
    "    :param data_columns_1: data columns for the first model\n",
    "    :param model_generator_2: function returning the second model to evaluate\n",
    "    :param target_2: target for the second model\n",
    "    :param data_columns_2: data columns for the second model\n",
    "    :param model_generator_as: function returning the assembly model\n",
    "    :param nb_fit_first: number of datapoints used for the first fit\n",
    "    :param nb_valid: number of datapoints used for each validation set\n",
    "    :param nb_test: number of datapoints used for the test set (at most)\n",
    "    :param epochs: number of epochs used on each fit\n",
    "    :param verbose: verbose level. Passed to fit and used to display the error dataframe\n",
    "    :param return_history: if True, returns the list of history of each walk\n",
    "    :param batch_size_fun: function used to compute the batch size based on the X_train tensor\n",
    "        if not specified, default to batch_size = len(train_idx)\n",
    "    \"\"\"\n",
    "    # initial index used\n",
    "    max_len = dg.batch_size\n",
    "    train_idx = np.arange(nb_fit_first)\n",
    "    valid_idx = np.arange(nb_fit_first, nb_fit_first + nb_valid)\n",
    "    if nb_fit_first + nb_valid >= max_len:\n",
    "        finished = True  # no test can be done\n",
    "    else:\n",
    "        finished = False  # a test set can be created\n",
    "        test_idx = np.arange(nb_fit_first + nb_valid, min(nb_fit_first + nb_valid + nb_test, max_len))\n",
    "    df_error = pd.DataFrame()\n",
    "    walk = 0\n",
    "    last_iter = False  # True when the last iteration is reached\n",
    "    all_history = []\n",
    "    \n",
    "    list_dg = [dg_1, dg_2]\n",
    "    \n",
    "    while not finished:\n",
    "        X_train, Y_train, pred_train = [], [], []\n",
    "        X_val, Y_val_unscaled, Y_val_real = [], [], []\n",
    "        X_test, Y_test_unscaled, Y_test_real = [], [], []\n",
    "        pred_test, pred_test_unscaled, pred_test_real = [], [], []\n",
    "        \n",
    "        # train the two first models\n",
    "        for i, model_generator in enumerate([model_generator_1, model_generator_2]):\n",
    "            dg_i = list_dg[i]\n",
    "            X_train.append(dg_i.get_x(train_idx, scaled=True))\n",
    "            Y_train.append(dg_i.get_y(train_idx, scaled=True))\n",
    "\n",
    "            if len(valid_idx) > 0:  # validation set: validation logger and early stop if needed\n",
    "                X_val.append(dg_i.get_x(valid_idx, geo=dg_i.loc_init, scaled=True, use_previous_scaler=True))\n",
    "                Y_val_unscaled.append(dg_i.get_y(valid_idx, geo=dg_i.loc_init, scaled=False))\n",
    "                Y_val_real.append(dg_i.remove_padded_y(Y_val_unscaled[-1], idx=valid_idx, geo=dg_i.loc_init))\n",
    "                batch_size_val = len(X_val[-1])\n",
    "                model_validation = model_generator(batch_input_shape=(batch_size_val, n_samples, dg_i.n_features))\n",
    "                val_log = ValidationLogger(model_validation, batch_size_val, X_val[-1], Y_val_real[-1], \n",
    "                                             valid_idx, dg_i.loc_init)\n",
    "                if es_stop_val:\n",
    "                    callbacks = [val_log, EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=25)]\n",
    "                else:\n",
    "                    callbacks = [val_log]\n",
    "            else:\n",
    "                callbacks = None            \n",
    "\n",
    "            # test set\n",
    "            X_test.append(dg_i.get_x(test_idx, scaled=True, geo=dg_i.loc_init, use_previous_scaler=True))\n",
    "            Y_test_unscaled.append(dg_i.get_y(test_idx, scaled=False, geo=dg_i.loc_init))\n",
    "            Y_test_real.append(dg_i.remove_padded_y(Y_test_unscaled[-1], idx=test_idx, geo=dg_i.loc_init))\n",
    "\n",
    "            batch_size_train = len(X_train[-1])\n",
    "            batch_size_test = len(X_test[-1])\n",
    "\n",
    "            # training of model 1\n",
    "            model_train = model_generator(batch_input_shape=(batch_size_train, dg_i.n_samples, dg_i.n_features))\n",
    "            history = model_train.fit(X_train[-1], Y_train[-1], batch_size=batch_size_train, epochs=epochs, \n",
    "                                      callbacks=callbacks, verbose=verbose)\n",
    "            # prediction on the training set\n",
    "            pred_train.append(model_train.predict(X_train[-1], batch_size=batch_size_train))\n",
    "            # prediction on the test set\n",
    "            model_prediction = model_generator(batch_input_shape=(batch_size_test, n_samples, dg_i.n_features))\n",
    "            model_prediction.set_weights(model_train.get_weights())\n",
    "            pred_test.append(model_prediction.predict(X_test[-1], batch_size=batch_size_test))\n",
    "            pred_test_unscaled.append(dg_i.inverse_transform_y(pred_test[-1], idx=test_idx, geo=dg_i.loc_init))\n",
    "            pred_test_real.append(dg_i.remove_padded_y(pred_test_unscaled[-1], idx=test_idx, geo=dg_i.loc_init))\n",
    "        \n",
    "        # train the assembly model\n",
    "        X_train_as = np.stack(pred_train, axis=2)\n",
    "        Y_train_as = Y_train[0]  # output the target of the first model\n",
    "        X_test_as = np.stack(pred_test, axis=2)\n",
    "        Y_test_as_real = Y_test_real[0]\n",
    "\n",
    "        batch_size_train = len(X_train_as)\n",
    "        batch_size_test = len(X_test_as)\n",
    "        dg_as = list_dg[0]  # used for inverse scaling and unpadding\n",
    "\n",
    "        model_train = model_generator_as(batch_input_shape=(batch_size_train, n_forecast, 2))\n",
    "        model_train.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                              metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "        history = model_train.fit(X_train_as, Y_train_as, batch_size=batch_size_train, epochs=epochs, \n",
    "                                  verbose=verbose)\n",
    "        pred_train_as = model_train.predict(X_train_as, batch_size=batch_size_train)\n",
    "\n",
    "        model_prediction = model_generator_as(batch_input_shape=(batch_size_test, n_forecast, 2))\n",
    "        model_prediction.set_weights(model_train.get_weights())\n",
    "        pred_test_as = model_prediction.predict(X_test_as, batch_size=batch_size_test)\n",
    "        # unscale and unpad the data\n",
    "        pred_test_as_unscaled = dg_as.inverse_transform_y(pred_test_as, idx=test_idx, geo=dg_as.loc_init)\n",
    "        pred_test_as_real = dg_as.remove_padded_y(pred_test_as_unscaled, idx=test_idx, geo=dg_as.loc_init)\n",
    "        \n",
    "        \"\"\"\n",
    "        if return_history:\n",
    "            # add test metrics to the history for this walk, based on the unpadded unscaled data \n",
    "            # except for the loss\n",
    "            for metric in model_train.metrics:\n",
    "                metric.reset_states()\n",
    "                if metric.name == 'loss':\n",
    "                    metric.update_state(Y_predicted_real, Y_test_real)\n",
    "                else:\n",
    "                    # compute metric accross each horizon\n",
    "                    for i in range(n_forecast):\n",
    "                        metric.update_state(Y_predicted_real[:, i], Y_test_real[:, i])\n",
    "                        history.history[f\"test_{metric.name}(t+{i+1})\"] = [metric.result().numpy()]\n",
    "                        metric.reset_states()\n",
    "                    # compute mean of metric on all horizon\n",
    "                    metric.update_state(Y_predicted_real, Y_test_real)\n",
    "                # prepend name for test set\n",
    "                history.history[f\"test_{metric.name}\"] = [metric.result().numpy()]\n",
    "            # add number of unpadded datapoints\n",
    "            history.history['nb_test_datapoints'] = [len(Y_test_real)]\n",
    "            all_history.append(history)\n",
    "        \"\"\"\n",
    "        if not return_history or verbose != 0:\n",
    "            # compute the error using the unpadded and unscaled data\n",
    "            for i in range(3):\n",
    "                if i == 2:  # assembled model\n",
    "                    error = compute_error(Y_test_as_real, pred_test_as_real)\n",
    "                    name = f'walk {walk + 1} model assembled'\n",
    "                    error['nb_test_datapoints'] = len(Y_test_as_real)\n",
    "                else:  # two first model\n",
    "                    error = compute_error(Y_test_real[i], pred_test_real[i])\n",
    "                    name = f'walk {walk + 1} model {i+1}'\n",
    "                    error['nb_test_datapoints'] = len(Y_test_real[i])\n",
    "                error['name'] = name\n",
    "                error['days_train'] = len(train_idx)\n",
    "                error['days_valid'] = len(valid_idx)\n",
    "                error['days_test'] = len(test_idx)\n",
    "                error = error.set_index('name')\n",
    "                df_error = df_error.append(error)\n",
    "        \n",
    "        if verbose != 0:\n",
    "            display(df_error)\n",
    "        if last_iter:\n",
    "            finished = True\n",
    "        # indexes for next fit\n",
    "        train_idx = np.arange(train_idx[-1] + 1 + nb_test)\n",
    "        valid_idx += nb_test\n",
    "        if test_idx[-1] + nb_test >= max_len:  # last iteration, less points can be used for the test set\n",
    "            last_iter = True  # last iteration to be done\n",
    "            test_idx = np.arange(test_idx[-1], max_len)\n",
    "        else:\n",
    "            test_idx += nb_test\n",
    "        walk += 1\n",
    "    if not return_history:  # compute the mean across all walks\n",
    "        len_df = len(df_error)\n",
    "        df_error.loc['mean model 1'] = df_error.iloc[range(0, len_df, 3)].mean()\n",
    "        df_error.loc['mean model 2'] = df_error.iloc[range(1, len_df, 3)].mean()\n",
    "        df_error.loc['mean model as'] = df_error.iloc[range(2, len_df, 3)].mean()\n",
    "    return df_error if not return_history else all_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6ac72cee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6481faaf0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6ac78e670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6484dac10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6484da790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6ac4f8c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6ac4f89d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6ac0df430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff648238790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff71c15ca60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6ac51ea60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6ac72cee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff67066aca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff670236a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff66871f790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff67064f280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6704aa700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff648435040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff64869b3a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff64869fee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6ac1a1040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6ac0df310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff64869b040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6687c5940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff66815b040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6cc066c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff670177040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6ac6f38b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff64827a310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff6ac3ea310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "df_error = walk_forward_assembly(get_dense_model, dg, get_encoder_decoder, dg_2, get_assemble,\n",
    "                     220, 30, 30, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE(t+1)</th>\n",
       "      <th>MSE(t+1)</th>\n",
       "      <th>MAE(t+2)</th>\n",
       "      <th>MSE(t+2)</th>\n",
       "      <th>MAE(t+3)</th>\n",
       "      <th>MSE(t+3)</th>\n",
       "      <th>MAE(t+4)</th>\n",
       "      <th>MSE(t+4)</th>\n",
       "      <th>MAE(t+5)</th>\n",
       "      <th>MSE(t+5)</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE(t+18)</th>\n",
       "      <th>MSE(t+18)</th>\n",
       "      <th>MAE(t+19)</th>\n",
       "      <th>MSE(t+19)</th>\n",
       "      <th>MAE(t+20)</th>\n",
       "      <th>MSE(t+20)</th>\n",
       "      <th>nb_test_datapoints</th>\n",
       "      <th>days_train</th>\n",
       "      <th>days_valid</th>\n",
       "      <th>days_test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>walk 1 model 1</th>\n",
       "      <td>84.359519</td>\n",
       "      <td>17172.500323</td>\n",
       "      <td>89.813538</td>\n",
       "      <td>2.042219e+04</td>\n",
       "      <td>112.391106</td>\n",
       "      <td>3.493799e+04</td>\n",
       "      <td>133.709972</td>\n",
       "      <td>5.574936e+04</td>\n",
       "      <td>162.788078</td>\n",
       "      <td>8.056094e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>274.352663</td>\n",
       "      <td>2.349093e+05</td>\n",
       "      <td>266.400583</td>\n",
       "      <td>2.063037e+05</td>\n",
       "      <td>265.222337</td>\n",
       "      <td>1.836229e+05</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 1 model 2</th>\n",
       "      <td>450.600920</td>\n",
       "      <td>364699.165079</td>\n",
       "      <td>421.811967</td>\n",
       "      <td>3.146787e+05</td>\n",
       "      <td>432.646109</td>\n",
       "      <td>3.303739e+05</td>\n",
       "      <td>437.511392</td>\n",
       "      <td>3.377634e+05</td>\n",
       "      <td>427.426859</td>\n",
       "      <td>3.221477e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>402.364421</td>\n",
       "      <td>4.988358e+05</td>\n",
       "      <td>405.601952</td>\n",
       "      <td>5.181328e+05</td>\n",
       "      <td>408.784488</td>\n",
       "      <td>5.365369e+05</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 1 model assembled</th>\n",
       "      <td>80.229933</td>\n",
       "      <td>15405.056767</td>\n",
       "      <td>94.533843</td>\n",
       "      <td>2.279574e+04</td>\n",
       "      <td>105.774797</td>\n",
       "      <td>3.109170e+04</td>\n",
       "      <td>136.053394</td>\n",
       "      <td>5.784007e+04</td>\n",
       "      <td>163.746402</td>\n",
       "      <td>8.160400e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>245.472736</td>\n",
       "      <td>1.945924e+05</td>\n",
       "      <td>266.583363</td>\n",
       "      <td>2.066119e+05</td>\n",
       "      <td>235.938544</td>\n",
       "      <td>1.426686e+05</td>\n",
       "      <td>690.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model 1</th>\n",
       "      <td>24.286091</td>\n",
       "      <td>1180.521232</td>\n",
       "      <td>157.300306</td>\n",
       "      <td>4.594330e+04</td>\n",
       "      <td>135.788077</td>\n",
       "      <td>3.504677e+04</td>\n",
       "      <td>117.818483</td>\n",
       "      <td>2.704263e+04</td>\n",
       "      <td>208.914730</td>\n",
       "      <td>8.280504e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>268.098850</td>\n",
       "      <td>1.077698e+05</td>\n",
       "      <td>222.236844</td>\n",
       "      <td>7.223126e+04</td>\n",
       "      <td>303.666583</td>\n",
       "      <td>1.367235e+05</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model 2</th>\n",
       "      <td>738.806763</td>\n",
       "      <td>973716.137758</td>\n",
       "      <td>765.629937</td>\n",
       "      <td>1.080124e+06</td>\n",
       "      <td>813.159996</td>\n",
       "      <td>1.245339e+06</td>\n",
       "      <td>842.099406</td>\n",
       "      <td>1.348647e+06</td>\n",
       "      <td>852.332266</td>\n",
       "      <td>1.382890e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>826.388212</td>\n",
       "      <td>1.252091e+06</td>\n",
       "      <td>829.324643</td>\n",
       "      <td>1.258092e+06</td>\n",
       "      <td>832.707795</td>\n",
       "      <td>1.265583e+06</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 2 model assembled</th>\n",
       "      <td>23.795614</td>\n",
       "      <td>1112.771730</td>\n",
       "      <td>150.065805</td>\n",
       "      <td>4.303688e+04</td>\n",
       "      <td>144.220197</td>\n",
       "      <td>3.821023e+04</td>\n",
       "      <td>101.586242</td>\n",
       "      <td>2.249797e+04</td>\n",
       "      <td>201.672379</td>\n",
       "      <td>7.903109e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>284.723816</td>\n",
       "      <td>1.193197e+05</td>\n",
       "      <td>333.880978</td>\n",
       "      <td>1.605228e+05</td>\n",
       "      <td>393.198593</td>\n",
       "      <td>2.321580e+05</td>\n",
       "      <td>690.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model 1</th>\n",
       "      <td>39.497256</td>\n",
       "      <td>3147.437893</td>\n",
       "      <td>49.557288</td>\n",
       "      <td>4.749236e+03</td>\n",
       "      <td>45.528840</td>\n",
       "      <td>4.309270e+03</td>\n",
       "      <td>61.411482</td>\n",
       "      <td>7.343018e+03</td>\n",
       "      <td>80.037802</td>\n",
       "      <td>1.160291e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>173.858495</td>\n",
       "      <td>5.855631e+04</td>\n",
       "      <td>194.819921</td>\n",
       "      <td>7.737171e+04</td>\n",
       "      <td>164.127304</td>\n",
       "      <td>5.723943e+04</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model 2</th>\n",
       "      <td>522.503503</td>\n",
       "      <td>371687.023222</td>\n",
       "      <td>432.838671</td>\n",
       "      <td>2.343321e+05</td>\n",
       "      <td>470.792789</td>\n",
       "      <td>2.835692e+05</td>\n",
       "      <td>516.808059</td>\n",
       "      <td>3.666708e+05</td>\n",
       "      <td>549.726193</td>\n",
       "      <td>4.306439e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>483.482697</td>\n",
       "      <td>3.367390e+05</td>\n",
       "      <td>481.181985</td>\n",
       "      <td>3.356021e+05</td>\n",
       "      <td>478.756484</td>\n",
       "      <td>3.342417e+05</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 3 model assembled</th>\n",
       "      <td>43.757012</td>\n",
       "      <td>3658.677410</td>\n",
       "      <td>46.480373</td>\n",
       "      <td>4.290812e+03</td>\n",
       "      <td>46.670262</td>\n",
       "      <td>4.463448e+03</td>\n",
       "      <td>50.326971</td>\n",
       "      <td>5.685681e+03</td>\n",
       "      <td>65.208359</td>\n",
       "      <td>8.302015e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>171.004556</td>\n",
       "      <td>5.745480e+04</td>\n",
       "      <td>194.424415</td>\n",
       "      <td>7.722490e+04</td>\n",
       "      <td>183.711934</td>\n",
       "      <td>6.556798e+04</td>\n",
       "      <td>690.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model 1</th>\n",
       "      <td>36.739895</td>\n",
       "      <td>3387.384925</td>\n",
       "      <td>33.365111</td>\n",
       "      <td>2.955587e+03</td>\n",
       "      <td>41.010548</td>\n",
       "      <td>6.359050e+03</td>\n",
       "      <td>64.221399</td>\n",
       "      <td>1.590894e+04</td>\n",
       "      <td>39.234208</td>\n",
       "      <td>4.987648e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>124.103556</td>\n",
       "      <td>3.929532e+04</td>\n",
       "      <td>151.345004</td>\n",
       "      <td>6.466431e+04</td>\n",
       "      <td>144.275666</td>\n",
       "      <td>6.026965e+04</td>\n",
       "      <td>690.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model 2</th>\n",
       "      <td>373.275917</td>\n",
       "      <td>275245.848825</td>\n",
       "      <td>354.703982</td>\n",
       "      <td>3.122792e+05</td>\n",
       "      <td>354.586685</td>\n",
       "      <td>2.765463e+05</td>\n",
       "      <td>359.399973</td>\n",
       "      <td>2.582068e+05</td>\n",
       "      <td>360.658219</td>\n",
       "      <td>2.523178e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>327.340121</td>\n",
       "      <td>2.316771e+05</td>\n",
       "      <td>327.331349</td>\n",
       "      <td>2.319820e+05</td>\n",
       "      <td>327.564582</td>\n",
       "      <td>2.327231e+05</td>\n",
       "      <td>690.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 4 model assembled</th>\n",
       "      <td>34.633537</td>\n",
       "      <td>3653.379143</td>\n",
       "      <td>32.701765</td>\n",
       "      <td>3.498612e+03</td>\n",
       "      <td>43.146211</td>\n",
       "      <td>6.622421e+03</td>\n",
       "      <td>65.530052</td>\n",
       "      <td>1.636344e+04</td>\n",
       "      <td>39.887588</td>\n",
       "      <td>5.046265e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>125.429629</td>\n",
       "      <td>3.937316e+04</td>\n",
       "      <td>150.310254</td>\n",
       "      <td>6.477976e+04</td>\n",
       "      <td>141.811642</td>\n",
       "      <td>6.000233e+04</td>\n",
       "      <td>690.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model 1</th>\n",
       "      <td>38.418268</td>\n",
       "      <td>3675.910655</td>\n",
       "      <td>29.067674</td>\n",
       "      <td>2.156781e+03</td>\n",
       "      <td>100.748479</td>\n",
       "      <td>2.130926e+04</td>\n",
       "      <td>80.645316</td>\n",
       "      <td>1.565063e+04</td>\n",
       "      <td>113.244598</td>\n",
       "      <td>2.869968e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>213.434181</td>\n",
       "      <td>1.049598e+05</td>\n",
       "      <td>223.626291</td>\n",
       "      <td>1.153328e+05</td>\n",
       "      <td>298.875471</td>\n",
       "      <td>2.094742e+05</td>\n",
       "      <td>322.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model 2</th>\n",
       "      <td>289.514663</td>\n",
       "      <td>196659.313843</td>\n",
       "      <td>255.907412</td>\n",
       "      <td>1.620347e+05</td>\n",
       "      <td>288.362672</td>\n",
       "      <td>2.130117e+05</td>\n",
       "      <td>315.252735</td>\n",
       "      <td>2.934433e+05</td>\n",
       "      <td>322.333204</td>\n",
       "      <td>3.481783e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>357.218003</td>\n",
       "      <td>5.580328e+05</td>\n",
       "      <td>363.925211</td>\n",
       "      <td>5.830801e+05</td>\n",
       "      <td>371.095001</td>\n",
       "      <td>6.077170e+05</td>\n",
       "      <td>322.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>walk 5 model assembled</th>\n",
       "      <td>39.269023</td>\n",
       "      <td>4009.160708</td>\n",
       "      <td>29.785831</td>\n",
       "      <td>2.379492e+03</td>\n",
       "      <td>98.164478</td>\n",
       "      <td>1.940103e+04</td>\n",
       "      <td>77.962426</td>\n",
       "      <td>1.429667e+04</td>\n",
       "      <td>108.617104</td>\n",
       "      <td>2.451976e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>229.966060</td>\n",
       "      <td>1.449182e+05</td>\n",
       "      <td>237.813177</td>\n",
       "      <td>1.513261e+05</td>\n",
       "      <td>304.321189</td>\n",
       "      <td>2.581150e+05</td>\n",
       "      <td>322.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model 1</th>\n",
       "      <td>44.660206</td>\n",
       "      <td>5712.751006</td>\n",
       "      <td>71.820783</td>\n",
       "      <td>1.524542e+04</td>\n",
       "      <td>87.093410</td>\n",
       "      <td>2.039247e+04</td>\n",
       "      <td>91.561331</td>\n",
       "      <td>2.433892e+04</td>\n",
       "      <td>120.843883</td>\n",
       "      <td>4.173124e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>210.769549</td>\n",
       "      <td>1.090981e+05</td>\n",
       "      <td>211.685729</td>\n",
       "      <td>1.071807e+05</td>\n",
       "      <td>235.233472</td>\n",
       "      <td>1.294659e+05</td>\n",
       "      <td>616.4</td>\n",
       "      <td>310.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model 2</th>\n",
       "      <td>474.940353</td>\n",
       "      <td>436401.497745</td>\n",
       "      <td>446.178394</td>\n",
       "      <td>4.206898e+05</td>\n",
       "      <td>471.909650</td>\n",
       "      <td>4.697681e+05</td>\n",
       "      <td>494.214313</td>\n",
       "      <td>5.209463e+05</td>\n",
       "      <td>502.495348</td>\n",
       "      <td>5.472355e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>479.358691</td>\n",
       "      <td>5.754751e+05</td>\n",
       "      <td>481.473028</td>\n",
       "      <td>5.853778e+05</td>\n",
       "      <td>483.781670</td>\n",
       "      <td>5.953603e+05</td>\n",
       "      <td>616.4</td>\n",
       "      <td>310.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean model as</th>\n",
       "      <td>44.337024</td>\n",
       "      <td>5567.809151</td>\n",
       "      <td>70.713523</td>\n",
       "      <td>1.520031e+04</td>\n",
       "      <td>87.595189</td>\n",
       "      <td>1.995777e+04</td>\n",
       "      <td>86.291817</td>\n",
       "      <td>2.333677e+04</td>\n",
       "      <td>115.826366</td>\n",
       "      <td>3.970063e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>211.319360</td>\n",
       "      <td>1.111317e+05</td>\n",
       "      <td>236.602437</td>\n",
       "      <td>1.320931e+05</td>\n",
       "      <td>251.796380</td>\n",
       "      <td>1.517024e+05</td>\n",
       "      <td>616.4</td>\n",
       "      <td>310.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          MAE(t+1)       MSE(t+1)    MAE(t+2)      MSE(t+2)  \\\n",
       "name                                                                          \n",
       "walk 1 model 1           84.359519   17172.500323   89.813538  2.042219e+04   \n",
       "walk 1 model 2          450.600920  364699.165079  421.811967  3.146787e+05   \n",
       "walk 1 model assembled   80.229933   15405.056767   94.533843  2.279574e+04   \n",
       "walk 2 model 1           24.286091    1180.521232  157.300306  4.594330e+04   \n",
       "walk 2 model 2          738.806763  973716.137758  765.629937  1.080124e+06   \n",
       "walk 2 model assembled   23.795614    1112.771730  150.065805  4.303688e+04   \n",
       "walk 3 model 1           39.497256    3147.437893   49.557288  4.749236e+03   \n",
       "walk 3 model 2          522.503503  371687.023222  432.838671  2.343321e+05   \n",
       "walk 3 model assembled   43.757012    3658.677410   46.480373  4.290812e+03   \n",
       "walk 4 model 1           36.739895    3387.384925   33.365111  2.955587e+03   \n",
       "walk 4 model 2          373.275917  275245.848825  354.703982  3.122792e+05   \n",
       "walk 4 model assembled   34.633537    3653.379143   32.701765  3.498612e+03   \n",
       "walk 5 model 1           38.418268    3675.910655   29.067674  2.156781e+03   \n",
       "walk 5 model 2          289.514663  196659.313843  255.907412  1.620347e+05   \n",
       "walk 5 model assembled   39.269023    4009.160708   29.785831  2.379492e+03   \n",
       "mean model 1             44.660206    5712.751006   71.820783  1.524542e+04   \n",
       "mean model 2            474.940353  436401.497745  446.178394  4.206898e+05   \n",
       "mean model as            44.337024    5567.809151   70.713523  1.520031e+04   \n",
       "\n",
       "                          MAE(t+3)      MSE(t+3)    MAE(t+4)      MSE(t+4)  \\\n",
       "name                                                                         \n",
       "walk 1 model 1          112.391106  3.493799e+04  133.709972  5.574936e+04   \n",
       "walk 1 model 2          432.646109  3.303739e+05  437.511392  3.377634e+05   \n",
       "walk 1 model assembled  105.774797  3.109170e+04  136.053394  5.784007e+04   \n",
       "walk 2 model 1          135.788077  3.504677e+04  117.818483  2.704263e+04   \n",
       "walk 2 model 2          813.159996  1.245339e+06  842.099406  1.348647e+06   \n",
       "walk 2 model assembled  144.220197  3.821023e+04  101.586242  2.249797e+04   \n",
       "walk 3 model 1           45.528840  4.309270e+03   61.411482  7.343018e+03   \n",
       "walk 3 model 2          470.792789  2.835692e+05  516.808059  3.666708e+05   \n",
       "walk 3 model assembled   46.670262  4.463448e+03   50.326971  5.685681e+03   \n",
       "walk 4 model 1           41.010548  6.359050e+03   64.221399  1.590894e+04   \n",
       "walk 4 model 2          354.586685  2.765463e+05  359.399973  2.582068e+05   \n",
       "walk 4 model assembled   43.146211  6.622421e+03   65.530052  1.636344e+04   \n",
       "walk 5 model 1          100.748479  2.130926e+04   80.645316  1.565063e+04   \n",
       "walk 5 model 2          288.362672  2.130117e+05  315.252735  2.934433e+05   \n",
       "walk 5 model assembled   98.164478  1.940103e+04   77.962426  1.429667e+04   \n",
       "mean model 1             87.093410  2.039247e+04   91.561331  2.433892e+04   \n",
       "mean model 2            471.909650  4.697681e+05  494.214313  5.209463e+05   \n",
       "mean model as            87.595189  1.995777e+04   86.291817  2.333677e+04   \n",
       "\n",
       "                          MAE(t+5)      MSE(t+5)  ...   MAE(t+18)  \\\n",
       "name                                              ...               \n",
       "walk 1 model 1          162.788078  8.056094e+04  ...  274.352663   \n",
       "walk 1 model 2          427.426859  3.221477e+05  ...  402.364421   \n",
       "walk 1 model assembled  163.746402  8.160400e+04  ...  245.472736   \n",
       "walk 2 model 1          208.914730  8.280504e+04  ...  268.098850   \n",
       "walk 2 model 2          852.332266  1.382890e+06  ...  826.388212   \n",
       "walk 2 model assembled  201.672379  7.903109e+04  ...  284.723816   \n",
       "walk 3 model 1           80.037802  1.160291e+04  ...  173.858495   \n",
       "walk 3 model 2          549.726193  4.306439e+05  ...  483.482697   \n",
       "walk 3 model assembled   65.208359  8.302015e+03  ...  171.004556   \n",
       "walk 4 model 1           39.234208  4.987648e+03  ...  124.103556   \n",
       "walk 4 model 2          360.658219  2.523178e+05  ...  327.340121   \n",
       "walk 4 model assembled   39.887588  5.046265e+03  ...  125.429629   \n",
       "walk 5 model 1          113.244598  2.869968e+04  ...  213.434181   \n",
       "walk 5 model 2          322.333204  3.481783e+05  ...  357.218003   \n",
       "walk 5 model assembled  108.617104  2.451976e+04  ...  229.966060   \n",
       "mean model 1            120.843883  4.173124e+04  ...  210.769549   \n",
       "mean model 2            502.495348  5.472355e+05  ...  479.358691   \n",
       "mean model as           115.826366  3.970063e+04  ...  211.319360   \n",
       "\n",
       "                           MSE(t+18)   MAE(t+19)     MSE(t+19)   MAE(t+20)  \\\n",
       "name                                                                         \n",
       "walk 1 model 1          2.349093e+05  266.400583  2.063037e+05  265.222337   \n",
       "walk 1 model 2          4.988358e+05  405.601952  5.181328e+05  408.784488   \n",
       "walk 1 model assembled  1.945924e+05  266.583363  2.066119e+05  235.938544   \n",
       "walk 2 model 1          1.077698e+05  222.236844  7.223126e+04  303.666583   \n",
       "walk 2 model 2          1.252091e+06  829.324643  1.258092e+06  832.707795   \n",
       "walk 2 model assembled  1.193197e+05  333.880978  1.605228e+05  393.198593   \n",
       "walk 3 model 1          5.855631e+04  194.819921  7.737171e+04  164.127304   \n",
       "walk 3 model 2          3.367390e+05  481.181985  3.356021e+05  478.756484   \n",
       "walk 3 model assembled  5.745480e+04  194.424415  7.722490e+04  183.711934   \n",
       "walk 4 model 1          3.929532e+04  151.345004  6.466431e+04  144.275666   \n",
       "walk 4 model 2          2.316771e+05  327.331349  2.319820e+05  327.564582   \n",
       "walk 4 model assembled  3.937316e+04  150.310254  6.477976e+04  141.811642   \n",
       "walk 5 model 1          1.049598e+05  223.626291  1.153328e+05  298.875471   \n",
       "walk 5 model 2          5.580328e+05  363.925211  5.830801e+05  371.095001   \n",
       "walk 5 model assembled  1.449182e+05  237.813177  1.513261e+05  304.321189   \n",
       "mean model 1            1.090981e+05  211.685729  1.071807e+05  235.233472   \n",
       "mean model 2            5.754751e+05  481.473028  5.853778e+05  483.781670   \n",
       "mean model as           1.111317e+05  236.602437  1.320931e+05  251.796380   \n",
       "\n",
       "                           MSE(t+20)  nb_test_datapoints  days_train  \\\n",
       "name                                                                   \n",
       "walk 1 model 1          1.836229e+05               690.0       250.0   \n",
       "walk 1 model 2          5.365369e+05               690.0       250.0   \n",
       "walk 1 model assembled  1.426686e+05               690.0       250.0   \n",
       "walk 2 model 1          1.367235e+05               690.0       280.0   \n",
       "walk 2 model 2          1.265583e+06               690.0       280.0   \n",
       "walk 2 model assembled  2.321580e+05               690.0       280.0   \n",
       "walk 3 model 1          5.723943e+04               690.0       310.0   \n",
       "walk 3 model 2          3.342417e+05               690.0       310.0   \n",
       "walk 3 model assembled  6.556798e+04               690.0       310.0   \n",
       "walk 4 model 1          6.026965e+04               690.0       340.0   \n",
       "walk 4 model 2          2.327231e+05               690.0       340.0   \n",
       "walk 4 model assembled  6.000233e+04               690.0       340.0   \n",
       "walk 5 model 1          2.094742e+05               322.0       370.0   \n",
       "walk 5 model 2          6.077170e+05               322.0       370.0   \n",
       "walk 5 model assembled  2.581150e+05               322.0       370.0   \n",
       "mean model 1            1.294659e+05               616.4       310.0   \n",
       "mean model 2            5.953603e+05               616.4       310.0   \n",
       "mean model as           1.517024e+05               616.4       310.0   \n",
       "\n",
       "                        days_valid  days_test  \n",
       "name                                           \n",
       "walk 1 model 1                 0.0       30.0  \n",
       "walk 1 model 2                 0.0       30.0  \n",
       "walk 1 model assembled         0.0       30.0  \n",
       "walk 2 model 1                 0.0       30.0  \n",
       "walk 2 model 2                 0.0       30.0  \n",
       "walk 2 model assembled         0.0       30.0  \n",
       "walk 3 model 1                 0.0       30.0  \n",
       "walk 3 model 2                 0.0       30.0  \n",
       "walk 3 model assembled         0.0       30.0  \n",
       "walk 4 model 1                 0.0       30.0  \n",
       "walk 4 model 2                 0.0       30.0  \n",
       "walk 4 model assembled         0.0       30.0  \n",
       "walk 5 model 1                 0.0       14.0  \n",
       "walk 5 model 2                 0.0       14.0  \n",
       "walk 5 model assembled         0.0       14.0  \n",
       "mean model 1                   0.0       26.8  \n",
       "mean model 2                   0.0       26.8  \n",
       "mean model as                  0.0       26.8  \n",
       "\n",
       "[18 rows x 44 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark for all models\n",
    "Run experiments for all models and save results to csv and source code of model to txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_dense_model\n",
      "results saved to ../res/2021-04-19_get_dense_model.csv\n",
      "results saved to ../res/2021-04-19_get_dense_model.txt\n",
      "get_encoder_decoder\n",
      "results saved to ../res/2021-04-19_get_encoder_decoder.csv\n",
      "results saved to ../res/2021-04-19_get_encoder_decoder.txt\n",
      "get_baseline\n",
      "results saved to ../res/2021-04-19_get_baseline.csv\n",
      "results saved to ../res/2021-04-19_get_baseline.txt\n",
      "get_custom_linear_regression\n",
      "results saved to ../res/2021-04-19_get_custom_linear_regression.csv\n",
      "results saved to ../res/2021-04-19_get_custom_linear_regression.txt\n",
      "get_assemble\n",
      "results saved to ../res/2021-04-19_get_assemble.csv\n",
      "results saved to ../res/2021-04-19_get_assemble.txt\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True  # remove warnings from tensorflow\n",
    "\n",
    "result_dir = \"../res/\"\n",
    "\n",
    "model_list_trainable = [\n",
    "    get_dense_model,\n",
    "    get_encoder_decoder\n",
    "]\n",
    "\n",
    "model_list_untrainable = [\n",
    "    get_baseline,\n",
    "    get_custom_linear_regression,\n",
    "]\n",
    "\n",
    "model_list_assemble = [\n",
    "    (get_dense_model, dg, get_encoder_decoder, dg_2, get_assemble)\n",
    "]\n",
    "\n",
    "nb_fit = 250\n",
    "nb_eval = 10\n",
    "nb_test = 30\n",
    "epochs = 300\n",
    "es_stop_val = True\n",
    "\n",
    "today_date = str(date.today())\n",
    "\n",
    "def benchmark(model_generator, walk):\n",
    "    model_name = model_generator.__name__\n",
    "    print(model_name)\n",
    "    df_errors = walk(model_generator, nb_fit, nb_eval, nb_test, \n",
    "                                        epochs=epochs, verbose=0, es_stop_val=es_stop_val)\n",
    "    # register csv for error\n",
    "    filename = f'{result_dir}{today_date}_{model_name}'\n",
    "    file_csv = f'{filename}.csv'\n",
    "    file_txt = f'{filename}.txt'\n",
    "    \n",
    "    i = 0\n",
    "    while os.path.isfile(file_csv) or os.path.isfile(file_txt):\n",
    "        i += 1\n",
    "        file_csv = f'{filename}_{i}.csv'\n",
    "        file_txt = f'{filename}_{i}.txt'\n",
    "    df_errors.to_csv(file_csv)\n",
    "    print(f'results saved to {file_csv}')\n",
    "    \n",
    "    # register source code of the model and additional infos\n",
    "    info_txt = inspect.getsource(model_generator) + f'\\n\\nepochs = {epochs}, es_stop_val = {es_stop_val}' + \\\n",
    "        '\\n\\n' + str(dg)\n",
    "    with open(file_txt, 'w') as file:\n",
    "        file.write(info_txt)\n",
    "    print(f'results saved to {file_txt}')\n",
    "\n",
    "for model_generator in model_list_trainable:\n",
    "    benchmark(model_generator, walk_forward_evaluation)\n",
    "    \n",
    "for model_generator in model_list_untrainable:\n",
    "    benchmark(model_generator, walk_forward_evaluation_untrainable)\n",
    "\n",
    "# assembly benchmark\n",
    "for gen_1, dg_model_1, gen_2, dg_model_2, gen_as in model_list_assemble:\n",
    "    model_name = gen_as.__name__\n",
    "    print(model_name)\n",
    "    df_errors = walk_forward_assembly(gen_1, dg_model_1, gen_2, dg_model_2, gen_as,\n",
    "                     nb_fit, nb_eval, nb_test, epochs=epochs, verbose=0)\n",
    "    # register csv for error\n",
    "    filename = f'{result_dir}{today_date}_{model_name}'\n",
    "    file_csv = f'{filename}.csv'\n",
    "    file_txt = f'{filename}.txt'\n",
    "    \n",
    "    i = 0\n",
    "    while os.path.isfile(file_csv) or os.path.isfile(file_txt):\n",
    "        i += 1\n",
    "        file_csv = f'{filename}_{i}.csv'\n",
    "        file_txt = f'{filename}_{i}.txt'\n",
    "    df_errors.to_csv(file_csv)\n",
    "    print(f'results saved to {file_csv}')\n",
    "    \n",
    "    # register source code of the models and additional infos\n",
    "    info_txt = 'model 1:\\n\\n' + inspect.getsource(gen_1) + \\\n",
    "        '\\n\\nmodel 2:\\n\\n' + inspect.getsource(gen_2) + \\\n",
    "        '\\n\\nmodel as:\\n\\n' + inspect.getsource(gen_as) + \\\n",
    "        f'\\n\\nepochs = {epochs}, es_stop_val = {es_stop_val}' + \\\n",
    "        '\\n\\ndg_1:\\n' + str(dg_model_1) + \\\n",
    "        '\\n\\ndg_2:\\n' + str(dg_model_2)\n",
    "    with open(file_txt, 'w') as file:\n",
    "        file.write(info_txt)\n",
    "    print(f'results saved to {file_txt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
