{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, date, timedelta\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import io\n",
    "from time import sleep\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Reshape, TimeDistributed, LSTM, Lambda, Bidirectional, RepeatVector, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ProgbarLogger, History\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from IPython.display import display\n",
    "from tensorflow.keras.metrics import get as metric_get\n",
    "import re\n",
    "import talos\n",
    "from copy import deepcopy, copy\n",
    "import time\n",
    "import util\n",
    "import functools \n",
    "import itertools\n",
    "import networkx as nx\n",
    "from typing import List, Iterator, Tuple, Dict, Union\n",
    "import json\n",
    "\n",
    "# Set up GPU:\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices: tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "load hospitalisations and trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics considered\n",
    "list_topics = {\n",
    "    'Fièvre': '/m/0cjf0',\n",
    "    'Mal de gorge': '/m/0b76bty',\n",
    "    #'Dyspnée': '/m/01cdt5',\n",
    "    #'Agueusie': '/m/05sfr2',\n",
    "    #'Anosmie': '/m/0m7pl',\n",
    "    #'Coronavirus': '/m/01cpyy',\n",
    "    #'Virus': '/m/0g9pc',\n",
    "    #'Température corporelle humaine': '/g/1213j0cz',\n",
    "    #'Épidémie': '/m/0hn9s',\n",
    "    'Symptôme': '/m/01b_06',\n",
    "    #'Thermomètre': '/m/07mf1',\n",
    "    #'Grippe espagnole': '/m/01c751',\n",
    "    #'Paracétamol': '/m/0lbt3',\n",
    "    #'Respiration': '/m/02gy9_',\n",
    "    #'Toux': '/m/01b_21'\n",
    "}\n",
    "\n",
    "# hospitalisations features given as input\n",
    "list_hosp_features = [\n",
    "    'NEW_HOSP',\n",
    "    'TOT_HOSP',\n",
    "    #'TOT_HOSP_log',\n",
    "    #'TOT_HOSP_pct',\n",
    "]\n",
    "\n",
    "europe = False  # if True, use european countries. Otherwise, use french regions and belgium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional information: what is the target, should some features remain unscaled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target, should be one of the hosp features\n",
    "target = 'TOT_HOSP'\n",
    "\n",
    "cumsum = False  # if True, the target will be accumulated over each day\n",
    "\n",
    "# features that should not be scaled\n",
    "unscaled = [\n",
    "    #'NEW_HOSP',\n",
    "    #'TOT_HOSP',\n",
    "    #'TOT_HOSP_log',\n",
    "    #'TOT_HOSP_pct',\n",
    "    #'Fièvre',\n",
    "    #'Mal de gorge',\n",
    "    #'Dyspnée',\n",
    "    #'Agueusie',\n",
    "    #'Anosmie',\n",
    "    #'Coronavirus',\n",
    "    #'Virus',\n",
    "    #'Température corporelle humaine',\n",
    "    #'Épidémie',\n",
    "    #'Symptôme',\n",
    "    #'Thermomètre',\n",
    "    #'Grippe espagnole',\n",
    "    #'Paracétamol',\n",
    "    #'Respiration',\n",
    "    #'Toux',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of prediction: how many days as input sould be used to predict how many days as output? Should we give a prediction on all days or only on the last?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_begin = \"2020-02-01\"\n",
    "n_forecast = 20\n",
    "n_samples = 30\n",
    "predict_one = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_metrics = [metric_get(\"MeanSquaredError\"), metric_get('MeanAbsoluteError'), \n",
    "                      metric_get('RootMeanSquaredError')]\n",
    "\n",
    "url_world = \"../data/hospi/world.csv\"\n",
    "url_pop = \"../data/population.txt\"\n",
    "url_trends = \"../data/trends/model/\"\n",
    "url_hospi_belgium = \"../data/hospi/be-covid-hospi.csv\"\n",
    "url_department_france = \"france_departements.csv\"\n",
    "url_hospi_france_new = \"../data/hospi/fr-covid-hospi.csv\"\n",
    "url_hospi_france_tot = \"../data/hospi/fr-covid-hospi-total.csv\"\n",
    "if europe:\n",
    "    population = util.get_world_population(url_pop)\n",
    "    renaming = {v: k for k, v in util.european_geocodes.items()}\n",
    "    geocodes = {k: v for k, v in util.european_geocodes.items() if population[k] > 1_000_000}\n",
    "    df_hospi = util.hospi_world(url_world, geocodes, renaming, new_hosp=True, date_begin=date_begin)\n",
    "    augment_population = {k: v/1000 for k, v in population.items()}\n",
    "else:\n",
    "    geocodes = util.french_region_and_be\n",
    "    population = pd.read_csv(url_department_france).groupby('regionTrends').agg({'population': 'sum'})\n",
    "    augment_population = {k: pop['population'] / 100_000 for k, pop in population.iterrows()}  # pop per 100.000\n",
    "    df_hospi = util.hospi_french_region_and_be(url_hospi_france_tot, url_hospi_france_new, url_hospi_belgium, \n",
    "                                           url_department_france, util.french_region_and_be, new_hosp_in=True, \n",
    "                                           tot_hosp=True, date_begin=date_begin)\n",
    "df_trends = util.create_df_trends(url_trends, list_topics, geocodes)  # TODO deal with augmented data\n",
    "for k in df_hospi.keys(): # Rolling average of 7 days \n",
    "    df_hospi[k] = df_hospi[k].rolling(7, center=True).mean().dropna()\n",
    "    df_trends[k] = df_trends[k].rolling(7, center=True).mean().dropna()\n",
    "merged_df = {k: pd.merge(df_hospi[k], df_trends[k], left_index=True, right_index=True).dropna() for k,v in geocodes.items()}\n",
    "    \n",
    "scaler_generator = MinMaxScaler\n",
    "dg = util.DataGenerator(merged_df, n_samples, n_forecast, target, scaler_generator=scaler_generator, scaler_type='batch',\n",
    "                       augment_merge=2, augment_adjacency=util.france_region_adjacency, augment_population=augment_population,\n",
    "                       predict_one=predict_one, cumsum=cumsum, data_columns=list_hosp_features)\n",
    "n_features = dg.n_features\n",
    "target_idx = dg.target_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not predict_one:\n",
    "    weights_loss = np.array([(1/x) for x in range(1, n_forecast+1)])\n",
    "else:\n",
    "    weights_loss = 1\n",
    "\n",
    "def custom_loss_function(y_true, y_pred):\n",
    "    y_true = y_true * weights_loss\n",
    "    y_pred = y_pred * weights_loss\n",
    "    return tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "def get_encoder_decoder(batch_input_shape):\n",
    "    model = Sequential()\n",
    "    #model.add(Bidirectional(LSTM(8, return_sequences=True, stateful=False), \n",
    "    #                        input_shape=(n_samples, n_features), merge_mode=\"ave\"))\n",
    "    model.add(LSTM(16, return_sequences=True, stateful=False, batch_input_shape=batch_input_shape, recurrent_dropout=0))\n",
    "    model.add(LSTM(8, return_sequences=False, stateful=False))\n",
    "    model.add(RepeatVector(n_forecast))  # repeat\n",
    "    model.add(LSTM(8, return_sequences=True, stateful=False))  # dec\n",
    "    if not predict_one:\n",
    "        model.add(LSTM(16, return_sequences=True, stateful=False))  # dec\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "        model.add(Reshape((n_forecast,)))\n",
    "    else:\n",
    "        model.add(LSTM(16, return_sequences=False, stateful=False))  # dec\n",
    "        model.add(Dense(1))\n",
    "        model.add(Reshape((1,)))\n",
    "    model.compile(loss=custom_loss_function, optimizer='adam', metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "    \n",
    "get_encoder_decoder((1, n_samples, n_features)).output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models to beat\n",
    "### MultiStepLastBaseline\n",
    "This model repeats the last value of hospitalisations `n_forecast` time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStepLastBaseline(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    repeat the last hospitalisations given as input n_forecast time\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        a = inputs[:, -1:, target_idx:target_idx+1]  # target of the last days\n",
    "        # a = tf.where(tf.not_equal(a, 0), tf.zeros_like(a), a)\n",
    "        if not predict_one:\n",
    "            return tf.tile(\n",
    "                a,\n",
    "                [1, n_forecast, 1]   # repeat target n_forecast time\n",
    "            )\n",
    "        else:\n",
    "            return tf.tile(a, [1, 1, 1])\n",
    "\n",
    "def get_baseline():\n",
    "    model = MultiStepLastBaseline()\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "linear regression of the last `n_sample` days used to predict the next `n_forecast` days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionHospi(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    repeat the last hospitalisations given as input n_forecast time\n",
    "    \"\"\"\n",
    "    def predict(self, inputs):\n",
    "        y = inputs[:, :, target_idx]  # target of the last days\n",
    "        length = len(inputs)\n",
    "        x = np.arange(n_samples).reshape(-1,1)  # dates of the target\n",
    "        if not predict_one:\n",
    "            result = np.zeros((length, n_forecast))\n",
    "            for i in range(length):\n",
    "                regr = LinearRegression().fit(x, y[i])  # linear regression of (days, target)\n",
    "                result[i] = regr.predict(np.arange(n_samples, n_samples+n_forecast).reshape(-1,1))\n",
    "        else:\n",
    "            result = np.zeros((length, 1))\n",
    "            for i in range(length):\n",
    "                regr = LinearRegression().fit(x, y[i])\n",
    "                result[i] = regre.predict([n_samples+n_forecast-1])\n",
    "        return result\n",
    "        \n",
    "def get_custom_linear_regression():\n",
    "    model = LinearRegressionHospi()\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 fully connected layer (Dense model)\n",
    "Using only the target in the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseModel = Sequential()\n",
    "DenseModel.add(Lambda(lambda x: x[:,:,target_idx]))  # select only the target of the previous days\n",
    "if not predict_one:\n",
    "    DenseModel.add(Dense(n_forecast))   # predict the next target based on the previous ones\n",
    "else:\n",
    "    DenseModel.add(Dense(1))\n",
    "DenseModel.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=[tf.metrics.MeanAbsoluteError()])\n",
    "\n",
    "def get_dense_model(batch_input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x[:,:,target_idx], batch_input_shape=batch_input_shape))  # select only the target of the previous days\n",
    "    model.add(Dense(n_forecast))   # predict the next target based on the previous ones\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                          metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple prediction\n",
    "use a percentage of values for training and the remaining values for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X% for training, remaining for test\n",
    "ratio_training = 0.9\n",
    "nb_datapoints = dg.batch_size\n",
    "max_train = int(ratio_training * nb_datapoints)\n",
    "train_idx = np.array(range(max_train))\n",
    "test_idx = np.array(range(max_train, nb_datapoints))\n",
    "X_train = dg.get_x(train_idx, scaled=True)\n",
    "Y_train = dg.get_y(train_idx, scaled=True)\n",
    "X_test_unscaled = dg.get_x(test_idx, scaled=False)\n",
    "Y_test_unscaled = dg.get_y(test_idx, scaled=False)\n",
    "X_test = dg.get_x(test_idx, scaled=True, use_previous_scaler=True)\n",
    "Y_test = dg.get_y(test_idx, scaled=True, use_previous_scaler=True)\n",
    "Y_test_unpadded_unscaled = dg.remove_padded_y(Y_test_unscaled, idx=test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (317, 30, 16)             1216      \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (317, 8)                  800       \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (317, 20, 8)              0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (317, 20, 8)              544       \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (317, 20, 16)             1600      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (317, 20, 1)              17        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (317, 20)                 0         \n",
      "=================================================================\n",
      "Total params: 4,177\n",
      "Trainable params: 4,177\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "66/66 [==============================] - 4s 6ms/step - loss: 0.0700 - mse: 0.7249 - mae: 0.7019 - root_mean_squared_error: 0.8496\n",
      "Epoch 2/50\n",
      "25/66 [==========>...................] - ETA: 0s - loss: 0.0420 - mse: 0.5527 - mae: 0.5619 - root_mean_squared_error: 0.7431"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ea520c5ff445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mencoder_decoder_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mencoder_decoder_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"computed in {str(str(timedelta(seconds=end)))} s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_input = len(train_idx)\n",
    "encoder_decoder_train = get_encoder_decoder(batch_input_shape=(batch_input, n_samples, n_features))\n",
    "encoder_decoder_train.summary()\n",
    "start = time.time()\n",
    "encoder_decoder_train.fit(X_train, Y_train, epochs=50, verbose=1, batch_size=batch_input)\n",
    "end = time.time() - start\n",
    "print(f\"computed in {str(str(timedelta(seconds=end)))} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01182181 0.03292095 0.05239069 0.07486242 0.10219903 0.13145376\n",
      " 0.15931553 0.18528912 0.21041148 0.23275081 0.25341427 0.27160552\n",
      " 0.28784532 0.3017836  0.31335848 0.32158675 0.32743756 0.33143487\n",
      " 0.33252344 0.33246252]\n",
      "[ 1.8350716   3.14282862  5.36804604  7.84245199 10.50754129 13.16738782\n",
      " 15.69878695 18.14806111 20.68732077 23.12374419 25.51227921 27.72285965\n",
      " 29.72751725 31.5347566  33.08889689 34.29457278 35.23735888 35.88072739\n",
      " 36.19764656 36.38213828]\n",
      "[ 3.81933408  5.32108304  7.68964482 10.20137742 12.92528528 15.58922774\n",
      " 18.2787198  20.68907373 23.08679783 25.50786655 27.88145711 30.05661317\n",
      " 32.12509532 34.01339593 35.6625149  37.14915806 38.34783643 39.41953212\n",
      " 40.34762178 41.08044124]\n"
     ]
    }
   ],
   "source": [
    "batch_input = len(test_idx)\n",
    "encoder_decoder_prediction = get_encoder_decoder(batch_input_shape=(batch_input, n_samples, n_features) )\n",
    "encoder_decoder_prediction.set_weights(encoder_decoder_train.get_weights())\n",
    "\n",
    "Y_predicted = np.squeeze(encoder_decoder_prediction.predict(X_test, batch_size=batch_input))\n",
    "Y_predicted_unscaled = dg.inverse_transform_y(Y_predicted, idx=test_idx)\n",
    "\n",
    "print(np.mean(Y_predicted - Y_test, axis=0))\n",
    "print(np.mean(Y_predicted_unscaled - Y_test_unscaled, axis=0))\n",
    "\n",
    "Y_predicted_unpadded_unscaled =  dg.remove_padded_y(Y_predicted_unscaled, idx=test_idx)\n",
    "\n",
    "print(np.mean(abs(Y_predicted_unpadded_unscaled - Y_test_unpadded_unscaled), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.46074878  -0.15338165  -0.40157006   0.74939613  -2.39673914\n",
      "  -2.88345411   0.05736714  -0.38828503  -3.97644928  -6.80736716\n",
      "  -8.93417875 -12.83595009 -16.29206925 -14.67572464 -17.6491546\n",
      " -21.76751208 -27.00664252 -30.76871981 -36.3254831  -36.26026571]\n",
      "[ 97.23976213 121.77371275 121.59357122 122.7349443  128.44000302\n",
      " 105.06647095  67.53846734 110.9022132  134.43473352 129.85358327\n",
      " 136.07685939 141.34590485 126.69369166  95.88279133 132.96303825\n",
      " 152.38068353 148.63060826 152.68435713 156.96830775 143.07008432]\n"
     ]
    }
   ],
   "source": [
    "baseline_model= get_baseline()\n",
    "Y_predicted_unscaled = np.squeeze(baseline_model.predict(X_test_unscaled))\n",
    "\n",
    "print(np.mean(Y_predicted_unscaled - Y_test_unscaled, axis=0))\n",
    "\n",
    "Y_predicted_unpadded_unscaled =  dg.remove_padded_y(Y_predicted_unscaled, idx=test_idx)\n",
    "\n",
    "print(np.mean(abs(Y_predicted_unpadded_unscaled - Y_test_unpadded_unscaled), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (1, 30)                   0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (1, 20)                   620       \n",
      "=================================================================\n",
      "Total params: 620\n",
      "Trainable params: 620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 1.5908 - mse: 1.5908 - mae: 0.7549 - root_mean_squared_error: 1.2600\n",
      "Epoch 2/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 1.0971 - mse: 1.0971 - mae: 0.6273 - root_mean_squared_error: 1.0471\n",
      "Epoch 3/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.9083 - mse: 0.9083 - mae: 0.5514 - root_mean_squared_error: 0.9530\n",
      "Epoch 4/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8290 - mse: 0.8290 - mae: 0.5294 - root_mean_squared_error: 0.9104\n",
      "Epoch 5/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7829 - mse: 0.7829 - mae: 0.5089 - root_mean_squared_error: 0.8840\n",
      "Epoch 6/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7777 - mse: 0.7777 - mae: 0.5034 - root_mean_squared_error: 0.8817\n",
      "Epoch 7/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7990 - mse: 0.7990 - mae: 0.5138 - root_mean_squared_error: 0.8937\n",
      "Epoch 8/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8278 - mse: 0.8278 - mae: 0.5058 - root_mean_squared_error: 0.9093\n",
      "Epoch 9/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7990 - mse: 0.7990 - mae: 0.5016 - root_mean_squared_error: 0.8937\n",
      "Epoch 10/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7891 - mse: 0.7891 - mae: 0.5046 - root_mean_squared_error: 0.8882\n",
      "Epoch 11/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8128 - mse: 0.8128 - mae: 0.5141 - root_mean_squared_error: 0.9015\n",
      "Epoch 12/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8053 - mse: 0.8053 - mae: 0.5037 - root_mean_squared_error: 0.8971\n",
      "Epoch 13/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7967 - mse: 0.7967 - mae: 0.5048 - root_mean_squared_error: 0.8924\n",
      "Epoch 14/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7787 - mse: 0.7787 - mae: 0.5017 - root_mean_squared_error: 0.8823\n",
      "Epoch 15/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7989 - mse: 0.7989 - mae: 0.5052 - root_mean_squared_error: 0.8936\n",
      "Epoch 16/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7864 - mse: 0.7864 - mae: 0.4972 - root_mean_squared_error: 0.8865\n",
      "Epoch 17/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7686 - mse: 0.7686 - mae: 0.4961 - root_mean_squared_error: 0.8764\n",
      "Epoch 18/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8093 - mse: 0.8093 - mae: 0.5068 - root_mean_squared_error: 0.8994\n",
      "Epoch 19/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8094 - mse: 0.8094 - mae: 0.5115 - root_mean_squared_error: 0.8989\n",
      "Epoch 20/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8146 - mse: 0.8146 - mae: 0.5156 - root_mean_squared_error: 0.9024\n",
      "Epoch 21/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7942 - mse: 0.7942 - mae: 0.5088 - root_mean_squared_error: 0.8909\n",
      "Epoch 22/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7580 - mse: 0.7580 - mae: 0.4940 - root_mean_squared_error: 0.8703\n",
      "Epoch 23/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8113 - mse: 0.8113 - mae: 0.5112 - root_mean_squared_error: 0.9003\n",
      "Epoch 24/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7863 - mse: 0.7863 - mae: 0.5007 - root_mean_squared_error: 0.8866\n",
      "Epoch 25/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8132 - mse: 0.8132 - mae: 0.5090 - root_mean_squared_error: 0.9016\n",
      "Epoch 26/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7765 - mse: 0.7765 - mae: 0.5017 - root_mean_squared_error: 0.8811\n",
      "Epoch 27/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7990 - mse: 0.7990 - mae: 0.5075 - root_mean_squared_error: 0.8937\n",
      "Epoch 28/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8003 - mse: 0.8003 - mae: 0.5069 - root_mean_squared_error: 0.8945\n",
      "Epoch 29/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7958 - mse: 0.7958 - mae: 0.5007 - root_mean_squared_error: 0.8920\n",
      "Epoch 30/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8110 - mse: 0.8110 - mae: 0.5122 - root_mean_squared_error: 0.9004\n",
      "Epoch 31/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7910 - mse: 0.7910 - mae: 0.5105 - root_mean_squared_error: 0.8893\n",
      "Epoch 32/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7890 - mse: 0.7890 - mae: 0.5022 - root_mean_squared_error: 0.8882\n",
      "Epoch 33/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8114 - mse: 0.8114 - mae: 0.5130 - root_mean_squared_error: 0.9007\n",
      "Epoch 34/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8010 - mse: 0.8010 - mae: 0.5091 - root_mean_squared_error: 0.8949\n",
      "Epoch 35/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8018 - mse: 0.8018 - mae: 0.5076 - root_mean_squared_error: 0.8953\n",
      "Epoch 36/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8167 - mse: 0.8167 - mae: 0.5124 - root_mean_squared_error: 0.9033\n",
      "Epoch 37/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7856 - mse: 0.7856 - mae: 0.5051 - root_mean_squared_error: 0.8862\n",
      "Epoch 38/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7859 - mse: 0.7859 - mae: 0.5008 - root_mean_squared_error: 0.8864\n",
      "Epoch 39/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7857 - mse: 0.7857 - mae: 0.5060 - root_mean_squared_error: 0.8862\n",
      "Epoch 40/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8101 - mse: 0.8101 - mae: 0.5156 - root_mean_squared_error: 0.8999\n",
      "Epoch 41/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8048 - mse: 0.8048 - mae: 0.5109 - root_mean_squared_error: 0.8970\n",
      "Epoch 42/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7825 - mse: 0.7825 - mae: 0.5051 - root_mean_squared_error: 0.8840\n",
      "Epoch 43/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7675 - mse: 0.7675 - mae: 0.5024 - root_mean_squared_error: 0.8759\n",
      "Epoch 44/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8163 - mse: 0.8163 - mae: 0.5105 - root_mean_squared_error: 0.9032\n",
      "Epoch 45/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7890 - mse: 0.7890 - mae: 0.5047 - root_mean_squared_error: 0.8881\n",
      "Epoch 46/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7855 - mse: 0.7855 - mae: 0.5068 - root_mean_squared_error: 0.8862\n",
      "Epoch 47/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8004 - mse: 0.8004 - mae: 0.5112 - root_mean_squared_error: 0.8946\n",
      "Epoch 48/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7871 - mse: 0.7871 - mae: 0.4978 - root_mean_squared_error: 0.8869\n",
      "Epoch 49/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8010 - mse: 0.8010 - mae: 0.5071 - root_mean_squared_error: 0.8949\n",
      "Epoch 50/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7982 - mse: 0.7982 - mae: 0.5152 - root_mean_squared_error: 0.8933\n",
      "Epoch 51/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7880 - mse: 0.7880 - mae: 0.5013 - root_mean_squared_error: 0.8873\n",
      "Epoch 52/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7786 - mse: 0.7786 - mae: 0.5024 - root_mean_squared_error: 0.8820\n",
      "Epoch 53/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7998 - mse: 0.7998 - mae: 0.5082 - root_mean_squared_error: 0.8942\n",
      "Epoch 54/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8063 - mse: 0.8063 - mae: 0.5102 - root_mean_squared_error: 0.8978\n",
      "Epoch 55/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8024 - mse: 0.8024 - mae: 0.5059 - root_mean_squared_error: 0.8955\n",
      "Epoch 56/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8025 - mse: 0.8025 - mae: 0.5088 - root_mean_squared_error: 0.8957\n",
      "Epoch 57/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7873 - mse: 0.7873 - mae: 0.5030 - root_mean_squared_error: 0.8869\n",
      "Epoch 58/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7876 - mse: 0.7876 - mae: 0.5056 - root_mean_squared_error: 0.8869\n",
      "Epoch 59/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8158 - mse: 0.8158 - mae: 0.5128 - root_mean_squared_error: 0.9030\n",
      "Epoch 60/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7955 - mse: 0.7955 - mae: 0.5085 - root_mean_squared_error: 0.8918\n",
      "Epoch 61/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7936 - mse: 0.7936 - mae: 0.5072 - root_mean_squared_error: 0.8907\n",
      "Epoch 62/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7774 - mse: 0.7774 - mae: 0.5017 - root_mean_squared_error: 0.8813\n",
      "Epoch 63/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7638 - mse: 0.7638 - mae: 0.4982 - root_mean_squared_error: 0.8738\n",
      "Epoch 64/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8145 - mse: 0.8145 - mae: 0.5087 - root_mean_squared_error: 0.9024\n",
      "Epoch 65/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7863 - mse: 0.7863 - mae: 0.5031 - root_mean_squared_error: 0.8867\n",
      "Epoch 66/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7749 - mse: 0.7749 - mae: 0.4985 - root_mean_squared_error: 0.8801\n",
      "Epoch 67/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7931 - mse: 0.7931 - mae: 0.5002 - root_mean_squared_error: 0.8905\n",
      "Epoch 68/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8207 - mse: 0.8207 - mae: 0.5166 - root_mean_squared_error: 0.9058\n",
      "Epoch 69/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8054 - mse: 0.8054 - mae: 0.5083 - root_mean_squared_error: 0.8972\n",
      "Epoch 70/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7634 - mse: 0.7634 - mae: 0.4942 - root_mean_squared_error: 0.8730\n",
      "Epoch 71/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8029 - mse: 0.8029 - mae: 0.5190 - root_mean_squared_error: 0.8960\n",
      "Epoch 72/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8210 - mse: 0.8210 - mae: 0.5127 - root_mean_squared_error: 0.9059\n",
      "Epoch 73/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7871 - mse: 0.7871 - mae: 0.5013 - root_mean_squared_error: 0.8870\n",
      "Epoch 74/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8180 - mse: 0.8180 - mae: 0.5134 - root_mean_squared_error: 0.9038\n",
      "Epoch 75/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7475 - mse: 0.7475 - mae: 0.4934 - root_mean_squared_error: 0.8643\n",
      "Epoch 76/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7790 - mse: 0.7790 - mae: 0.5039 - root_mean_squared_error: 0.8825\n",
      "Epoch 77/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7526 - mse: 0.7526 - mae: 0.4957 - root_mean_squared_error: 0.8673\n",
      "Epoch 78/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8237 - mse: 0.8237 - mae: 0.5166 - root_mean_squared_error: 0.9073\n",
      "Epoch 79/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8093 - mse: 0.8093 - mae: 0.5135 - root_mean_squared_error: 0.8995\n",
      "Epoch 80/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7865 - mse: 0.7865 - mae: 0.5006 - root_mean_squared_error: 0.8868\n",
      "Epoch 81/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8172 - mse: 0.8172 - mae: 0.5111 - root_mean_squared_error: 0.9036\n",
      "Epoch 82/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8137 - mse: 0.8137 - mae: 0.5163 - root_mean_squared_error: 0.9020\n",
      "Epoch 83/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8241 - mse: 0.8241 - mae: 0.5095 - root_mean_squared_error: 0.9075\n",
      "Epoch 84/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7896 - mse: 0.7896 - mae: 0.5020 - root_mean_squared_error: 0.8883\n",
      "Epoch 85/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8021 - mse: 0.8021 - mae: 0.5093 - root_mean_squared_error: 0.8955\n",
      "Epoch 86/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7862 - mse: 0.7862 - mae: 0.5023 - root_mean_squared_error: 0.8865\n",
      "Epoch 87/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7766 - mse: 0.7766 - mae: 0.5015 - root_mean_squared_error: 0.8811\n",
      "Epoch 88/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8325 - mse: 0.8325 - mae: 0.5179 - root_mean_squared_error: 0.9121\n",
      "Epoch 89/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7887 - mse: 0.7887 - mae: 0.4974 - root_mean_squared_error: 0.8880\n",
      "Epoch 90/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7967 - mse: 0.7967 - mae: 0.5099 - root_mean_squared_error: 0.8925\n",
      "Epoch 91/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7962 - mse: 0.7962 - mae: 0.5067 - root_mean_squared_error: 0.8922\n",
      "Epoch 92/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8122 - mse: 0.8122 - mae: 0.5131 - root_mean_squared_error: 0.9011\n",
      "Epoch 93/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8054 - mse: 0.8054 - mae: 0.5132 - root_mean_squared_error: 0.8973\n",
      "Epoch 94/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7737 - mse: 0.7737 - mae: 0.4998 - root_mean_squared_error: 0.8794\n",
      "Epoch 95/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7849 - mse: 0.7849 - mae: 0.5061 - root_mean_squared_error: 0.8856\n",
      "Epoch 96/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7859 - mse: 0.7859 - mae: 0.5052 - root_mean_squared_error: 0.8863\n",
      "Epoch 97/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7675 - mse: 0.7675 - mae: 0.5010 - root_mean_squared_error: 0.8757\n",
      "Epoch 98/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7746 - mse: 0.7746 - mae: 0.4978 - root_mean_squared_error: 0.8800\n",
      "Epoch 99/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8233 - mse: 0.8233 - mae: 0.5189 - root_mean_squared_error: 0.9070\n",
      "Epoch 100/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8123 - mse: 0.8123 - mae: 0.5036 - root_mean_squared_error: 0.9006\n",
      "Epoch 101/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8064 - mse: 0.8064 - mae: 0.5085 - root_mean_squared_error: 0.8979\n",
      "Epoch 102/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8047 - mse: 0.8047 - mae: 0.5065 - root_mean_squared_error: 0.8969\n",
      "Epoch 103/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7878 - mse: 0.7878 - mae: 0.5082 - root_mean_squared_error: 0.8875\n",
      "Epoch 104/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8060 - mse: 0.8060 - mae: 0.5069 - root_mean_squared_error: 0.8977\n",
      "Epoch 105/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7929 - mse: 0.7929 - mae: 0.5072 - root_mean_squared_error: 0.8904\n",
      "Epoch 106/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7816 - mse: 0.7816 - mae: 0.5033 - root_mean_squared_error: 0.8839\n",
      "Epoch 107/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8031 - mse: 0.8031 - mae: 0.5040 - root_mean_squared_error: 0.8958\n",
      "Epoch 108/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7829 - mse: 0.7829 - mae: 0.5040 - root_mean_squared_error: 0.8846\n",
      "Epoch 109/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7915 - mse: 0.7915 - mae: 0.5078 - root_mean_squared_error: 0.8895\n",
      "Epoch 110/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8169 - mse: 0.8169 - mae: 0.5109 - root_mean_squared_error: 0.9037\n",
      "Epoch 111/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8016 - mse: 0.8016 - mae: 0.5088 - root_mean_squared_error: 0.8953\n",
      "Epoch 112/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7935 - mse: 0.7935 - mae: 0.5031 - root_mean_squared_error: 0.8907\n",
      "Epoch 113/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7977 - mse: 0.7977 - mae: 0.5078 - root_mean_squared_error: 0.8930\n",
      "Epoch 114/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7903 - mse: 0.7903 - mae: 0.5050 - root_mean_squared_error: 0.8889\n",
      "Epoch 115/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7931 - mse: 0.7931 - mae: 0.5090 - root_mean_squared_error: 0.8905\n",
      "Epoch 116/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8183 - mse: 0.8183 - mae: 0.5147 - root_mean_squared_error: 0.9043\n",
      "Epoch 117/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7999 - mse: 0.7999 - mae: 0.5066 - root_mean_squared_error: 0.8942\n",
      "Epoch 118/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7959 - mse: 0.7959 - mae: 0.5026 - root_mean_squared_error: 0.8920\n",
      "Epoch 119/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8041 - mse: 0.8041 - mae: 0.5046 - root_mean_squared_error: 0.8964\n",
      "Epoch 120/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7813 - mse: 0.7813 - mae: 0.4990 - root_mean_squared_error: 0.8838\n",
      "Epoch 121/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8172 - mse: 0.8172 - mae: 0.5121 - root_mean_squared_error: 0.9039\n",
      "Epoch 122/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8077 - mse: 0.8077 - mae: 0.5044 - root_mean_squared_error: 0.8983\n",
      "Epoch 123/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7855 - mse: 0.7855 - mae: 0.4999 - root_mean_squared_error: 0.8862\n",
      "Epoch 124/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8113 - mse: 0.8113 - mae: 0.5094 - root_mean_squared_error: 0.9006\n",
      "Epoch 125/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7737 - mse: 0.7737 - mae: 0.4942 - root_mean_squared_error: 0.8794\n",
      "Epoch 126/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8239 - mse: 0.8239 - mae: 0.5183 - root_mean_squared_error: 0.9075\n",
      "Epoch 127/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7991 - mse: 0.7991 - mae: 0.4998 - root_mean_squared_error: 0.8939\n",
      "Epoch 128/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7790 - mse: 0.7790 - mae: 0.5005 - root_mean_squared_error: 0.8824\n",
      "Epoch 129/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8097 - mse: 0.8097 - mae: 0.5087 - root_mean_squared_error: 0.8998\n",
      "Epoch 130/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8128 - mse: 0.8128 - mae: 0.5107 - root_mean_squared_error: 0.9012\n",
      "Epoch 131/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8105 - mse: 0.8105 - mae: 0.5120 - root_mean_squared_error: 0.9000\n",
      "Epoch 132/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8110 - mse: 0.8110 - mae: 0.5116 - root_mean_squared_error: 0.9002\n",
      "Epoch 133/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7846 - mse: 0.7846 - mae: 0.5053 - root_mean_squared_error: 0.8855\n",
      "Epoch 134/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7852 - mse: 0.7852 - mae: 0.5025 - root_mean_squared_error: 0.8861\n",
      "Epoch 135/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7966 - mse: 0.7966 - mae: 0.5035 - root_mean_squared_error: 0.8923\n",
      "Epoch 136/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8057 - mse: 0.8057 - mae: 0.5035 - root_mean_squared_error: 0.8974\n",
      "Epoch 137/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7687 - mse: 0.7687 - mae: 0.5007 - root_mean_squared_error: 0.8766\n",
      "Epoch 138/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8183 - mse: 0.8183 - mae: 0.5155 - root_mean_squared_error: 0.9044\n",
      "Epoch 139/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7808 - mse: 0.7808 - mae: 0.5036 - root_mean_squared_error: 0.8835\n",
      "Epoch 140/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8075 - mse: 0.8075 - mae: 0.5126 - root_mean_squared_error: 0.8985\n",
      "Epoch 141/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8301 - mse: 0.8301 - mae: 0.5132 - root_mean_squared_error: 0.9109\n",
      "Epoch 142/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7821 - mse: 0.7821 - mae: 0.4969 - root_mean_squared_error: 0.8842\n",
      "Epoch 143/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8009 - mse: 0.8009 - mae: 0.5053 - root_mean_squared_error: 0.8948\n",
      "Epoch 144/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7780 - mse: 0.7780 - mae: 0.5057 - root_mean_squared_error: 0.8818\n",
      "Epoch 145/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7859 - mse: 0.7859 - mae: 0.5033 - root_mean_squared_error: 0.8864\n",
      "Epoch 146/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7984 - mse: 0.7984 - mae: 0.5078 - root_mean_squared_error: 0.8935\n",
      "Epoch 147/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7808 - mse: 0.7808 - mae: 0.5016 - root_mean_squared_error: 0.8835\n",
      "Epoch 148/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7907 - mse: 0.7907 - mae: 0.5001 - root_mean_squared_error: 0.8888\n",
      "Epoch 149/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7943 - mse: 0.7943 - mae: 0.5069 - root_mean_squared_error: 0.8912\n",
      "Epoch 150/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7886 - mse: 0.7886 - mae: 0.5062 - root_mean_squared_error: 0.8879\n",
      "Epoch 151/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8069 - mse: 0.8069 - mae: 0.5077 - root_mean_squared_error: 0.8981\n",
      "Epoch 152/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7762 - mse: 0.7762 - mae: 0.4997 - root_mean_squared_error: 0.8808\n",
      "Epoch 153/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8214 - mse: 0.8214 - mae: 0.5076 - root_mean_squared_error: 0.9062\n",
      "Epoch 154/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7798 - mse: 0.7798 - mae: 0.5016 - root_mean_squared_error: 0.8829\n",
      "Epoch 155/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7958 - mse: 0.7958 - mae: 0.5085 - root_mean_squared_error: 0.8920\n",
      "Epoch 156/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7949 - mse: 0.7949 - mae: 0.5055 - root_mean_squared_error: 0.8914\n",
      "Epoch 157/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8079 - mse: 0.8079 - mae: 0.5123 - root_mean_squared_error: 0.8987\n",
      "Epoch 158/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8190 - mse: 0.8190 - mae: 0.5125 - root_mean_squared_error: 0.9049\n",
      "Epoch 159/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7963 - mse: 0.7963 - mae: 0.5008 - root_mean_squared_error: 0.8921\n",
      "Epoch 160/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7773 - mse: 0.7773 - mae: 0.5024 - root_mean_squared_error: 0.8815\n",
      "Epoch 161/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7941 - mse: 0.7941 - mae: 0.5058 - root_mean_squared_error: 0.8911\n",
      "Epoch 162/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7912 - mse: 0.7912 - mae: 0.5031 - root_mean_squared_error: 0.8894\n",
      "Epoch 163/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7920 - mse: 0.7920 - mae: 0.5028 - root_mean_squared_error: 0.8898\n",
      "Epoch 164/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8047 - mse: 0.8047 - mae: 0.5069 - root_mean_squared_error: 0.8968\n",
      "Epoch 165/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7862 - mse: 0.7862 - mae: 0.5057 - root_mean_squared_error: 0.8866\n",
      "Epoch 166/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7782 - mse: 0.7782 - mae: 0.5016 - root_mean_squared_error: 0.8818\n",
      "Epoch 167/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8052 - mse: 0.8052 - mae: 0.5078 - root_mean_squared_error: 0.8967\n",
      "Epoch 168/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7525 - mse: 0.7525 - mae: 0.4896 - root_mean_squared_error: 0.8670\n",
      "Epoch 169/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7877 - mse: 0.7877 - mae: 0.5028 - root_mean_squared_error: 0.8872\n",
      "Epoch 170/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7843 - mse: 0.7843 - mae: 0.5039 - root_mean_squared_error: 0.8855\n",
      "Epoch 171/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7883 - mse: 0.7883 - mae: 0.5039 - root_mean_squared_error: 0.8877\n",
      "Epoch 172/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8149 - mse: 0.8149 - mae: 0.5092 - root_mean_squared_error: 0.9021\n",
      "Epoch 173/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8100 - mse: 0.8100 - mae: 0.5139 - root_mean_squared_error: 0.8999\n",
      "Epoch 174/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7744 - mse: 0.7744 - mae: 0.5043 - root_mean_squared_error: 0.8798\n",
      "Epoch 175/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7932 - mse: 0.7932 - mae: 0.5091 - root_mean_squared_error: 0.8904\n",
      "Epoch 176/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7856 - mse: 0.7856 - mae: 0.5044 - root_mean_squared_error: 0.8863\n",
      "Epoch 177/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7953 - mse: 0.7953 - mae: 0.5063 - root_mean_squared_error: 0.8916\n",
      "Epoch 178/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8094 - mse: 0.8094 - mae: 0.5067 - root_mean_squared_error: 0.8996\n",
      "Epoch 179/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8137 - mse: 0.8137 - mae: 0.5121 - root_mean_squared_error: 0.9019\n",
      "Epoch 180/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7850 - mse: 0.7850 - mae: 0.4995 - root_mean_squared_error: 0.8859\n",
      "Epoch 181/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7999 - mse: 0.7999 - mae: 0.5046 - root_mean_squared_error: 0.8942\n",
      "Epoch 182/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7846 - mse: 0.7846 - mae: 0.5030 - root_mean_squared_error: 0.8857\n",
      "Epoch 183/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8117 - mse: 0.8117 - mae: 0.5135 - root_mean_squared_error: 0.9008\n",
      "Epoch 184/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7963 - mse: 0.7963 - mae: 0.5028 - root_mean_squared_error: 0.8921\n",
      "Epoch 185/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7920 - mse: 0.7920 - mae: 0.5047 - root_mean_squared_error: 0.8898\n",
      "Epoch 186/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7980 - mse: 0.7980 - mae: 0.5061 - root_mean_squared_error: 0.8932\n",
      "Epoch 187/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7684 - mse: 0.7684 - mae: 0.4971 - root_mean_squared_error: 0.8764\n",
      "Epoch 188/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7833 - mse: 0.7833 - mae: 0.5048 - root_mean_squared_error: 0.8850\n",
      "Epoch 189/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7719 - mse: 0.7719 - mae: 0.5021 - root_mean_squared_error: 0.8785\n",
      "Epoch 190/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7843 - mse: 0.7843 - mae: 0.5054 - root_mean_squared_error: 0.8855\n",
      "Epoch 191/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7946 - mse: 0.7946 - mae: 0.5081 - root_mean_squared_error: 0.8912\n",
      "Epoch 192/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8040 - mse: 0.8040 - mae: 0.5082 - root_mean_squared_error: 0.8966\n",
      "Epoch 193/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7766 - mse: 0.7766 - mae: 0.4989 - root_mean_squared_error: 0.8810\n",
      "Epoch 194/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8006 - mse: 0.8006 - mae: 0.5051 - root_mean_squared_error: 0.8945\n",
      "Epoch 195/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7875 - mse: 0.7875 - mae: 0.5011 - root_mean_squared_error: 0.8872\n",
      "Epoch 196/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7765 - mse: 0.7765 - mae: 0.4967 - root_mean_squared_error: 0.8809\n",
      "Epoch 197/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7916 - mse: 0.7916 - mae: 0.5033 - root_mean_squared_error: 0.8891\n",
      "Epoch 198/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8152 - mse: 0.8152 - mae: 0.5116 - root_mean_squared_error: 0.9026\n",
      "Epoch 199/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7737 - mse: 0.7737 - mae: 0.4970 - root_mean_squared_error: 0.8791\n",
      "Epoch 200/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7918 - mse: 0.7918 - mae: 0.5071 - root_mean_squared_error: 0.8898\n",
      "Epoch 201/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7744 - mse: 0.7744 - mae: 0.4977 - root_mean_squared_error: 0.8797\n",
      "Epoch 202/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7986 - mse: 0.7986 - mae: 0.5080 - root_mean_squared_error: 0.8936\n",
      "Epoch 203/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7821 - mse: 0.7821 - mae: 0.5022 - root_mean_squared_error: 0.8840\n",
      "Epoch 204/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8103 - mse: 0.8103 - mae: 0.5102 - root_mean_squared_error: 0.8999\n",
      "Epoch 205/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7860 - mse: 0.7860 - mae: 0.5024 - root_mean_squared_error: 0.8864\n",
      "Epoch 206/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7759 - mse: 0.7759 - mae: 0.4969 - root_mean_squared_error: 0.8808\n",
      "Epoch 207/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7999 - mse: 0.7999 - mae: 0.5160 - root_mean_squared_error: 0.8942\n",
      "Epoch 208/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8033 - mse: 0.8033 - mae: 0.5057 - root_mean_squared_error: 0.8961\n",
      "Epoch 209/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8137 - mse: 0.8137 - mae: 0.5092 - root_mean_squared_error: 0.9019\n",
      "Epoch 210/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8234 - mse: 0.8234 - mae: 0.5162 - root_mean_squared_error: 0.9072\n",
      "Epoch 211/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7698 - mse: 0.7698 - mae: 0.4973 - root_mean_squared_error: 0.8772\n",
      "Epoch 212/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8075 - mse: 0.8075 - mae: 0.5120 - root_mean_squared_error: 0.8983\n",
      "Epoch 213/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7817 - mse: 0.7817 - mae: 0.5006 - root_mean_squared_error: 0.8840\n",
      "Epoch 214/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7633 - mse: 0.7633 - mae: 0.4957 - root_mean_squared_error: 0.8734\n",
      "Epoch 215/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7955 - mse: 0.7955 - mae: 0.5065 - root_mean_squared_error: 0.8916\n",
      "Epoch 216/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7796 - mse: 0.7796 - mae: 0.4980 - root_mean_squared_error: 0.8826\n",
      "Epoch 217/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7783 - mse: 0.7783 - mae: 0.4943 - root_mean_squared_error: 0.8821\n",
      "Epoch 218/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7749 - mse: 0.7749 - mae: 0.5003 - root_mean_squared_error: 0.8797\n",
      "Epoch 219/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7633 - mse: 0.7633 - mae: 0.4977 - root_mean_squared_error: 0.8735\n",
      "Epoch 220/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7894 - mse: 0.7894 - mae: 0.5028 - root_mean_squared_error: 0.8884\n",
      "Epoch 221/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8074 - mse: 0.8074 - mae: 0.5105 - root_mean_squared_error: 0.8984\n",
      "Epoch 222/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7810 - mse: 0.7810 - mae: 0.5009 - root_mean_squared_error: 0.8834\n",
      "Epoch 223/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8222 - mse: 0.8222 - mae: 0.5133 - root_mean_squared_error: 0.9066\n",
      "Epoch 224/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7952 - mse: 0.7952 - mae: 0.5062 - root_mean_squared_error: 0.8917\n",
      "Epoch 225/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8022 - mse: 0.8022 - mae: 0.5108 - root_mean_squared_error: 0.8956\n",
      "Epoch 226/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8027 - mse: 0.8027 - mae: 0.5081 - root_mean_squared_error: 0.8957\n",
      "Epoch 227/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7823 - mse: 0.7823 - mae: 0.5039 - root_mean_squared_error: 0.8843\n",
      "Epoch 228/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8001 - mse: 0.8001 - mae: 0.5066 - root_mean_squared_error: 0.8944\n",
      "Epoch 229/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8208 - mse: 0.8208 - mae: 0.5096 - root_mean_squared_error: 0.9055\n",
      "Epoch 230/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7778 - mse: 0.7778 - mae: 0.4990 - root_mean_squared_error: 0.8818\n",
      "Epoch 231/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8003 - mse: 0.8003 - mae: 0.5094 - root_mean_squared_error: 0.8945\n",
      "Epoch 232/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8043 - mse: 0.8043 - mae: 0.5085 - root_mean_squared_error: 0.8968\n",
      "Epoch 233/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8227 - mse: 0.8227 - mae: 0.5189 - root_mean_squared_error: 0.9069\n",
      "Epoch 234/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8026 - mse: 0.8026 - mae: 0.5068 - root_mean_squared_error: 0.8957\n",
      "Epoch 235/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8139 - mse: 0.8139 - mae: 0.5118 - root_mean_squared_error: 0.9021\n",
      "Epoch 236/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8045 - mse: 0.8045 - mae: 0.5109 - root_mean_squared_error: 0.8967\n",
      "Epoch 237/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7978 - mse: 0.7978 - mae: 0.5018 - root_mean_squared_error: 0.8930\n",
      "Epoch 238/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7994 - mse: 0.7994 - mae: 0.5062 - root_mean_squared_error: 0.8938\n",
      "Epoch 239/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7992 - mse: 0.7992 - mae: 0.5023 - root_mean_squared_error: 0.8938\n",
      "Epoch 240/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7978 - mse: 0.7978 - mae: 0.5021 - root_mean_squared_error: 0.8931\n",
      "Epoch 241/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7699 - mse: 0.7699 - mae: 0.4972 - root_mean_squared_error: 0.8771\n",
      "Epoch 242/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7995 - mse: 0.7995 - mae: 0.5045 - root_mean_squared_error: 0.8939\n",
      "Epoch 243/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7891 - mse: 0.7891 - mae: 0.4975 - root_mean_squared_error: 0.8882\n",
      "Epoch 244/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7981 - mse: 0.7981 - mae: 0.5049 - root_mean_squared_error: 0.8933\n",
      "Epoch 245/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8001 - mse: 0.8001 - mae: 0.5041 - root_mean_squared_error: 0.8944\n",
      "Epoch 246/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7880 - mse: 0.7880 - mae: 0.5029 - root_mean_squared_error: 0.8876\n",
      "Epoch 247/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7702 - mse: 0.7702 - mae: 0.5028 - root_mean_squared_error: 0.8774\n",
      "Epoch 248/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7781 - mse: 0.7781 - mae: 0.5027 - root_mean_squared_error: 0.8819\n",
      "Epoch 249/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8039 - mse: 0.8039 - mae: 0.5129 - root_mean_squared_error: 0.8966\n",
      "Epoch 250/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7808 - mse: 0.7808 - mae: 0.5010 - root_mean_squared_error: 0.8834\n",
      "Epoch 251/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7728 - mse: 0.7728 - mae: 0.5012 - root_mean_squared_error: 0.8787\n",
      "Epoch 252/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8070 - mse: 0.8070 - mae: 0.5089 - root_mean_squared_error: 0.8981\n",
      "Epoch 253/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7614 - mse: 0.7614 - mae: 0.4974 - root_mean_squared_error: 0.8720\n",
      "Epoch 254/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8003 - mse: 0.8003 - mae: 0.5067 - root_mean_squared_error: 0.8944\n",
      "Epoch 255/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8058 - mse: 0.8058 - mae: 0.5096 - root_mean_squared_error: 0.8976\n",
      "Epoch 256/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7722 - mse: 0.7722 - mae: 0.4971 - root_mean_squared_error: 0.8786\n",
      "Epoch 257/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8036 - mse: 0.8036 - mae: 0.5105 - root_mean_squared_error: 0.8961\n",
      "Epoch 258/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7700 - mse: 0.7700 - mae: 0.4971 - root_mean_squared_error: 0.8774\n",
      "Epoch 259/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7816 - mse: 0.7816 - mae: 0.5036 - root_mean_squared_error: 0.8840\n",
      "Epoch 260/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8122 - mse: 0.8122 - mae: 0.5146 - root_mean_squared_error: 0.9012\n",
      "Epoch 261/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8002 - mse: 0.8002 - mae: 0.5094 - root_mean_squared_error: 0.8943\n",
      "Epoch 262/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7958 - mse: 0.7958 - mae: 0.5025 - root_mean_squared_error: 0.8917\n",
      "Epoch 263/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7969 - mse: 0.7969 - mae: 0.5131 - root_mean_squared_error: 0.8926\n",
      "Epoch 264/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7746 - mse: 0.7746 - mae: 0.4991 - root_mean_squared_error: 0.8799\n",
      "Epoch 265/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7786 - mse: 0.7786 - mae: 0.4979 - root_mean_squared_error: 0.8822\n",
      "Epoch 266/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8010 - mse: 0.8010 - mae: 0.5101 - root_mean_squared_error: 0.8949\n",
      "Epoch 267/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7752 - mse: 0.7752 - mae: 0.5009 - root_mean_squared_error: 0.8803\n",
      "Epoch 268/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7845 - mse: 0.7845 - mae: 0.5048 - root_mean_squared_error: 0.8857\n",
      "Epoch 269/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7932 - mse: 0.7932 - mae: 0.5072 - root_mean_squared_error: 0.8903\n",
      "Epoch 270/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7943 - mse: 0.7943 - mae: 0.5056 - root_mean_squared_error: 0.8911\n",
      "Epoch 271/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7557 - mse: 0.7557 - mae: 0.4969 - root_mean_squared_error: 0.8690\n",
      "Epoch 272/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8202 - mse: 0.8202 - mae: 0.5162 - root_mean_squared_error: 0.9055\n",
      "Epoch 273/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7776 - mse: 0.7776 - mae: 0.5006 - root_mean_squared_error: 0.8814\n",
      "Epoch 274/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8162 - mse: 0.8162 - mae: 0.5132 - root_mean_squared_error: 0.9031\n",
      "Epoch 275/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7588 - mse: 0.7588 - mae: 0.4904 - root_mean_squared_error: 0.8705\n",
      "Epoch 276/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8139 - mse: 0.8139 - mae: 0.5142 - root_mean_squared_error: 0.9018\n",
      "Epoch 277/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7971 - mse: 0.7971 - mae: 0.5052 - root_mean_squared_error: 0.8921\n",
      "Epoch 278/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7793 - mse: 0.7793 - mae: 0.4994 - root_mean_squared_error: 0.8826\n",
      "Epoch 279/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7927 - mse: 0.7927 - mae: 0.5030 - root_mean_squared_error: 0.8902\n",
      "Epoch 280/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7939 - mse: 0.7939 - mae: 0.5079 - root_mean_squared_error: 0.8910\n",
      "Epoch 281/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7866 - mse: 0.7866 - mae: 0.5025 - root_mean_squared_error: 0.8868\n",
      "Epoch 282/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7757 - mse: 0.7757 - mae: 0.4994 - root_mean_squared_error: 0.8802\n",
      "Epoch 283/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7611 - mse: 0.7611 - mae: 0.4997 - root_mean_squared_error: 0.8717\n",
      "Epoch 284/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7991 - mse: 0.7991 - mae: 0.5060 - root_mean_squared_error: 0.8938\n",
      "Epoch 285/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7895 - mse: 0.7895 - mae: 0.5042 - root_mean_squared_error: 0.8885\n",
      "Epoch 286/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7763 - mse: 0.7763 - mae: 0.5032 - root_mean_squared_error: 0.8807\n",
      "Epoch 287/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8005 - mse: 0.8005 - mae: 0.5103 - root_mean_squared_error: 0.8946\n",
      "Epoch 288/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8028 - mse: 0.8028 - mae: 0.5122 - root_mean_squared_error: 0.8959\n",
      "Epoch 289/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7829 - mse: 0.7829 - mae: 0.5033 - root_mean_squared_error: 0.8847\n",
      "Epoch 290/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8062 - mse: 0.8062 - mae: 0.5076 - root_mean_squared_error: 0.8976\n",
      "Epoch 291/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8000 - mse: 0.8000 - mae: 0.5111 - root_mean_squared_error: 0.8941\n",
      "Epoch 292/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7931 - mse: 0.7931 - mae: 0.5073 - root_mean_squared_error: 0.8905\n",
      "Epoch 293/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7939 - mse: 0.7939 - mae: 0.5034 - root_mean_squared_error: 0.8909\n",
      "Epoch 294/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8133 - mse: 0.8133 - mae: 0.5086 - root_mean_squared_error: 0.9016\n",
      "Epoch 295/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8067 - mse: 0.8067 - mae: 0.5092 - root_mean_squared_error: 0.8979\n",
      "Epoch 296/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7914 - mse: 0.7914 - mae: 0.5061 - root_mean_squared_error: 0.8895\n",
      "Epoch 297/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7818 - mse: 0.7818 - mae: 0.4995 - root_mean_squared_error: 0.8840\n",
      "Epoch 298/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8056 - mse: 0.8056 - mae: 0.5092 - root_mean_squared_error: 0.8974\n",
      "Epoch 299/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7634 - mse: 0.7634 - mae: 0.4952 - root_mean_squared_error: 0.8735\n",
      "Epoch 300/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7696 - mse: 0.7696 - mae: 0.4964 - root_mean_squared_error: 0.8770\n",
      "Epoch 301/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7982 - mse: 0.7982 - mae: 0.5061 - root_mean_squared_error: 0.8933\n",
      "Epoch 302/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7586 - mse: 0.7586 - mae: 0.4910 - root_mean_squared_error: 0.8702\n",
      "Epoch 303/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7746 - mse: 0.7746 - mae: 0.4974 - root_mean_squared_error: 0.8799\n",
      "Epoch 304/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8083 - mse: 0.8083 - mae: 0.5056 - root_mean_squared_error: 0.8990\n",
      "Epoch 305/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8265 - mse: 0.8265 - mae: 0.5119 - root_mean_squared_error: 0.9084\n",
      "Epoch 306/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7863 - mse: 0.7863 - mae: 0.5016 - root_mean_squared_error: 0.8864\n",
      "Epoch 307/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7726 - mse: 0.7726 - mae: 0.5039 - root_mean_squared_error: 0.8786\n",
      "Epoch 308/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7895 - mse: 0.7895 - mae: 0.5078 - root_mean_squared_error: 0.8884\n",
      "Epoch 309/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8008 - mse: 0.8008 - mae: 0.5040 - root_mean_squared_error: 0.8948\n",
      "Epoch 310/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7747 - mse: 0.7747 - mae: 0.4988 - root_mean_squared_error: 0.8801\n",
      "Epoch 311/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7771 - mse: 0.7771 - mae: 0.4972 - root_mean_squared_error: 0.8813\n",
      "Epoch 312/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7901 - mse: 0.7901 - mae: 0.5058 - root_mean_squared_error: 0.8886\n",
      "Epoch 313/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7625 - mse: 0.7625 - mae: 0.4978 - root_mean_squared_error: 0.8729\n",
      "Epoch 314/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7930 - mse: 0.7930 - mae: 0.5070 - root_mean_squared_error: 0.8904\n",
      "Epoch 315/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8059 - mse: 0.8059 - mae: 0.5082 - root_mean_squared_error: 0.8976\n",
      "Epoch 316/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7647 - mse: 0.7647 - mae: 0.4928 - root_mean_squared_error: 0.8737\n",
      "Epoch 317/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7783 - mse: 0.7783 - mae: 0.5062 - root_mean_squared_error: 0.8819\n",
      "Epoch 318/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7720 - mse: 0.7720 - mae: 0.4947 - root_mean_squared_error: 0.8784\n",
      "Epoch 319/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8023 - mse: 0.8023 - mae: 0.5051 - root_mean_squared_error: 0.8955\n",
      "Epoch 320/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7764 - mse: 0.7764 - mae: 0.4989 - root_mean_squared_error: 0.8808\n",
      "Epoch 321/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7931 - mse: 0.7931 - mae: 0.5085 - root_mean_squared_error: 0.8904\n",
      "Epoch 322/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7985 - mse: 0.7985 - mae: 0.5030 - root_mean_squared_error: 0.8935\n",
      "Epoch 323/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8008 - mse: 0.8008 - mae: 0.5048 - root_mean_squared_error: 0.8948\n",
      "Epoch 324/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8271 - mse: 0.8271 - mae: 0.5136 - root_mean_squared_error: 0.9093\n",
      "Epoch 325/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8054 - mse: 0.8054 - mae: 0.5100 - root_mean_squared_error: 0.8973\n",
      "Epoch 326/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7937 - mse: 0.7937 - mae: 0.5080 - root_mean_squared_error: 0.8908\n",
      "Epoch 327/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8137 - mse: 0.8137 - mae: 0.5096 - root_mean_squared_error: 0.9016\n",
      "Epoch 328/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7825 - mse: 0.7825 - mae: 0.4969 - root_mean_squared_error: 0.8845\n",
      "Epoch 329/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7759 - mse: 0.7759 - mae: 0.4945 - root_mean_squared_error: 0.8806\n",
      "Epoch 330/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7666 - mse: 0.7666 - mae: 0.4977 - root_mean_squared_error: 0.8745\n",
      "Epoch 331/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7902 - mse: 0.7902 - mae: 0.5028 - root_mean_squared_error: 0.8888\n",
      "Epoch 332/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7711 - mse: 0.7711 - mae: 0.4979 - root_mean_squared_error: 0.8780\n",
      "Epoch 333/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7821 - mse: 0.7821 - mae: 0.5055 - root_mean_squared_error: 0.8843\n",
      "Epoch 334/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7888 - mse: 0.7888 - mae: 0.5043 - root_mean_squared_error: 0.8880\n",
      "Epoch 335/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8080 - mse: 0.8080 - mae: 0.5086 - root_mean_squared_error: 0.8988\n",
      "Epoch 336/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7935 - mse: 0.7935 - mae: 0.5021 - root_mean_squared_error: 0.8905\n",
      "Epoch 337/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7803 - mse: 0.7803 - mae: 0.4942 - root_mean_squared_error: 0.8828\n",
      "Epoch 338/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7913 - mse: 0.7913 - mae: 0.5055 - root_mean_squared_error: 0.8895\n",
      "Epoch 339/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8074 - mse: 0.8074 - mae: 0.5021 - root_mean_squared_error: 0.8981\n",
      "Epoch 340/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7800 - mse: 0.7800 - mae: 0.5030 - root_mean_squared_error: 0.8830\n",
      "Epoch 341/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8043 - mse: 0.8043 - mae: 0.5100 - root_mean_squared_error: 0.8966\n",
      "Epoch 342/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8076 - mse: 0.8076 - mae: 0.5131 - root_mean_squared_error: 0.8985\n",
      "Epoch 343/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8013 - mse: 0.8013 - mae: 0.5003 - root_mean_squared_error: 0.8939\n",
      "Epoch 344/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7980 - mse: 0.7980 - mae: 0.5116 - root_mean_squared_error: 0.8932\n",
      "Epoch 345/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8078 - mse: 0.8078 - mae: 0.5077 - root_mean_squared_error: 0.8986\n",
      "Epoch 346/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8116 - mse: 0.8116 - mae: 0.5060 - root_mean_squared_error: 0.9005\n",
      "Epoch 347/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7969 - mse: 0.7969 - mae: 0.5078 - root_mean_squared_error: 0.8925\n",
      "Epoch 348/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7747 - mse: 0.7747 - mae: 0.4985 - root_mean_squared_error: 0.8800\n",
      "Epoch 349/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7751 - mse: 0.7751 - mae: 0.5046 - root_mean_squared_error: 0.8802\n",
      "Epoch 350/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7892 - mse: 0.7892 - mae: 0.5021 - root_mean_squared_error: 0.8883\n",
      "Epoch 351/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8116 - mse: 0.8116 - mae: 0.5076 - root_mean_squared_error: 0.9008\n",
      "Epoch 352/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7846 - mse: 0.7846 - mae: 0.5017 - root_mean_squared_error: 0.8855\n",
      "Epoch 353/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7579 - mse: 0.7579 - mae: 0.4934 - root_mean_squared_error: 0.8702\n",
      "Epoch 354/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8067 - mse: 0.8067 - mae: 0.5082 - root_mean_squared_error: 0.8980\n",
      "Epoch 355/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7907 - mse: 0.7907 - mae: 0.5069 - root_mean_squared_error: 0.8890\n",
      "Epoch 356/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7955 - mse: 0.7955 - mae: 0.5042 - root_mean_squared_error: 0.8917\n",
      "Epoch 357/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8252 - mse: 0.8252 - mae: 0.5134 - root_mean_squared_error: 0.9080\n",
      "Epoch 358/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7837 - mse: 0.7837 - mae: 0.5029 - root_mean_squared_error: 0.8850\n",
      "Epoch 359/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7901 - mse: 0.7901 - mae: 0.5033 - root_mean_squared_error: 0.8888\n",
      "Epoch 360/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8051 - mse: 0.8051 - mae: 0.5091 - root_mean_squared_error: 0.8970\n",
      "Epoch 361/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8091 - mse: 0.8091 - mae: 0.5033 - root_mean_squared_error: 0.8991\n",
      "Epoch 362/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7931 - mse: 0.7931 - mae: 0.5025 - root_mean_squared_error: 0.8905\n",
      "Epoch 363/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7866 - mse: 0.7866 - mae: 0.5049 - root_mean_squared_error: 0.8867\n",
      "Epoch 364/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7735 - mse: 0.7735 - mae: 0.4988 - root_mean_squared_error: 0.8793\n",
      "Epoch 365/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8207 - mse: 0.8207 - mae: 0.5147 - root_mean_squared_error: 0.9057\n",
      "Epoch 366/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8211 - mse: 0.8211 - mae: 0.5153 - root_mean_squared_error: 0.9060\n",
      "Epoch 367/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7746 - mse: 0.7746 - mae: 0.4979 - root_mean_squared_error: 0.8799\n",
      "Epoch 368/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7836 - mse: 0.7836 - mae: 0.5049 - root_mean_squared_error: 0.8850\n",
      "Epoch 369/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7892 - mse: 0.7892 - mae: 0.5038 - root_mean_squared_error: 0.8882\n",
      "Epoch 370/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7910 - mse: 0.7910 - mae: 0.5062 - root_mean_squared_error: 0.8892\n",
      "Epoch 371/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7709 - mse: 0.7709 - mae: 0.4957 - root_mean_squared_error: 0.8777\n",
      "Epoch 372/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7906 - mse: 0.7906 - mae: 0.5030 - root_mean_squared_error: 0.8889\n",
      "Epoch 373/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8359 - mse: 0.8359 - mae: 0.5171 - root_mean_squared_error: 0.9137\n",
      "Epoch 374/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7958 - mse: 0.7958 - mae: 0.5054 - root_mean_squared_error: 0.8917\n",
      "Epoch 375/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8132 - mse: 0.8132 - mae: 0.5116 - root_mean_squared_error: 0.9012\n",
      "Epoch 376/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8180 - mse: 0.8180 - mae: 0.5166 - root_mean_squared_error: 0.9042\n",
      "Epoch 377/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8002 - mse: 0.8002 - mae: 0.5101 - root_mean_squared_error: 0.8944\n",
      "Epoch 378/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7746 - mse: 0.7746 - mae: 0.5036 - root_mean_squared_error: 0.8800\n",
      "Epoch 379/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8078 - mse: 0.8078 - mae: 0.5083 - root_mean_squared_error: 0.8986\n",
      "Epoch 380/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7949 - mse: 0.7949 - mae: 0.5019 - root_mean_squared_error: 0.8915\n",
      "Epoch 381/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7777 - mse: 0.7777 - mae: 0.5034 - root_mean_squared_error: 0.8813\n",
      "Epoch 382/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8195 - mse: 0.8195 - mae: 0.5098 - root_mean_squared_error: 0.9044\n",
      "Epoch 383/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8289 - mse: 0.8289 - mae: 0.5166 - root_mean_squared_error: 0.9102\n",
      "Epoch 384/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7884 - mse: 0.7884 - mae: 0.5007 - root_mean_squared_error: 0.8878\n",
      "Epoch 385/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7783 - mse: 0.7783 - mae: 0.5009 - root_mean_squared_error: 0.8817\n",
      "Epoch 386/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8281 - mse: 0.8281 - mae: 0.5141 - root_mean_squared_error: 0.9099\n",
      "Epoch 387/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8113 - mse: 0.8113 - mae: 0.5116 - root_mean_squared_error: 0.9005\n",
      "Epoch 388/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7984 - mse: 0.7984 - mae: 0.5078 - root_mean_squared_error: 0.8934\n",
      "Epoch 389/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7897 - mse: 0.7897 - mae: 0.5028 - root_mean_squared_error: 0.8886\n",
      "Epoch 390/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8027 - mse: 0.8027 - mae: 0.5090 - root_mean_squared_error: 0.8958\n",
      "Epoch 391/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7925 - mse: 0.7925 - mae: 0.5098 - root_mean_squared_error: 0.8902\n",
      "Epoch 392/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7652 - mse: 0.7652 - mae: 0.4966 - root_mean_squared_error: 0.8745\n",
      "Epoch 393/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8080 - mse: 0.8080 - mae: 0.5109 - root_mean_squared_error: 0.8988\n",
      "Epoch 394/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8236 - mse: 0.8236 - mae: 0.5141 - root_mean_squared_error: 0.9073\n",
      "Epoch 395/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8074 - mse: 0.8074 - mae: 0.5086 - root_mean_squared_error: 0.8984\n",
      "Epoch 396/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7789 - mse: 0.7789 - mae: 0.4991 - root_mean_squared_error: 0.8824\n",
      "Epoch 397/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7940 - mse: 0.7940 - mae: 0.5068 - root_mean_squared_error: 0.8909\n",
      "Epoch 398/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7960 - mse: 0.7960 - mae: 0.5074 - root_mean_squared_error: 0.8921\n",
      "Epoch 399/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.8120 - mse: 0.8120 - mae: 0.5090 - root_mean_squared_error: 0.9009\n",
      "Epoch 400/400\n",
      "233/233 [==============================] - 0s 1ms/step - loss: 0.7838 - mse: 0.7838 - mae: 0.5043 - root_mean_squared_error: 0.8852\n",
      "computed in 0:01:51.917585 s\n"
     ]
    }
   ],
   "source": [
    "batch_input_shape = (1, n_samples, n_features)\n",
    "dense_model = get_dense_model(batch_input_shape)\n",
    "dense_model.summary()\n",
    "start = time.time()\n",
    "dense_model.fit(X_train, Y_train, epochs=400, verbose=1)\n",
    "end = time.time() - start\n",
    "print(f\"computed in {str(str(timedelta(seconds=end)))} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predicted = np.squeeze(dense_model.predict(X_test, batch_size=1))\n",
    "Y_predicted_unscaled = dg.inverse_transform_y(Y_predicted, idx=test_idx)\n",
    "\n",
    "print(np.mean(Y_predicted - Y_test, axis=0))\n",
    "print(np.mean(Y_predicted_unscaled - Y_test_unscaled, axis=0))\n",
    "\n",
    "Y_predicted_unpadded_unscaled =  dg.remove_padded_y(Y_predicted_unscaled, idx=test_idx)\n",
    "\n",
    "print(mean_absolute_error(Y_predicted_unpadded_unscaled, Y_test_unpadded_unscaled, multioutput=\"raw_values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_vs_actual(df_y_real, df_y_predicted, prediction_dates, cur_loc, horizon=1, mode=0):\n",
    "    \"\"\"\n",
    "    plot the prediction done on a specific horizon along with the last points of data before\n",
    "    \"\"\"\n",
    "    if not predict_one:\n",
    "        if mode == 0:\n",
    "            horizon_range = range(horizon, horizon+1)\n",
    "        elif mode == 1:\n",
    "            horizon_range = range(1, horizon+1)\n",
    "    else:\n",
    "        horizon_range = [1]\n",
    "\n",
    "    df_real = df_y_real[cur_loc]\n",
    "    df_pred = df_y_predicted[cur_loc]\n",
    "    prediction_dates = prediction_dates[:, horizon]\n",
    "    \n",
    "    for horizon in horizon_range:\n",
    "        fig = plt.figure(figsize=(6,3))\n",
    "        expected = df_real[f\"{target}(t+{horizon})\"].values\n",
    "        plt.plot(prediction_dates, expected, marker=\".\", label=\"True value\")\n",
    "        prediction = df_pred[f\"{target}(t+{horizon})\"].values\n",
    "\n",
    "        if not predict_one:\n",
    "            plot_label = f\"Prediction horizon {horizon}\"\n",
    "        else:\n",
    "            plot_label = f\"Prediction horizon {n_forecast}\"\n",
    "        plt.plot(prediction_dates, prediction, marker='.', label=plot_label)\n",
    "\n",
    "        ax = fig.axes[0]\n",
    "        # set locator\n",
    "        ax.xaxis.set_major_locator(mdates.DayLocator(interval=7))\n",
    "        # set formatter\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y'))\n",
    "        # set font and rotation for date tick labels\n",
    "        plt.gcf().autofmt_xdate()\n",
    "        plt.title(f\"Plot true and predicted values for {cur_loc}\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationLogger(ProgbarLogger):\n",
    "    \"\"\"\n",
    "    compute metrics on the validation set, using a different batch size than the training set\n",
    "    at the end of each epoch, the weights of the training model are used to set the validation model\n",
    "    the error is computed based on the unscaled and unpadded values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, validation_model, val_batch_size, X_val, Y_val, val_idx, val_geo):\n",
    "        \"\"\"\n",
    "        :param validation_model: validation model ready to be used\n",
    "        :param X_val: X validation set\n",
    "        :param Y_val: Y validation set, already unscaled and unpadded\n",
    "        :param val_idx: validation idx\n",
    "        :param val_geo: validation geoloc\n",
    "        \"\"\"\n",
    "        super(ValidationLogger, self).__init__(count_mode='steps')\n",
    "        self.validation_model = validation_model\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.X_val = X_val\n",
    "        self.Y_val = Y_val\n",
    "        self.val_idx = val_idx\n",
    "        self.val_geo = val_geo\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        at the end of each epoch, compute the metrics asked on the validation set\n",
    "        \"\"\"        \n",
    "        self.validation_model.set_weights(self.model.get_weights())\n",
    "        Y_pred = self.validation_model.predict(self.X_val, batch_size=self.val_batch_size)\n",
    "        Y_pred = dg.inverse_transform_y(Y_pred, geo=self.val_geo, idx=self.val_idx)\n",
    "        Y_pred = dg.remove_padded_y(Y_pred, geo=self.val_geo, idx=self.val_idx)\n",
    "        for metric in validation_metrics:\n",
    "            metric.reset_states()\n",
    "            metric.update_state(Y_pred, self.Y_val)\n",
    "            logs[f\"val_{metric.name}\"] = metric.result().numpy()  # prepend name for validation set\n",
    "        super(ValidationLogger, self).on_epoch_end(epoch, logs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(Y_expected, Y_actual) -> pd.DataFrame:\n",
    "    MAE = mean_absolute_error(Y_expected, Y_actual, multioutput=\"raw_values\")\n",
    "    MSE = mean_squared_error(Y_expected, Y_actual, multioutput=\"raw_values\")\n",
    "    values = {}\n",
    "    for t in range(n_forecast):\n",
    "        values[f'MAE(t+{t+1})'] = [MAE[t]]\n",
    "        values[f'MSE(t+{t+1})'] = [MSE[t]]\n",
    "    return pd.DataFrame(values)\n",
    "\n",
    "\n",
    "def walk_forward_evaluation(model_generator: callable, nb_fit_first: int, nb_validation: int, nb_test: int, \n",
    "                            epochs: int = 400, plot=False, verbose: int = 1,\n",
    "                            return_history: bool = False, batch_size_fun: callable = None,\n",
    "                            es_stop_val: bool = False\n",
    "                           ) -> Union[pd.DataFrame, List[History]]:\n",
    "    \"\"\"\n",
    "    evaluates a model using a walk forward evaluation: multiples fit are done, each followed by at most nb_test\n",
    "    to evaluate the model\n",
    "    :param model_generator: function returning the model to evaluate\n",
    "    :param nb_fit_first: number of datapoints used for the first fit\n",
    "    :param nb_validation: number of datapoints used for each evaluation set\n",
    "    :param nb_test: number of datapoints used for the test set (at most)\n",
    "    :param epochs: number of epochs used on each fit\n",
    "    :param callbacks: list of callbacks to use\n",
    "    :param verbose: verbose level. Passed to fit and used to display the error dataframe\n",
    "    :param return_history: if True, returns the list of history of each walk\n",
    "    :param batch_size_fun: function used to compute the batch size based on the X_train tensor\n",
    "        if not specified, default to batch_size = len(train_idx)\n",
    "    \"\"\"\n",
    "    # initial index used\n",
    "    max_len = dg.batch_size\n",
    "    train_idx = np.arange(nb_fit_first)\n",
    "    valid_idx = np.arange(nb_fit_first, nb_fit_first + nb_validation)\n",
    "    if nb_fit_first + nb_validation >= max_len:\n",
    "        finished = True  # no test can be done\n",
    "    else:\n",
    "        finished = False  # a test set can be created\n",
    "        test_idx = np.arange(nb_fit_first + nb_validation, min(nb_fit_first + nb_validation + nb_test, max_len))\n",
    "    df_error = pd.DataFrame()\n",
    "    walk = 0\n",
    "    last_iter = False  # True when the last iteration is reached\n",
    "    all_history = []\n",
    "    while not finished:\n",
    "        X_train = dg.get_x(train_idx)\n",
    "        Y_train = dg.get_y(train_idx)\n",
    "        if batch_size_fun is None:\n",
    "            batch_size = len(train_idx)\n",
    "        else:\n",
    "            batch_size = batch_size_fun(X_train)\n",
    "            \n",
    "        if len(valid_idx) > 0:\n",
    "            X_val = dg.get_x(valid_idx, geo=dg.loc_init)\n",
    "            Y_val_real = dg.get_y(valid_idx, geo=dg.loc_init, scaled=False)\n",
    "            Y_val_real = dg.remove_padded_y(Y_val_real, geo=dg.loc_init, idx=valid_idx)\n",
    "            batch_size_val = len(X_val)\n",
    "            model_validation = model_generator(batch_input_shape=(batch_size_val, n_samples, n_features))\n",
    "            val_log = ValidationLogger(model_validation, len(X_val), X_val, Y_val_real, valid_idx, dg.loc_init)\n",
    "            if es_stop_val:\n",
    "                callbacks = [val_log, EarlyStopping(monitor=\"val_root_mean_squared_error\", patience=25)]\n",
    "            else:\n",
    "                callbacks = [val_log]\n",
    "        else:\n",
    "            callbacks = None\n",
    "            \n",
    "        model_train = model_generator(batch_input_shape=(batch_size, n_samples, n_features))\n",
    "\n",
    "        history = model_train.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=verbose)\n",
    "        \n",
    "        # test only on the unaugmented data without padding\n",
    "        batch_size = len(test_idx)\n",
    "        # construct a new model and use the previous weights to set it\n",
    "        model_pred = model_generator(batch_input_shape=(batch_size, n_samples, n_features))\n",
    "        model_pred.set_weights(model_train.get_weights())\n",
    "        X_test = dg.get_x(test_idx, geo=dg.loc_init, use_previous_scaler=True)\n",
    "        Y_test = dg.get_y(test_idx, geo=dg.loc_init, use_previous_scaler=True)\n",
    "        Y_test_real = dg.get_y(test_idx, geo=dg.loc_init, scaled=False)\n",
    "        Y_test_real = dg.remove_padded_y(Y_test_real, geo=dg.loc_init, idx=test_idx)\n",
    "        Y_predicted = model_pred.predict(X_test, batch_size=batch_size)\n",
    "        Y_predicted_real = dg.inverse_transform_y(Y_predicted, geo=dg.loc_init, idx=test_idx)\n",
    "        Y_predicted_real = dg.remove_padded_y(Y_predicted_real, geo=dg.loc_init, idx=test_idx)        \n",
    "        if return_history:\n",
    "            # add test metrics to the history for this walk, based on the unpadded unscaled data \n",
    "            # except for the loss\n",
    "            for metric in model_train.metrics:\n",
    "                metric.reset_states()\n",
    "                if metric.name == 'loss':\n",
    "                    metric.update_state(Y_predicted_real, Y_test_real)\n",
    "                else:\n",
    "                    # compute metric accross each horizon\n",
    "                    for i in range(n_forecast):\n",
    "                        metric.update_state(Y_predicted_real[:, i], Y_test_real[:, i])\n",
    "                        history.history[f\"test_{metric.name}(t+{i+1})\"] = [metric.result().numpy()]\n",
    "                        metric.reset_states()\n",
    "                    # compute mean of metric on all horizon\n",
    "                    metric.update_state(Y_predicted_real, Y_test_real)\n",
    "                # prepend name for test set\n",
    "                history.history[f\"test_{metric.name}\"] = [metric.result().numpy()]\n",
    "            # add number of unpadded datapoints\n",
    "            history.history['nb_test_datapoints'] = [len(Y_test_real)]\n",
    "            all_history.append(history)\n",
    "        if not return_history or verbose != 0:\n",
    "            # compute the error using the unpadded and unscaled data\n",
    "            error = compute_error(Y_test_real, Y_predicted_real)\n",
    "            name = f'walk {walk + 1}'\n",
    "            error['walk'] = name\n",
    "            error['nb_test_datapoints'] = len(Y_test_real)\n",
    "            error['days_train'] = len(train_idx)\n",
    "            error['days_valid'] = len(valid_idx)\n",
    "            error['days_test'] = len(test_idx)\n",
    "            error = error.set_index('walk')\n",
    "            df_error = df_error.append(error)\n",
    "        if verbose != 0:\n",
    "            display(df_error)\n",
    "        if last_iter:\n",
    "            finished = True\n",
    "        # indexes for next fit\n",
    "        train_idx = np.arange(train_idx[-1] + 1 + nb_test)\n",
    "        valid_idx += nb_test\n",
    "        if test_idx[-1] + nb_test >= max_len:  # last iteration, less points can be used for the test set\n",
    "            last_iter = True  # last iteration to be done\n",
    "            test_idx = np.arange(test_idx[-1], max_len)\n",
    "        else:\n",
    "            test_idx += nb_test\n",
    "        walk += 1\n",
    "    return df_error if not return_history else all_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 1.6259 - mse: 1.6259 - mae: 0.9620 - root_mean_squared_error: 1.2751 - val_mean_squared_error: 2473278.5000 - val_mean_absolute_error: 924.1101 - val_root_mean_squared_error: 1572.6660\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.5144 - mse: 1.5144 - mae: 0.9266 - root_mean_squared_error: 1.2306 - val_mean_squared_error: 2414139.7500 - val_mean_absolute_error: 913.9866 - val_root_mean_squared_error: 1553.7502\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.4413 - mse: 1.4413 - mae: 0.9033 - root_mean_squared_error: 1.2005 - val_mean_squared_error: 2364063.7500 - val_mean_absolute_error: 905.1722 - val_root_mean_squared_error: 1537.5513\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3843 - mse: 1.3843 - mae: 0.8849 - root_mean_squared_error: 1.1766 - val_mean_squared_error: 2318699.0000 - val_mean_absolute_error: 897.0258 - val_root_mean_squared_error: 1522.7274\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.3365 - mse: 1.3365 - mae: 0.8693 - root_mean_squared_error: 1.1561 - val_mean_squared_error: 2277503.0000 - val_mean_absolute_error: 889.4780 - val_root_mean_squared_error: 1509.1398\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2947 - mse: 1.2947 - mae: 0.8556 - root_mean_squared_error: 1.1379 - val_mean_squared_error: 2238743.0000 - val_mean_absolute_error: 882.2722 - val_root_mean_squared_error: 1496.2430\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2574 - mse: 1.2574 - mae: 0.8432 - root_mean_squared_error: 1.1213 - val_mean_squared_error: 2202158.5000 - val_mean_absolute_error: 875.5283 - val_root_mean_squared_error: 1483.9673\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2234 - mse: 1.2234 - mae: 0.8318 - root_mean_squared_error: 1.1061 - val_mean_squared_error: 2167569.5000 - val_mean_absolute_error: 869.0909 - val_root_mean_squared_error: 1472.2667\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1920 - mse: 1.1920 - mae: 0.8212 - root_mean_squared_error: 1.0918 - val_mean_squared_error: 2134420.0000 - val_mean_absolute_error: 862.8420 - val_root_mean_squared_error: 1460.9655\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1629 - mse: 1.1629 - mae: 0.8112 - root_mean_squared_error: 1.0784 - val_mean_squared_error: 2102521.2500 - val_mean_absolute_error: 856.7518 - val_root_mean_squared_error: 1450.0073\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.1356 - mse: 1.1356 - mae: 0.8018 - root_mean_squared_error: 1.0656 - val_mean_squared_error: 2071944.3750 - val_mean_absolute_error: 850.9165 - val_root_mean_squared_error: 1439.4250\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1098 - mse: 1.1098 - mae: 0.7929 - root_mean_squared_error: 1.0535 - val_mean_squared_error: 2042298.7500 - val_mean_absolute_error: 845.1887 - val_root_mean_squared_error: 1429.0902\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0854 - mse: 1.0854 - mae: 0.7843 - root_mean_squared_error: 1.0418 - val_mean_squared_error: 2013175.3750 - val_mean_absolute_error: 839.4856 - val_root_mean_squared_error: 1418.8641\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0622 - mse: 1.0622 - mae: 0.7761 - root_mean_squared_error: 1.0306 - val_mean_squared_error: 1985248.1250 - val_mean_absolute_error: 833.9802 - val_root_mean_squared_error: 1408.9883\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0400 - mse: 1.0400 - mae: 0.7682 - root_mean_squared_error: 1.0198 - val_mean_squared_error: 1957966.7500 - val_mean_absolute_error: 828.5533 - val_root_mean_squared_error: 1399.2737\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0188 - mse: 1.0188 - mae: 0.7606 - root_mean_squared_error: 1.0093 - val_mean_squared_error: 1931197.5000 - val_mean_absolute_error: 823.1849 - val_root_mean_squared_error: 1389.6752\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9984 - mse: 0.9984 - mae: 0.7533 - root_mean_squared_error: 0.9992 - val_mean_squared_error: 1904845.8750 - val_mean_absolute_error: 817.9118 - val_root_mean_squared_error: 1380.1615\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9788 - mse: 0.9788 - mae: 0.7461 - root_mean_squared_error: 0.9894 - val_mean_squared_error: 1879176.7500 - val_mean_absolute_error: 812.6947 - val_root_mean_squared_error: 1370.8307\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9599 - mse: 0.9599 - mae: 0.7392 - root_mean_squared_error: 0.9798 - val_mean_squared_error: 1853993.8750 - val_mean_absolute_error: 807.5350 - val_root_mean_squared_error: 1361.6144\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9417 - mse: 0.9417 - mae: 0.7325 - root_mean_squared_error: 0.9704 - val_mean_squared_error: 1829279.2500 - val_mean_absolute_error: 802.4283 - val_root_mean_squared_error: 1352.5085\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9241 - mse: 0.9241 - mae: 0.7259 - root_mean_squared_error: 0.9613 - val_mean_squared_error: 1804851.6250 - val_mean_absolute_error: 797.3234 - val_root_mean_squared_error: 1343.4475\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9071 - mse: 0.9071 - mae: 0.7195 - root_mean_squared_error: 0.9524 - val_mean_squared_error: 1780689.0000 - val_mean_absolute_error: 792.2099 - val_root_mean_squared_error: 1334.4246\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8906 - mse: 0.8906 - mae: 0.7132 - root_mean_squared_error: 0.9437 - val_mean_squared_error: 1756955.6250 - val_mean_absolute_error: 787.1123 - val_root_mean_squared_error: 1325.5021\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8746 - mse: 0.8746 - mae: 0.7071 - root_mean_squared_error: 0.9352 - val_mean_squared_error: 1733837.5000 - val_mean_absolute_error: 782.1378 - val_root_mean_squared_error: 1316.7527\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8590 - mse: 0.8590 - mae: 0.7010 - root_mean_squared_error: 0.9268 - val_mean_squared_error: 1710748.0000 - val_mean_absolute_error: 777.1219 - val_root_mean_squared_error: 1307.9556\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8439 - mse: 0.8439 - mae: 0.6951 - root_mean_squared_error: 0.9186 - val_mean_squared_error: 1688006.8750 - val_mean_absolute_error: 772.1375 - val_root_mean_squared_error: 1299.2330\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8292 - mse: 0.8292 - mae: 0.6893 - root_mean_squared_error: 0.9106 - val_mean_squared_error: 1665436.5000 - val_mean_absolute_error: 767.1584 - val_root_mean_squared_error: 1290.5179\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.8149 - mse: 0.8149 - mae: 0.6836 - root_mean_squared_error: 0.9027 - val_mean_squared_error: 1643331.5000 - val_mean_absolute_error: 762.2327 - val_root_mean_squared_error: 1281.9249\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8009 - mse: 0.8009 - mae: 0.6780 - root_mean_squared_error: 0.8950 - val_mean_squared_error: 1621274.8750 - val_mean_absolute_error: 757.2882 - val_root_mean_squared_error: 1273.2930\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7874 - mse: 0.7874 - mae: 0.6725 - root_mean_squared_error: 0.8873 - val_mean_squared_error: 1599957.2500 - val_mean_absolute_error: 752.4109 - val_root_mean_squared_error: 1264.8940\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7741 - mse: 0.7741 - mae: 0.6671 - root_mean_squared_error: 0.8798 - val_mean_squared_error: 1578574.5000 - val_mean_absolute_error: 747.5342 - val_root_mean_squared_error: 1256.4132\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7612 - mse: 0.7612 - mae: 0.6617 - root_mean_squared_error: 0.8725 - val_mean_squared_error: 1557291.0000 - val_mean_absolute_error: 742.6270 - val_root_mean_squared_error: 1247.9147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7486 - mse: 0.7486 - mae: 0.6564 - root_mean_squared_error: 0.8652 - val_mean_squared_error: 1536426.6250 - val_mean_absolute_error: 737.7775 - val_root_mean_squared_error: 1239.5267\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7362 - mse: 0.7362 - mae: 0.6512 - root_mean_squared_error: 0.8580 - val_mean_squared_error: 1515781.2500 - val_mean_absolute_error: 732.9811 - val_root_mean_squared_error: 1231.1708\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7242 - mse: 0.7242 - mae: 0.6461 - root_mean_squared_error: 0.8510 - val_mean_squared_error: 1495688.3750 - val_mean_absolute_error: 728.2217 - val_root_mean_squared_error: 1222.9834\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7125 - mse: 0.7125 - mae: 0.6410 - root_mean_squared_error: 0.8441 - val_mean_squared_error: 1475597.5000 - val_mean_absolute_error: 723.5109 - val_root_mean_squared_error: 1214.7417\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7010 - mse: 0.7010 - mae: 0.6360 - root_mean_squared_error: 0.8372 - val_mean_squared_error: 1455803.3750 - val_mean_absolute_error: 718.8486 - val_root_mean_squared_error: 1206.5668\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6897 - mse: 0.6897 - mae: 0.6310 - root_mean_squared_error: 0.8305 - val_mean_squared_error: 1436268.2500 - val_mean_absolute_error: 714.2274 - val_root_mean_squared_error: 1198.4441\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6787 - mse: 0.6787 - mae: 0.6262 - root_mean_squared_error: 0.8239 - val_mean_squared_error: 1417047.6250 - val_mean_absolute_error: 709.6993 - val_root_mean_squared_error: 1190.3981\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6680 - mse: 0.6680 - mae: 0.6213 - root_mean_squared_error: 0.8173 - val_mean_squared_error: 1398203.3750 - val_mean_absolute_error: 705.2132 - val_root_mean_squared_error: 1182.4565\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6575 - mse: 0.6575 - mae: 0.6166 - root_mean_squared_error: 0.8109 - val_mean_squared_error: 1379590.8750 - val_mean_absolute_error: 700.7218 - val_root_mean_squared_error: 1174.5598\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6472 - mse: 0.6472 - mae: 0.6119 - root_mean_squared_error: 0.8045 - val_mean_squared_error: 1361425.5000 - val_mean_absolute_error: 696.2989 - val_root_mean_squared_error: 1166.8014\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6372 - mse: 0.6372 - mae: 0.6072 - root_mean_squared_error: 0.7982 - val_mean_squared_error: 1343466.6250 - val_mean_absolute_error: 691.9319 - val_root_mean_squared_error: 1159.0801\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6273 - mse: 0.6273 - mae: 0.6026 - root_mean_squared_error: 0.7920 - val_mean_squared_error: 1325664.2500 - val_mean_absolute_error: 687.5830 - val_root_mean_squared_error: 1151.3750\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6177 - mse: 0.6177 - mae: 0.5981 - root_mean_squared_error: 0.7859 - val_mean_squared_error: 1308435.7500 - val_mean_absolute_error: 683.3303 - val_root_mean_squared_error: 1143.8688\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6083 - mse: 0.6083 - mae: 0.5936 - root_mean_squared_error: 0.7799 - val_mean_squared_error: 1291405.3750 - val_mean_absolute_error: 679.0703 - val_root_mean_squared_error: 1136.4001\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5991 - mse: 0.5991 - mae: 0.5891 - root_mean_squared_error: 0.7740 - val_mean_squared_error: 1274720.3750 - val_mean_absolute_error: 674.8743 - val_root_mean_squared_error: 1129.0352\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5901 - mse: 0.5901 - mae: 0.5848 - root_mean_squared_error: 0.7682 - val_mean_squared_error: 1258245.6250 - val_mean_absolute_error: 670.6719 - val_root_mean_squared_error: 1121.7155\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5813 - mse: 0.5813 - mae: 0.5804 - root_mean_squared_error: 0.7624 - val_mean_squared_error: 1242180.3750 - val_mean_absolute_error: 666.5613 - val_root_mean_squared_error: 1114.5315\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5727 - mse: 0.5727 - mae: 0.5761 - root_mean_squared_error: 0.7567 - val_mean_squared_error: 1226444.2500 - val_mean_absolute_error: 662.4363 - val_root_mean_squared_error: 1107.4495\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5642 - mse: 0.5642 - mae: 0.5719 - root_mean_squared_error: 0.7512 - val_mean_squared_error: 1210982.1250 - val_mean_absolute_error: 658.3561 - val_root_mean_squared_error: 1100.4463\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5560 - mse: 0.5560 - mae: 0.5677 - root_mean_squared_error: 0.7457 - val_mean_squared_error: 1195818.3750 - val_mean_absolute_error: 654.3535 - val_root_mean_squared_error: 1093.5348\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5480 - mse: 0.5480 - mae: 0.5636 - root_mean_squared_error: 0.7402 - val_mean_squared_error: 1180789.2500 - val_mean_absolute_error: 650.3260 - val_root_mean_squared_error: 1086.6414\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5401 - mse: 0.5401 - mae: 0.5595 - root_mean_squared_error: 0.7349 - val_mean_squared_error: 1166020.1250 - val_mean_absolute_error: 646.4098 - val_root_mean_squared_error: 1079.8241\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5324 - mse: 0.5324 - mae: 0.5554 - root_mean_squared_error: 0.7297 - val_mean_squared_error: 1152340.1250 - val_mean_absolute_error: 642.6528 - val_root_mean_squared_error: 1073.4711\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5249 - mse: 0.5249 - mae: 0.5516 - root_mean_squared_error: 0.7245 - val_mean_squared_error: 1137747.0000 - val_mean_absolute_error: 638.9431 - val_root_mean_squared_error: 1066.6523\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5176 - mse: 0.5176 - mae: 0.5474 - root_mean_squared_error: 0.7194 - val_mean_squared_error: 1124773.8750 - val_mean_absolute_error: 635.3314 - val_root_mean_squared_error: 1060.5536\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5104 - mse: 0.5104 - mae: 0.5439 - root_mean_squared_error: 0.7144 - val_mean_squared_error: 1110900.0000 - val_mean_absolute_error: 631.7673 - val_root_mean_squared_error: 1053.9924\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5034 - mse: 0.5034 - mae: 0.5398 - root_mean_squared_error: 0.7095 - val_mean_squared_error: 1098192.3750 - val_mean_absolute_error: 628.3570 - val_root_mean_squared_error: 1047.9468\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4965 - mse: 0.4965 - mae: 0.5363 - root_mean_squared_error: 0.7046 - val_mean_squared_error: 1085145.3750 - val_mean_absolute_error: 624.9860 - val_root_mean_squared_error: 1041.7030\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4898 - mse: 0.4898 - mae: 0.5324 - root_mean_squared_error: 0.6999 - val_mean_squared_error: 1072748.7500 - val_mean_absolute_error: 621.7024 - val_root_mean_squared_error: 1035.7358\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4833 - mse: 0.4833 - mae: 0.5289 - root_mean_squared_error: 0.6952 - val_mean_squared_error: 1060228.1250 - val_mean_absolute_error: 618.4482 - val_root_mean_squared_error: 1029.6738\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4769 - mse: 0.4769 - mae: 0.5252 - root_mean_squared_error: 0.6906 - val_mean_squared_error: 1048252.7500 - val_mean_absolute_error: 615.2477 - val_root_mean_squared_error: 1023.8421\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4707 - mse: 0.4707 - mae: 0.5217 - root_mean_squared_error: 0.6861 - val_mean_squared_error: 1036230.3125 - val_mean_absolute_error: 612.0943 - val_root_mean_squared_error: 1017.9540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4646 - mse: 0.4646 - mae: 0.5182 - root_mean_squared_error: 0.6816 - val_mean_squared_error: 1024740.5000 - val_mean_absolute_error: 608.9827 - val_root_mean_squared_error: 1012.2947\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4586 - mse: 0.4586 - mae: 0.5148 - root_mean_squared_error: 0.6772 - val_mean_squared_error: 1013382.0625 - val_mean_absolute_error: 605.9100 - val_root_mean_squared_error: 1006.6688\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4528 - mse: 0.4528 - mae: 0.5113 - root_mean_squared_error: 0.6729 - val_mean_squared_error: 1002048.0000 - val_mean_absolute_error: 602.8127 - val_root_mean_squared_error: 1001.0235\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4471 - mse: 0.4471 - mae: 0.5081 - root_mean_squared_error: 0.6687 - val_mean_squared_error: 991130.5625 - val_mean_absolute_error: 599.8276 - val_root_mean_squared_error: 995.5554\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4416 - mse: 0.4416 - mae: 0.5047 - root_mean_squared_error: 0.6645 - val_mean_squared_error: 980488.5000 - val_mean_absolute_error: 596.8287 - val_root_mean_squared_error: 990.1962\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4362 - mse: 0.4362 - mae: 0.5015 - root_mean_squared_error: 0.6604 - val_mean_squared_error: 969846.7500 - val_mean_absolute_error: 593.8564 - val_root_mean_squared_error: 984.8080\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4308 - mse: 0.4308 - mae: 0.4982 - root_mean_squared_error: 0.6564 - val_mean_squared_error: 959804.9375 - val_mean_absolute_error: 590.9365 - val_root_mean_squared_error: 979.6964\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4257 - mse: 0.4257 - mae: 0.4952 - root_mean_squared_error: 0.6524 - val_mean_squared_error: 949448.3750 - val_mean_absolute_error: 587.9999 - val_root_mean_squared_error: 974.3964\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4206 - mse: 0.4206 - mae: 0.4920 - root_mean_squared_error: 0.6485 - val_mean_squared_error: 940040.1875 - val_mean_absolute_error: 585.1483 - val_root_mean_squared_error: 969.5566\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4156 - mse: 0.4156 - mae: 0.4891 - root_mean_squared_error: 0.6447 - val_mean_squared_error: 930077.7500 - val_mean_absolute_error: 582.2511 - val_root_mean_squared_error: 964.4054\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4108 - mse: 0.4108 - mae: 0.4859 - root_mean_squared_error: 0.6409 - val_mean_squared_error: 921183.1250 - val_mean_absolute_error: 579.4530 - val_root_mean_squared_error: 959.7828\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4061 - mse: 0.4061 - mae: 0.4832 - root_mean_squared_error: 0.6372 - val_mean_squared_error: 911657.6875 - val_mean_absolute_error: 576.5852 - val_root_mean_squared_error: 954.8076\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4014 - mse: 0.4014 - mae: 0.4800 - root_mean_squared_error: 0.6336 - val_mean_squared_error: 903036.9375 - val_mean_absolute_error: 573.7991 - val_root_mean_squared_error: 950.2825\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3969 - mse: 0.3969 - mae: 0.4774 - root_mean_squared_error: 0.6300 - val_mean_squared_error: 894028.4375 - val_mean_absolute_error: 570.9586 - val_root_mean_squared_error: 945.5308\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3925 - mse: 0.3925 - mae: 0.4743 - root_mean_squared_error: 0.6265 - val_mean_squared_error: 885852.8750 - val_mean_absolute_error: 568.2216 - val_root_mean_squared_error: 941.1976\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3881 - mse: 0.3881 - mae: 0.4717 - root_mean_squared_error: 0.6230"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8342061be258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mno_div\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m df_errors = walk_forward_evaluation(get_dense_model, 250, 10, 30, epochs=300, verbose=1, batch_size_fun=no_div,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                    es_stop_val=False)\n",
      "\u001b[0;32m<ipython-input-14-93302560fc4f>\u001b[0m in \u001b[0;36mwalk_forward_evaluation\u001b[0;34m(model_generator, nb_fit_first, nb_validation, nb_test, epochs, plot, verbose, return_history, batch_size_fun, es_stop_val)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mmodel_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# test only on the unaugmented data without padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-5a130ed1f32f>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \"\"\"        \n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_geo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_padded_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_geo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1623\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   2969\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2970\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2971\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   2972\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   2973\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "no_div = lambda x : len(x)\n",
    "df_errors = walk_forward_evaluation(get_dense_model, 250, 10, 30, epochs=300, verbose=1, batch_size_fun=no_div,\n",
    "                                   es_stop_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 1.1839 - mse: 1.1839 - mae: 0.7759 - root_mean_squared_error: 1.0793 - val_mean_squared_error: 331.9821 - val_mean_absolute_error: 11.6835 - val_root_mean_squared_error: 18.2204\n",
      "Epoch 2/200\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.4572 - mse: 0.4572 - mae: 0.4603 - root_mean_squared_error: 0.6760 - val_mean_squared_error: 294.8744 - val_mean_absolute_error: 11.0358 - val_root_mean_squared_error: 17.1719\n",
      "Epoch 3/200\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.3894 - mse: 0.3894 - mae: 0.4078 - root_mean_squared_error: 0.6240 - val_mean_squared_error: 282.7642 - val_mean_absolute_error: 10.7674 - val_root_mean_squared_error: 16.8156\n",
      "Epoch 4/200\n",
      "179/179 [==============================] - 0s 1ms/step - loss: 0.3648 - mse: 0.3648 - mae: 0.3878 - root_mean_squared_error: 0.6040 - val_mean_squared_error: 296.3748 - val_mean_absolute_error: 11.0865 - val_root_mean_squared_error: 17.2155\n",
      "Epoch 00004: early stopping\n",
      "computed in 0:00:00.943076 s\n"
     ]
    }
   ],
   "source": [
    "# X% for training, remaining for test\n",
    "ratio_training = 0.8\n",
    "nb_datapoints = dg.batch_size\n",
    "max_train = int(ratio_training * nb_datapoints)\n",
    "train_idx = np.array(range(max_train))\n",
    "val_idx = np.array(range(max_train, nb_datapoints))\n",
    "X_train = dg.get_x(train_idx, scaled=True)\n",
    "Y_train = dg.get_y(train_idx, scaled=True)\n",
    "X_val = dg.get_x(val_idx, scaled=True, use_previous_scaler=True)\n",
    "Y_val_unscaled = dg.get_y(val_idx, geo=dg.loc_init, scaled=False, use_previous_scaler=True)\n",
    "Y_val_unpadded_unscaled = dg.remove_padded_y(Y_val_unscaled, idx=val_idx, geo=dg.loc_init)\n",
    "\n",
    "model_generator = get_dense_model\n",
    "\n",
    "batch_size_train = len(train_idx)\n",
    "batch_size_val = len(X_val)\n",
    "model = model_generator(batch_input_shape=(batch_size_train, n_samples, n_features))\n",
    "model_validation = model_generator(batch_input_shape=(batch_size_val, n_samples, n_features))\n",
    "val_log = ValidationLogger(model_validation, len(X_val), X_val, Y_val_unpadded_unscaled, val_idx, dg.loc_init)\n",
    "callbacks = [val_log, EarlyStopping(monitor='val_root_mean_squared_error', mode='min', verbose=1, patience=1)]\n",
    "    \n",
    "start = time.time()\n",
    "history = model.fit(X_train, Y_train, batch_size=batch_size_train, epochs=200, callbacks=callbacks)\n",
    "end = time.time() - start\n",
    "print(f\"computed in {str(str(timedelta(seconds=end)))} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [14:52:08<00:00, 1163.65s/it]  \n"
     ]
    }
   ],
   "source": [
    "hyper_parameter = {\n",
    "    'hidden_1': [4, 8, 16],\n",
    "    'hidden_2': [0, 4, 8, 16],\n",
    "    'lr': [0.001, 0.01],\n",
    "    'activation': ['relu', 'elu'],\n",
    "    'reg': [lambda x: regularizers.l1(l=x), lambda x: None],\n",
    "    'regw': [5e-4, 1e-3],\n",
    "    'optimizer': ['Adam', 'RMSprop'],\n",
    "    'losses': ['mse', 'mae', custom_loss_function],\n",
    "    'scaling': [MinMaxScaler, StandardScaler],\n",
    "    'batch_size_div': [1],\n",
    "    'epochs': [600]\n",
    "}\n",
    "\n",
    "def get_encoder_decoder(batch_input_shape, p):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(p['hidden_1'], return_sequences=(p['hidden_2'] != 0), \n",
    "                   batch_input_shape=batch_input_shape, kernel_regularizer=p['reg'](p['regw'])))\n",
    "    if p['hidden_2'] != 0:\n",
    "        model.add(LSTM(p['hidden_2'], return_sequences=False, kernel_regularizer=p['reg'](p['regw'])))\n",
    "    model.add(RepeatVector(n_forecast))  # repeat\n",
    "    if p['hidden_2'] != 0:\n",
    "        model.add(LSTM(p['hidden_2'], return_sequences=True, kernel_regularizer=p['reg'](p['regw'])))  # dec\n",
    "    if not predict_one:\n",
    "        model.add(LSTM(p['hidden_1'], return_sequences=True, kernel_regularizer=p['reg'](p['regw'])))  # dec\n",
    "        model.add(TimeDistributed(Dense(1, kernel_regularizer=p['reg'](p['regw']), activation=p['activation'])))\n",
    "        model.add(Reshape((n_forecast,)))\n",
    "    else:\n",
    "        model.add(LSTM(p['hidden_1'], return_sequences=False, kernel_regularizer=p['reg'](p['regw'])))  # dec\n",
    "        model.add(Dense(1, kernel_regularizer=p['reg'](p['regw']), activation=p['activation']))\n",
    "        model.add(Reshape((1,)))\n",
    "    model.compile(loss=p[\"losses\"], optimizer=p[\"optimizer\"], metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    K.set_value(model.optimizer.learning_rate, p['lr'])\n",
    "    return model\n",
    "    \n",
    "\n",
    "def talos_walk(x_train, y_train, x_val, y_val, p):\n",
    "    \"\"\"\n",
    "    walk forward evaluation used by talos to determine the best model to keep\n",
    "    \"\"\"\n",
    "    dg.set_scaler(p['scaling'])\n",
    "    model_generator = lambda batch_input_shape: get_encoder_decoder(batch_input_shape, p)\n",
    "\n",
    "    batch_size_fun = lambda x : int(x.shape[0] / p['batch_size_div'])\n",
    "    history_list = walk_forward_evaluation(model_generator, nb_fit_first=250, nb_validation=0, nb_test=30, \n",
    "                                      epochs=p['epochs'], plot=False, verbose=0, \n",
    "                                      batch_size_fun=batch_size_fun, return_history=True, es_stop_val=True)\n",
    "    # compute history to return, based on the list of history\n",
    "    history = History()\n",
    "    # used to trick talos to compute the right amount of round_epochs. needs to be the first entry of the dict\n",
    "    # create an array of len == mean of number of epoch of each history\n",
    "    history.history['ep'] = np.arange(int(np.mean([len(hist.history['loss']) for hist in history_list]))) + 1\n",
    "    for log in history_list[0].history.keys():\n",
    "        history.history[log] = [np.mean([hist.history[log][-1] for hist in history_list])]\n",
    "    return history, model_generator((1, n_samples, n_features))\n",
    "\n",
    "scan_object = talos.Scan(\n",
    "    x=[],\n",
    "    y=[],\n",
    "    x_val=[],\n",
    "    y_val=[],\n",
    "    params=hyper_parameter,\n",
    "    model=talos_walk,\n",
    "    experiment_name='trends1', \n",
    "    fraction_limit=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 0.021138007286936045\n",
      "MSE 0.0021799016976729035\n",
      "RMSE 0.046687012538313866\n",
      "TEST MAE 11.598709106445312\n",
      "TEST MSE 368.9996337890625\n",
      "TEST RMSE 18.94180679321289\n",
      "Index(['test_mae(t+16)', 'test_root_mean_squared_error(t+20)',\n",
      "       'test_mse(t+14)', 'test_mae(t+8)', 'batch_size_div', 'hidden_1',\n",
      "       'test_root_mean_squared_error(t+3)', 'scaling', 'test_mse(t+8)',\n",
      "       'test_mae(t+18)', 'test_mae(t+6)', 'test_root_mean_squared_error(t+2)',\n",
      "       'test_mae(t+3)', 'test_mse(t+9)', 'test_loss',\n",
      "       'root_mean_squared_error', 'test_mae(t+11)',\n",
      "       'test_root_mean_squared_error(t+10)', 'regw', 'loss', 'test_mae(t+15)',\n",
      "       'test_root_mean_squared_error(t+14)', 'duration',\n",
      "       'test_root_mean_squared_error(t+11)', 'test_mae(t+7)', 'test_mae(t+1)',\n",
      "       'test_root_mean_squared_error(t+7)', 'optimizer', 'test_mse(t+1)',\n",
      "       'test_mae(t+17)', 'mae', 'test_mse(t+5)', 'end', 'test_mae(t+9)',\n",
      "       'test_mae(t+10)', 'test_mae', 'test_root_mean_squared_error(t+9)',\n",
      "       'test_mse(t+12)', 'test_mse(t+11)', 'test_mse(t+6)', 'round_epochs',\n",
      "       'test_mse(t+20)', 'test_mse(t+4)', 'losses', 'test_mse(t+19)',\n",
      "       'test_mae(t+20)', 'test_mae(t+14)', 'test_root_mean_squared_error',\n",
      "       'epochs', 'test_root_mean_squared_error(t+15)', 'test_mae(t+2)',\n",
      "       'activation', 'test_mae(t+13)', 'test_root_mean_squared_error(t+17)',\n",
      "       'test_mae(t+4)', 'test_mae(t+12)', 'ep', 'test_mae(t+5)', 'mse',\n",
      "       'test_mse(t+15)', 'test_mse(t+7)', 'test_mse(t+2)',\n",
      "       'test_root_mean_squared_error(t+4)', 'nb_test_datapoints',\n",
      "       'test_mse(t+18)', 'lr', 'test_mse', 'test_root_mean_squared_error(t+8)',\n",
      "       'start', 'test_mse(t+16)', 'test_root_mean_squared_error(t+5)',\n",
      "       'test_root_mean_squared_error(t+16)', 'test_mse(t+13)',\n",
      "       'test_root_mean_squared_error(t+1)', 'test_mse(t+3)',\n",
      "       'test_root_mean_squared_error(t+18)',\n",
      "       'test_root_mean_squared_error(t+6)',\n",
      "       'test_root_mean_squared_error(t+19)',\n",
      "       'test_root_mean_squared_error(t+13)', 'test_mse(t+17)',\n",
      "       'test_mse(t+10)', 'test_root_mean_squared_error(t+12)', 'reg',\n",
      "       'hidden_2', 'test_mae(t+19)'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_1</th>\n",
       "      <th>hidden_2</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size_div</th>\n",
       "      <th>scaling</th>\n",
       "      <th>test_mse</th>\n",
       "      <th>test_mae</th>\n",
       "      <th>test_mae(t+1)</th>\n",
       "      <th>test_mae(t+2)</th>\n",
       "      <th>test_mae(t+3)</th>\n",
       "      <th>...</th>\n",
       "      <th>test_mae(t+11)</th>\n",
       "      <th>test_mae(t+12)</th>\n",
       "      <th>test_mae(t+13)</th>\n",
       "      <th>test_mae(t+14)</th>\n",
       "      <th>test_mae(t+15)</th>\n",
       "      <th>test_mae(t+16)</th>\n",
       "      <th>test_mae(t+17)</th>\n",
       "      <th>test_mae(t+18)</th>\n",
       "      <th>test_mae(t+19)</th>\n",
       "      <th>test_mae(t+20)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>368.999634</td>\n",
       "      <td>11.613309</td>\n",
       "      <td>3.778100</td>\n",
       "      <td>4.935235</td>\n",
       "      <td>5.915179</td>\n",
       "      <td>...</td>\n",
       "      <td>12.376617</td>\n",
       "      <td>13.007839</td>\n",
       "      <td>13.633907</td>\n",
       "      <td>14.263512</td>\n",
       "      <td>14.920258</td>\n",
       "      <td>15.531412</td>\n",
       "      <td>16.099224</td>\n",
       "      <td>16.683821</td>\n",
       "      <td>17.256342</td>\n",
       "      <td>17.777834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>377.847046</td>\n",
       "      <td>12.958789</td>\n",
       "      <td>8.388412</td>\n",
       "      <td>9.880347</td>\n",
       "      <td>10.006425</td>\n",
       "      <td>...</td>\n",
       "      <td>13.358921</td>\n",
       "      <td>13.645789</td>\n",
       "      <td>13.990844</td>\n",
       "      <td>14.349859</td>\n",
       "      <td>14.669656</td>\n",
       "      <td>15.053366</td>\n",
       "      <td>15.370562</td>\n",
       "      <td>15.586323</td>\n",
       "      <td>15.798634</td>\n",
       "      <td>15.899250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>406.827850</td>\n",
       "      <td>12.593399</td>\n",
       "      <td>6.765717</td>\n",
       "      <td>7.613912</td>\n",
       "      <td>8.365121</td>\n",
       "      <td>...</td>\n",
       "      <td>13.468274</td>\n",
       "      <td>13.845459</td>\n",
       "      <td>14.167676</td>\n",
       "      <td>14.456120</td>\n",
       "      <td>14.748785</td>\n",
       "      <td>15.063616</td>\n",
       "      <td>15.360583</td>\n",
       "      <td>15.663295</td>\n",
       "      <td>16.004938</td>\n",
       "      <td>16.381424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>443.305725</td>\n",
       "      <td>12.500948</td>\n",
       "      <td>3.693830</td>\n",
       "      <td>4.283959</td>\n",
       "      <td>5.205354</td>\n",
       "      <td>...</td>\n",
       "      <td>13.635902</td>\n",
       "      <td>14.493777</td>\n",
       "      <td>15.292961</td>\n",
       "      <td>16.010576</td>\n",
       "      <td>16.731924</td>\n",
       "      <td>17.372627</td>\n",
       "      <td>18.027727</td>\n",
       "      <td>18.646788</td>\n",
       "      <td>19.265472</td>\n",
       "      <td>19.899242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>456.099213</td>\n",
       "      <td>12.087252</td>\n",
       "      <td>4.626646</td>\n",
       "      <td>6.715505</td>\n",
       "      <td>8.265052</td>\n",
       "      <td>...</td>\n",
       "      <td>13.107601</td>\n",
       "      <td>13.613174</td>\n",
       "      <td>14.048625</td>\n",
       "      <td>14.454420</td>\n",
       "      <td>14.844568</td>\n",
       "      <td>15.207379</td>\n",
       "      <td>15.532366</td>\n",
       "      <td>15.824340</td>\n",
       "      <td>16.075926</td>\n",
       "      <td>16.363590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.010</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>483.986328</td>\n",
       "      <td>12.050247</td>\n",
       "      <td>6.499933</td>\n",
       "      <td>6.763069</td>\n",
       "      <td>7.382070</td>\n",
       "      <td>...</td>\n",
       "      <td>12.414900</td>\n",
       "      <td>13.128962</td>\n",
       "      <td>13.799572</td>\n",
       "      <td>14.418126</td>\n",
       "      <td>15.001484</td>\n",
       "      <td>15.375210</td>\n",
       "      <td>15.975498</td>\n",
       "      <td>16.462032</td>\n",
       "      <td>17.175732</td>\n",
       "      <td>17.527330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>492.530121</td>\n",
       "      <td>13.382237</td>\n",
       "      <td>4.648648</td>\n",
       "      <td>6.005950</td>\n",
       "      <td>7.197309</td>\n",
       "      <td>...</td>\n",
       "      <td>14.322783</td>\n",
       "      <td>14.980151</td>\n",
       "      <td>15.623283</td>\n",
       "      <td>16.238644</td>\n",
       "      <td>16.897707</td>\n",
       "      <td>17.515383</td>\n",
       "      <td>18.128284</td>\n",
       "      <td>18.703037</td>\n",
       "      <td>19.297628</td>\n",
       "      <td>19.869844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>492.417572</td>\n",
       "      <td>12.767149</td>\n",
       "      <td>10.886259</td>\n",
       "      <td>10.267223</td>\n",
       "      <td>9.985346</td>\n",
       "      <td>...</td>\n",
       "      <td>13.149528</td>\n",
       "      <td>13.611848</td>\n",
       "      <td>13.995646</td>\n",
       "      <td>14.344925</td>\n",
       "      <td>14.654423</td>\n",
       "      <td>14.907431</td>\n",
       "      <td>15.129011</td>\n",
       "      <td>15.319042</td>\n",
       "      <td>15.537729</td>\n",
       "      <td>15.761189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>513.338318</td>\n",
       "      <td>13.462949</td>\n",
       "      <td>5.290785</td>\n",
       "      <td>6.213699</td>\n",
       "      <td>7.302274</td>\n",
       "      <td>...</td>\n",
       "      <td>13.636876</td>\n",
       "      <td>14.471559</td>\n",
       "      <td>15.364357</td>\n",
       "      <td>16.282177</td>\n",
       "      <td>17.208750</td>\n",
       "      <td>18.127089</td>\n",
       "      <td>19.030231</td>\n",
       "      <td>19.875698</td>\n",
       "      <td>20.663685</td>\n",
       "      <td>21.355709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>555.854492</td>\n",
       "      <td>11.598709</td>\n",
       "      <td>5.281905</td>\n",
       "      <td>5.901314</td>\n",
       "      <td>6.462759</td>\n",
       "      <td>...</td>\n",
       "      <td>11.540359</td>\n",
       "      <td>12.110660</td>\n",
       "      <td>12.728726</td>\n",
       "      <td>13.400656</td>\n",
       "      <td>14.205097</td>\n",
       "      <td>15.057336</td>\n",
       "      <td>15.980198</td>\n",
       "      <td>16.984249</td>\n",
       "      <td>18.057579</td>\n",
       "      <td>19.119024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>529.013794</td>\n",
       "      <td>13.925307</td>\n",
       "      <td>10.291524</td>\n",
       "      <td>7.956648</td>\n",
       "      <td>8.333316</td>\n",
       "      <td>...</td>\n",
       "      <td>14.794746</td>\n",
       "      <td>15.399143</td>\n",
       "      <td>15.964369</td>\n",
       "      <td>16.449862</td>\n",
       "      <td>16.894203</td>\n",
       "      <td>17.319576</td>\n",
       "      <td>17.739185</td>\n",
       "      <td>18.148319</td>\n",
       "      <td>18.582518</td>\n",
       "      <td>19.032888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>520.161377</td>\n",
       "      <td>14.117408</td>\n",
       "      <td>8.824968</td>\n",
       "      <td>9.530283</td>\n",
       "      <td>10.032682</td>\n",
       "      <td>...</td>\n",
       "      <td>14.818234</td>\n",
       "      <td>15.214798</td>\n",
       "      <td>15.651771</td>\n",
       "      <td>16.024471</td>\n",
       "      <td>16.392265</td>\n",
       "      <td>16.754742</td>\n",
       "      <td>17.105921</td>\n",
       "      <td>17.433689</td>\n",
       "      <td>17.764273</td>\n",
       "      <td>18.080326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>634.228088</td>\n",
       "      <td>15.367864</td>\n",
       "      <td>13.861606</td>\n",
       "      <td>6.116435</td>\n",
       "      <td>6.202892</td>\n",
       "      <td>...</td>\n",
       "      <td>17.255331</td>\n",
       "      <td>17.876137</td>\n",
       "      <td>18.364826</td>\n",
       "      <td>18.826605</td>\n",
       "      <td>19.294817</td>\n",
       "      <td>19.725729</td>\n",
       "      <td>20.088127</td>\n",
       "      <td>20.413914</td>\n",
       "      <td>20.740185</td>\n",
       "      <td>21.033175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.010</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>615.603760</td>\n",
       "      <td>13.877359</td>\n",
       "      <td>7.828645</td>\n",
       "      <td>7.554558</td>\n",
       "      <td>8.338289</td>\n",
       "      <td>...</td>\n",
       "      <td>14.562158</td>\n",
       "      <td>15.185158</td>\n",
       "      <td>15.812385</td>\n",
       "      <td>16.420794</td>\n",
       "      <td>16.974445</td>\n",
       "      <td>17.505888</td>\n",
       "      <td>18.054600</td>\n",
       "      <td>18.589483</td>\n",
       "      <td>19.133724</td>\n",
       "      <td>19.610550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>606.898926</td>\n",
       "      <td>13.702591</td>\n",
       "      <td>3.943191</td>\n",
       "      <td>4.929643</td>\n",
       "      <td>6.199639</td>\n",
       "      <td>...</td>\n",
       "      <td>14.859236</td>\n",
       "      <td>15.702940</td>\n",
       "      <td>16.506563</td>\n",
       "      <td>17.246296</td>\n",
       "      <td>18.005705</td>\n",
       "      <td>18.741405</td>\n",
       "      <td>19.450014</td>\n",
       "      <td>20.108582</td>\n",
       "      <td>20.715981</td>\n",
       "      <td>21.310787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>663.687500</td>\n",
       "      <td>15.409854</td>\n",
       "      <td>14.098557</td>\n",
       "      <td>14.238568</td>\n",
       "      <td>14.308229</td>\n",
       "      <td>...</td>\n",
       "      <td>15.410071</td>\n",
       "      <td>15.503851</td>\n",
       "      <td>15.596975</td>\n",
       "      <td>15.707144</td>\n",
       "      <td>15.876019</td>\n",
       "      <td>16.060770</td>\n",
       "      <td>16.303602</td>\n",
       "      <td>16.555473</td>\n",
       "      <td>16.807163</td>\n",
       "      <td>17.055710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>662.759583</td>\n",
       "      <td>13.560087</td>\n",
       "      <td>7.855592</td>\n",
       "      <td>7.480397</td>\n",
       "      <td>8.068832</td>\n",
       "      <td>...</td>\n",
       "      <td>13.729120</td>\n",
       "      <td>14.448841</td>\n",
       "      <td>15.317167</td>\n",
       "      <td>16.092915</td>\n",
       "      <td>17.021358</td>\n",
       "      <td>17.732309</td>\n",
       "      <td>18.581205</td>\n",
       "      <td>19.293995</td>\n",
       "      <td>19.980360</td>\n",
       "      <td>20.639826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>639.696411</td>\n",
       "      <td>15.031433</td>\n",
       "      <td>26.171202</td>\n",
       "      <td>7.267821</td>\n",
       "      <td>8.403698</td>\n",
       "      <td>...</td>\n",
       "      <td>15.765282</td>\n",
       "      <td>16.066397</td>\n",
       "      <td>16.321272</td>\n",
       "      <td>16.505648</td>\n",
       "      <td>16.664925</td>\n",
       "      <td>16.785013</td>\n",
       "      <td>16.864689</td>\n",
       "      <td>16.939903</td>\n",
       "      <td>16.998920</td>\n",
       "      <td>17.026264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>706.112976</td>\n",
       "      <td>14.086211</td>\n",
       "      <td>5.320840</td>\n",
       "      <td>6.203963</td>\n",
       "      <td>7.472972</td>\n",
       "      <td>...</td>\n",
       "      <td>15.304962</td>\n",
       "      <td>16.044222</td>\n",
       "      <td>16.744120</td>\n",
       "      <td>17.387867</td>\n",
       "      <td>17.973007</td>\n",
       "      <td>18.481647</td>\n",
       "      <td>18.921314</td>\n",
       "      <td>19.276642</td>\n",
       "      <td>19.614832</td>\n",
       "      <td>19.877411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>675.383057</td>\n",
       "      <td>14.122136</td>\n",
       "      <td>6.344134</td>\n",
       "      <td>6.712139</td>\n",
       "      <td>7.769174</td>\n",
       "      <td>...</td>\n",
       "      <td>15.203131</td>\n",
       "      <td>15.857417</td>\n",
       "      <td>16.507896</td>\n",
       "      <td>17.144571</td>\n",
       "      <td>17.739826</td>\n",
       "      <td>18.283808</td>\n",
       "      <td>18.794672</td>\n",
       "      <td>19.302101</td>\n",
       "      <td>19.873262</td>\n",
       "      <td>20.499794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>684.079712</td>\n",
       "      <td>15.909692</td>\n",
       "      <td>16.279470</td>\n",
       "      <td>9.322615</td>\n",
       "      <td>11.208075</td>\n",
       "      <td>...</td>\n",
       "      <td>16.354940</td>\n",
       "      <td>17.054209</td>\n",
       "      <td>17.702763</td>\n",
       "      <td>18.302721</td>\n",
       "      <td>18.880907</td>\n",
       "      <td>19.436762</td>\n",
       "      <td>19.973253</td>\n",
       "      <td>20.482304</td>\n",
       "      <td>20.987247</td>\n",
       "      <td>21.464321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>865.923889</td>\n",
       "      <td>15.202335</td>\n",
       "      <td>10.201191</td>\n",
       "      <td>11.156207</td>\n",
       "      <td>10.675257</td>\n",
       "      <td>...</td>\n",
       "      <td>14.762134</td>\n",
       "      <td>15.525005</td>\n",
       "      <td>16.344530</td>\n",
       "      <td>17.144669</td>\n",
       "      <td>18.004837</td>\n",
       "      <td>18.956707</td>\n",
       "      <td>19.820560</td>\n",
       "      <td>20.570244</td>\n",
       "      <td>21.282341</td>\n",
       "      <td>21.971457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>696.291321</td>\n",
       "      <td>14.334661</td>\n",
       "      <td>4.491560</td>\n",
       "      <td>5.356360</td>\n",
       "      <td>6.497049</td>\n",
       "      <td>...</td>\n",
       "      <td>15.356086</td>\n",
       "      <td>16.356821</td>\n",
       "      <td>17.320501</td>\n",
       "      <td>18.186972</td>\n",
       "      <td>19.015043</td>\n",
       "      <td>19.818275</td>\n",
       "      <td>20.570786</td>\n",
       "      <td>21.317150</td>\n",
       "      <td>22.038813</td>\n",
       "      <td>22.742344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>775.460449</td>\n",
       "      <td>14.744368</td>\n",
       "      <td>6.523486</td>\n",
       "      <td>7.249412</td>\n",
       "      <td>8.642371</td>\n",
       "      <td>...</td>\n",
       "      <td>15.755215</td>\n",
       "      <td>16.620220</td>\n",
       "      <td>17.434778</td>\n",
       "      <td>18.139967</td>\n",
       "      <td>18.733814</td>\n",
       "      <td>19.302925</td>\n",
       "      <td>19.821541</td>\n",
       "      <td>20.240486</td>\n",
       "      <td>20.523680</td>\n",
       "      <td>20.653320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>784.466553</td>\n",
       "      <td>17.545256</td>\n",
       "      <td>27.541447</td>\n",
       "      <td>10.874722</td>\n",
       "      <td>10.017864</td>\n",
       "      <td>...</td>\n",
       "      <td>17.323406</td>\n",
       "      <td>18.089382</td>\n",
       "      <td>18.867624</td>\n",
       "      <td>19.602230</td>\n",
       "      <td>20.291792</td>\n",
       "      <td>20.900875</td>\n",
       "      <td>21.416956</td>\n",
       "      <td>21.869158</td>\n",
       "      <td>22.305794</td>\n",
       "      <td>22.722769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>870.827454</td>\n",
       "      <td>17.582806</td>\n",
       "      <td>12.931618</td>\n",
       "      <td>13.667276</td>\n",
       "      <td>14.256495</td>\n",
       "      <td>...</td>\n",
       "      <td>18.424906</td>\n",
       "      <td>18.719252</td>\n",
       "      <td>18.987040</td>\n",
       "      <td>19.240505</td>\n",
       "      <td>19.481171</td>\n",
       "      <td>19.684546</td>\n",
       "      <td>19.844309</td>\n",
       "      <td>19.968094</td>\n",
       "      <td>20.086826</td>\n",
       "      <td>20.199846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>922.228821</td>\n",
       "      <td>15.370725</td>\n",
       "      <td>8.709828</td>\n",
       "      <td>9.178472</td>\n",
       "      <td>9.917445</td>\n",
       "      <td>...</td>\n",
       "      <td>15.803117</td>\n",
       "      <td>16.493097</td>\n",
       "      <td>17.157476</td>\n",
       "      <td>17.849754</td>\n",
       "      <td>18.602829</td>\n",
       "      <td>19.323193</td>\n",
       "      <td>20.026464</td>\n",
       "      <td>20.644396</td>\n",
       "      <td>21.308001</td>\n",
       "      <td>21.939484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>872.959717</td>\n",
       "      <td>14.153067</td>\n",
       "      <td>5.672068</td>\n",
       "      <td>6.369383</td>\n",
       "      <td>6.991834</td>\n",
       "      <td>...</td>\n",
       "      <td>14.680539</td>\n",
       "      <td>15.687491</td>\n",
       "      <td>16.687002</td>\n",
       "      <td>17.704224</td>\n",
       "      <td>18.527758</td>\n",
       "      <td>19.242878</td>\n",
       "      <td>20.142241</td>\n",
       "      <td>20.870043</td>\n",
       "      <td>21.810358</td>\n",
       "      <td>22.743580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.010</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>1053.068115</td>\n",
       "      <td>14.899381</td>\n",
       "      <td>6.602954</td>\n",
       "      <td>7.370401</td>\n",
       "      <td>8.048459</td>\n",
       "      <td>...</td>\n",
       "      <td>15.253789</td>\n",
       "      <td>16.229065</td>\n",
       "      <td>17.169615</td>\n",
       "      <td>18.072643</td>\n",
       "      <td>18.977373</td>\n",
       "      <td>19.857183</td>\n",
       "      <td>20.748529</td>\n",
       "      <td>21.677843</td>\n",
       "      <td>22.650225</td>\n",
       "      <td>23.624945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>850.296387</td>\n",
       "      <td>15.869330</td>\n",
       "      <td>8.834017</td>\n",
       "      <td>8.318821</td>\n",
       "      <td>9.074226</td>\n",
       "      <td>...</td>\n",
       "      <td>16.442091</td>\n",
       "      <td>17.334753</td>\n",
       "      <td>18.208447</td>\n",
       "      <td>19.097219</td>\n",
       "      <td>19.994881</td>\n",
       "      <td>20.745625</td>\n",
       "      <td>21.429533</td>\n",
       "      <td>21.995888</td>\n",
       "      <td>22.477907</td>\n",
       "      <td>22.848272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>970.601929</td>\n",
       "      <td>13.723557</td>\n",
       "      <td>5.622512</td>\n",
       "      <td>6.327600</td>\n",
       "      <td>7.275214</td>\n",
       "      <td>...</td>\n",
       "      <td>14.364141</td>\n",
       "      <td>15.170038</td>\n",
       "      <td>15.976849</td>\n",
       "      <td>16.770655</td>\n",
       "      <td>17.561710</td>\n",
       "      <td>18.364143</td>\n",
       "      <td>19.110636</td>\n",
       "      <td>19.784697</td>\n",
       "      <td>20.445417</td>\n",
       "      <td>21.034103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.010</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>995.794189</td>\n",
       "      <td>17.655621</td>\n",
       "      <td>14.386787</td>\n",
       "      <td>14.380050</td>\n",
       "      <td>14.851608</td>\n",
       "      <td>...</td>\n",
       "      <td>17.347822</td>\n",
       "      <td>17.551104</td>\n",
       "      <td>18.112757</td>\n",
       "      <td>18.542997</td>\n",
       "      <td>19.155266</td>\n",
       "      <td>19.640114</td>\n",
       "      <td>20.183678</td>\n",
       "      <td>20.766685</td>\n",
       "      <td>21.438229</td>\n",
       "      <td>22.122795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>1123.774658</td>\n",
       "      <td>21.696770</td>\n",
       "      <td>24.962830</td>\n",
       "      <td>24.887920</td>\n",
       "      <td>24.185865</td>\n",
       "      <td>...</td>\n",
       "      <td>20.841635</td>\n",
       "      <td>20.619600</td>\n",
       "      <td>20.445692</td>\n",
       "      <td>20.330851</td>\n",
       "      <td>20.287035</td>\n",
       "      <td>20.256460</td>\n",
       "      <td>20.234434</td>\n",
       "      <td>20.233940</td>\n",
       "      <td>20.255970</td>\n",
       "      <td>20.274055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>1272.195190</td>\n",
       "      <td>17.446192</td>\n",
       "      <td>13.921982</td>\n",
       "      <td>13.878373</td>\n",
       "      <td>14.175632</td>\n",
       "      <td>...</td>\n",
       "      <td>16.170933</td>\n",
       "      <td>16.977177</td>\n",
       "      <td>17.879543</td>\n",
       "      <td>18.826962</td>\n",
       "      <td>19.783237</td>\n",
       "      <td>20.725595</td>\n",
       "      <td>21.666378</td>\n",
       "      <td>22.582603</td>\n",
       "      <td>23.486774</td>\n",
       "      <td>24.339378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>1304.846680</td>\n",
       "      <td>18.532808</td>\n",
       "      <td>15.984436</td>\n",
       "      <td>10.514296</td>\n",
       "      <td>12.141199</td>\n",
       "      <td>...</td>\n",
       "      <td>18.900497</td>\n",
       "      <td>19.336975</td>\n",
       "      <td>19.878744</td>\n",
       "      <td>20.487968</td>\n",
       "      <td>21.144838</td>\n",
       "      <td>21.847029</td>\n",
       "      <td>22.528599</td>\n",
       "      <td>23.188278</td>\n",
       "      <td>23.889082</td>\n",
       "      <td>24.600050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>1249.239746</td>\n",
       "      <td>23.861946</td>\n",
       "      <td>27.681740</td>\n",
       "      <td>27.131174</td>\n",
       "      <td>26.596161</td>\n",
       "      <td>...</td>\n",
       "      <td>23.191870</td>\n",
       "      <td>22.880596</td>\n",
       "      <td>22.608377</td>\n",
       "      <td>22.377552</td>\n",
       "      <td>22.217377</td>\n",
       "      <td>22.071198</td>\n",
       "      <td>21.957775</td>\n",
       "      <td>21.863983</td>\n",
       "      <td>21.785938</td>\n",
       "      <td>21.703148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>1398.817627</td>\n",
       "      <td>16.185120</td>\n",
       "      <td>4.451111</td>\n",
       "      <td>4.746300</td>\n",
       "      <td>5.950820</td>\n",
       "      <td>...</td>\n",
       "      <td>17.316998</td>\n",
       "      <td>18.654621</td>\n",
       "      <td>19.892683</td>\n",
       "      <td>21.163805</td>\n",
       "      <td>22.344378</td>\n",
       "      <td>23.448616</td>\n",
       "      <td>24.658430</td>\n",
       "      <td>25.654512</td>\n",
       "      <td>26.854719</td>\n",
       "      <td>27.796484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.StandardSc...</td>\n",
       "      <td>1571.534668</td>\n",
       "      <td>19.911650</td>\n",
       "      <td>10.646804</td>\n",
       "      <td>12.710405</td>\n",
       "      <td>12.868601</td>\n",
       "      <td>...</td>\n",
       "      <td>19.775427</td>\n",
       "      <td>20.954788</td>\n",
       "      <td>22.192665</td>\n",
       "      <td>23.455677</td>\n",
       "      <td>24.692188</td>\n",
       "      <td>25.870413</td>\n",
       "      <td>26.976841</td>\n",
       "      <td>28.005070</td>\n",
       "      <td>28.940859</td>\n",
       "      <td>29.753147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>2303.050537</td>\n",
       "      <td>26.078085</td>\n",
       "      <td>27.192299</td>\n",
       "      <td>21.304930</td>\n",
       "      <td>22.674973</td>\n",
       "      <td>...</td>\n",
       "      <td>27.043064</td>\n",
       "      <td>27.228987</td>\n",
       "      <td>27.370951</td>\n",
       "      <td>27.466963</td>\n",
       "      <td>27.530399</td>\n",
       "      <td>27.583254</td>\n",
       "      <td>27.614235</td>\n",
       "      <td>27.625099</td>\n",
       "      <td>27.700760</td>\n",
       "      <td>27.750038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>2453.873047</td>\n",
       "      <td>24.772991</td>\n",
       "      <td>23.705402</td>\n",
       "      <td>22.339560</td>\n",
       "      <td>22.496401</td>\n",
       "      <td>...</td>\n",
       "      <td>24.573008</td>\n",
       "      <td>24.917761</td>\n",
       "      <td>25.307981</td>\n",
       "      <td>25.669456</td>\n",
       "      <td>26.092098</td>\n",
       "      <td>26.500811</td>\n",
       "      <td>26.909277</td>\n",
       "      <td>27.304714</td>\n",
       "      <td>27.690290</td>\n",
       "      <td>28.070812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.010</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>1866.772583</td>\n",
       "      <td>27.344875</td>\n",
       "      <td>29.914427</td>\n",
       "      <td>30.584156</td>\n",
       "      <td>29.111483</td>\n",
       "      <td>...</td>\n",
       "      <td>26.906099</td>\n",
       "      <td>26.683458</td>\n",
       "      <td>26.434025</td>\n",
       "      <td>26.301178</td>\n",
       "      <td>26.210037</td>\n",
       "      <td>26.082321</td>\n",
       "      <td>26.049553</td>\n",
       "      <td>26.092022</td>\n",
       "      <td>26.229645</td>\n",
       "      <td>26.407499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>2424.874023</td>\n",
       "      <td>27.427322</td>\n",
       "      <td>21.928265</td>\n",
       "      <td>21.628294</td>\n",
       "      <td>22.732355</td>\n",
       "      <td>...</td>\n",
       "      <td>28.268524</td>\n",
       "      <td>28.791918</td>\n",
       "      <td>29.283121</td>\n",
       "      <td>29.739248</td>\n",
       "      <td>30.144047</td>\n",
       "      <td>30.493069</td>\n",
       "      <td>30.819393</td>\n",
       "      <td>31.146364</td>\n",
       "      <td>31.456766</td>\n",
       "      <td>31.712200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>2571.682617</td>\n",
       "      <td>25.798573</td>\n",
       "      <td>22.609127</td>\n",
       "      <td>22.871740</td>\n",
       "      <td>23.472391</td>\n",
       "      <td>...</td>\n",
       "      <td>25.751194</td>\n",
       "      <td>26.026449</td>\n",
       "      <td>26.307207</td>\n",
       "      <td>26.589748</td>\n",
       "      <td>26.957422</td>\n",
       "      <td>27.380726</td>\n",
       "      <td>27.819260</td>\n",
       "      <td>28.299473</td>\n",
       "      <td>28.744661</td>\n",
       "      <td>29.232882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>2238.039795</td>\n",
       "      <td>18.461998</td>\n",
       "      <td>6.858403</td>\n",
       "      <td>8.017258</td>\n",
       "      <td>9.049136</td>\n",
       "      <td>...</td>\n",
       "      <td>20.889761</td>\n",
       "      <td>21.887777</td>\n",
       "      <td>22.764872</td>\n",
       "      <td>23.488838</td>\n",
       "      <td>24.036427</td>\n",
       "      <td>24.459564</td>\n",
       "      <td>24.788841</td>\n",
       "      <td>25.059458</td>\n",
       "      <td>25.333035</td>\n",
       "      <td>25.586086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>2529.952393</td>\n",
       "      <td>26.131266</td>\n",
       "      <td>18.512066</td>\n",
       "      <td>19.036589</td>\n",
       "      <td>20.126352</td>\n",
       "      <td>...</td>\n",
       "      <td>26.968700</td>\n",
       "      <td>27.602985</td>\n",
       "      <td>28.264090</td>\n",
       "      <td>28.891703</td>\n",
       "      <td>29.477776</td>\n",
       "      <td>30.045944</td>\n",
       "      <td>30.613148</td>\n",
       "      <td>31.176956</td>\n",
       "      <td>31.773968</td>\n",
       "      <td>32.308380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.010</td>\n",
       "      <td>179</td>\n",
       "      <td>&lt;class 'sklearn.preprocessing._data.MinMaxScal...</td>\n",
       "      <td>2983.060547</td>\n",
       "      <td>30.724981</td>\n",
       "      <td>29.919691</td>\n",
       "      <td>30.373322</td>\n",
       "      <td>30.136894</td>\n",
       "      <td>...</td>\n",
       "      <td>30.757483</td>\n",
       "      <td>30.842653</td>\n",
       "      <td>30.878279</td>\n",
       "      <td>30.959229</td>\n",
       "      <td>30.943300</td>\n",
       "      <td>31.013866</td>\n",
       "      <td>30.951878</td>\n",
       "      <td>31.119537</td>\n",
       "      <td>31.071012</td>\n",
       "      <td>31.414185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_1  hidden_2     lr  batch_size_div  \\\n",
       "29         8        16  0.001               1   \n",
       "12        16        16  0.010               1   \n",
       "38         8         4  0.001             179   \n",
       "5          8         0  0.010               1   \n",
       "30         4         4  0.001               1   \n",
       "33         8        16  0.010             179   \n",
       "15         8        16  0.001             179   \n",
       "21         4         0  0.010               1   \n",
       "23         8         4  0.001             179   \n",
       "36         4         0  0.001             179   \n",
       "34        16         0  0.001               1   \n",
       "31         8         0  0.001             179   \n",
       "16         4         0  0.001               1   \n",
       "22         4        16  0.010             179   \n",
       "3          8         0  0.001             179   \n",
       "24         8         8  0.001             179   \n",
       "7          8         0  0.010             179   \n",
       "18         8         8  0.001               1   \n",
       "1          4         8  0.001               1   \n",
       "45         8         0  0.010               1   \n",
       "8         16         4  0.001               1   \n",
       "41         8         4  0.001             179   \n",
       "6         16         0  0.010               1   \n",
       "39         4        16  0.010               1   \n",
       "4         16         4  0.001               1   \n",
       "2          4         4  0.001               1   \n",
       "19        16         0  0.010             179   \n",
       "35         8         0  0.010             179   \n",
       "0          4         8  0.010             179   \n",
       "44         8         4  0.010               1   \n",
       "27        16        16  0.001               1   \n",
       "32        16        16  0.010             179   \n",
       "40        16         0  0.010               1   \n",
       "43         8         8  0.010               1   \n",
       "13         4         8  0.001               1   \n",
       "17         4         8  0.001             179   \n",
       "28         4         4  0.010             179   \n",
       "25         4        16  0.001               1   \n",
       "14         4         0  0.001               1   \n",
       "37         4        16  0.010               1   \n",
       "10        16        16  0.010             179   \n",
       "26        16         8  0.001               1   \n",
       "20        16         0  0.010             179   \n",
       "42         4         0  0.001             179   \n",
       "9          8         0  0.001             179   \n",
       "11         8        16  0.010             179   \n",
       "\n",
       "                                              scaling     test_mse   test_mae  \\\n",
       "29  <class 'sklearn.preprocessing._data.MinMaxScal...   368.999634  11.613309   \n",
       "12  <class 'sklearn.preprocessing._data.StandardSc...   377.847046  12.958789   \n",
       "38  <class 'sklearn.preprocessing._data.StandardSc...   406.827850  12.593399   \n",
       "5   <class 'sklearn.preprocessing._data.MinMaxScal...   443.305725  12.500948   \n",
       "30  <class 'sklearn.preprocessing._data.MinMaxScal...   456.099213  12.087252   \n",
       "33  <class 'sklearn.preprocessing._data.StandardSc...   483.986328  12.050247   \n",
       "15  <class 'sklearn.preprocessing._data.MinMaxScal...   492.530121  13.382237   \n",
       "21  <class 'sklearn.preprocessing._data.MinMaxScal...   492.417572  12.767149   \n",
       "23  <class 'sklearn.preprocessing._data.MinMaxScal...   513.338318  13.462949   \n",
       "36  <class 'sklearn.preprocessing._data.StandardSc...   555.854492  11.598709   \n",
       "34  <class 'sklearn.preprocessing._data.StandardSc...   529.013794  13.925307   \n",
       "31  <class 'sklearn.preprocessing._data.StandardSc...   520.161377  14.117408   \n",
       "16  <class 'sklearn.preprocessing._data.MinMaxScal...   634.228088  15.367864   \n",
       "22  <class 'sklearn.preprocessing._data.StandardSc...   615.603760  13.877359   \n",
       "3   <class 'sklearn.preprocessing._data.MinMaxScal...   606.898926  13.702591   \n",
       "24  <class 'sklearn.preprocessing._data.StandardSc...   663.687500  15.409854   \n",
       "7   <class 'sklearn.preprocessing._data.StandardSc...   662.759583  13.560087   \n",
       "18  <class 'sklearn.preprocessing._data.MinMaxScal...   639.696411  15.031433   \n",
       "1   <class 'sklearn.preprocessing._data.MinMaxScal...   706.112976  14.086211   \n",
       "45  <class 'sklearn.preprocessing._data.MinMaxScal...   675.383057  14.122136   \n",
       "8   <class 'sklearn.preprocessing._data.MinMaxScal...   684.079712  15.909692   \n",
       "41  <class 'sklearn.preprocessing._data.MinMaxScal...   865.923889  15.202335   \n",
       "6   <class 'sklearn.preprocessing._data.StandardSc...   696.291321  14.334661   \n",
       "39  <class 'sklearn.preprocessing._data.StandardSc...   775.460449  14.744368   \n",
       "4   <class 'sklearn.preprocessing._data.MinMaxScal...   784.466553  17.545256   \n",
       "2   <class 'sklearn.preprocessing._data.StandardSc...   870.827454  17.582806   \n",
       "19  <class 'sklearn.preprocessing._data.StandardSc...   922.228821  15.370725   \n",
       "35  <class 'sklearn.preprocessing._data.StandardSc...   872.959717  14.153067   \n",
       "0   <class 'sklearn.preprocessing._data.StandardSc...  1053.068115  14.899381   \n",
       "44  <class 'sklearn.preprocessing._data.StandardSc...   850.296387  15.869330   \n",
       "27  <class 'sklearn.preprocessing._data.StandardSc...   970.601929  13.723557   \n",
       "32  <class 'sklearn.preprocessing._data.StandardSc...   995.794189  17.655621   \n",
       "40  <class 'sklearn.preprocessing._data.MinMaxScal...  1123.774658  21.696770   \n",
       "43  <class 'sklearn.preprocessing._data.StandardSc...  1272.195190  17.446192   \n",
       "13  <class 'sklearn.preprocessing._data.StandardSc...  1304.846680  18.532808   \n",
       "17  <class 'sklearn.preprocessing._data.MinMaxScal...  1249.239746  23.861946   \n",
       "28  <class 'sklearn.preprocessing._data.StandardSc...  1398.817627  16.185120   \n",
       "25  <class 'sklearn.preprocessing._data.StandardSc...  1571.534668  19.911650   \n",
       "14  <class 'sklearn.preprocessing._data.MinMaxScal...  2303.050537  26.078085   \n",
       "37  <class 'sklearn.preprocessing._data.MinMaxScal...  2453.873047  24.772991   \n",
       "10  <class 'sklearn.preprocessing._data.MinMaxScal...  1866.772583  27.344875   \n",
       "26  <class 'sklearn.preprocessing._data.MinMaxScal...  2424.874023  27.427322   \n",
       "20  <class 'sklearn.preprocessing._data.MinMaxScal...  2571.682617  25.798573   \n",
       "42  <class 'sklearn.preprocessing._data.MinMaxScal...  2238.039795  18.461998   \n",
       "9   <class 'sklearn.preprocessing._data.MinMaxScal...  2529.952393  26.131266   \n",
       "11  <class 'sklearn.preprocessing._data.MinMaxScal...  2983.060547  30.724981   \n",
       "\n",
       "    test_mae(t+1)  test_mae(t+2)  test_mae(t+3)  ...  test_mae(t+11)  \\\n",
       "29       3.778100       4.935235       5.915179  ...       12.376617   \n",
       "12       8.388412       9.880347      10.006425  ...       13.358921   \n",
       "38       6.765717       7.613912       8.365121  ...       13.468274   \n",
       "5        3.693830       4.283959       5.205354  ...       13.635902   \n",
       "30       4.626646       6.715505       8.265052  ...       13.107601   \n",
       "33       6.499933       6.763069       7.382070  ...       12.414900   \n",
       "15       4.648648       6.005950       7.197309  ...       14.322783   \n",
       "21      10.886259      10.267223       9.985346  ...       13.149528   \n",
       "23       5.290785       6.213699       7.302274  ...       13.636876   \n",
       "36       5.281905       5.901314       6.462759  ...       11.540359   \n",
       "34      10.291524       7.956648       8.333316  ...       14.794746   \n",
       "31       8.824968       9.530283      10.032682  ...       14.818234   \n",
       "16      13.861606       6.116435       6.202892  ...       17.255331   \n",
       "22       7.828645       7.554558       8.338289  ...       14.562158   \n",
       "3        3.943191       4.929643       6.199639  ...       14.859236   \n",
       "24      14.098557      14.238568      14.308229  ...       15.410071   \n",
       "7        7.855592       7.480397       8.068832  ...       13.729120   \n",
       "18      26.171202       7.267821       8.403698  ...       15.765282   \n",
       "1        5.320840       6.203963       7.472972  ...       15.304962   \n",
       "45       6.344134       6.712139       7.769174  ...       15.203131   \n",
       "8       16.279470       9.322615      11.208075  ...       16.354940   \n",
       "41      10.201191      11.156207      10.675257  ...       14.762134   \n",
       "6        4.491560       5.356360       6.497049  ...       15.356086   \n",
       "39       6.523486       7.249412       8.642371  ...       15.755215   \n",
       "4       27.541447      10.874722      10.017864  ...       17.323406   \n",
       "2       12.931618      13.667276      14.256495  ...       18.424906   \n",
       "19       8.709828       9.178472       9.917445  ...       15.803117   \n",
       "35       5.672068       6.369383       6.991834  ...       14.680539   \n",
       "0        6.602954       7.370401       8.048459  ...       15.253789   \n",
       "44       8.834017       8.318821       9.074226  ...       16.442091   \n",
       "27       5.622512       6.327600       7.275214  ...       14.364141   \n",
       "32      14.386787      14.380050      14.851608  ...       17.347822   \n",
       "40      24.962830      24.887920      24.185865  ...       20.841635   \n",
       "43      13.921982      13.878373      14.175632  ...       16.170933   \n",
       "13      15.984436      10.514296      12.141199  ...       18.900497   \n",
       "17      27.681740      27.131174      26.596161  ...       23.191870   \n",
       "28       4.451111       4.746300       5.950820  ...       17.316998   \n",
       "25      10.646804      12.710405      12.868601  ...       19.775427   \n",
       "14      27.192299      21.304930      22.674973  ...       27.043064   \n",
       "37      23.705402      22.339560      22.496401  ...       24.573008   \n",
       "10      29.914427      30.584156      29.111483  ...       26.906099   \n",
       "26      21.928265      21.628294      22.732355  ...       28.268524   \n",
       "20      22.609127      22.871740      23.472391  ...       25.751194   \n",
       "42       6.858403       8.017258       9.049136  ...       20.889761   \n",
       "9       18.512066      19.036589      20.126352  ...       26.968700   \n",
       "11      29.919691      30.373322      30.136894  ...       30.757483   \n",
       "\n",
       "    test_mae(t+12)  test_mae(t+13)  test_mae(t+14)  test_mae(t+15)  \\\n",
       "29       13.007839       13.633907       14.263512       14.920258   \n",
       "12       13.645789       13.990844       14.349859       14.669656   \n",
       "38       13.845459       14.167676       14.456120       14.748785   \n",
       "5        14.493777       15.292961       16.010576       16.731924   \n",
       "30       13.613174       14.048625       14.454420       14.844568   \n",
       "33       13.128962       13.799572       14.418126       15.001484   \n",
       "15       14.980151       15.623283       16.238644       16.897707   \n",
       "21       13.611848       13.995646       14.344925       14.654423   \n",
       "23       14.471559       15.364357       16.282177       17.208750   \n",
       "36       12.110660       12.728726       13.400656       14.205097   \n",
       "34       15.399143       15.964369       16.449862       16.894203   \n",
       "31       15.214798       15.651771       16.024471       16.392265   \n",
       "16       17.876137       18.364826       18.826605       19.294817   \n",
       "22       15.185158       15.812385       16.420794       16.974445   \n",
       "3        15.702940       16.506563       17.246296       18.005705   \n",
       "24       15.503851       15.596975       15.707144       15.876019   \n",
       "7        14.448841       15.317167       16.092915       17.021358   \n",
       "18       16.066397       16.321272       16.505648       16.664925   \n",
       "1        16.044222       16.744120       17.387867       17.973007   \n",
       "45       15.857417       16.507896       17.144571       17.739826   \n",
       "8        17.054209       17.702763       18.302721       18.880907   \n",
       "41       15.525005       16.344530       17.144669       18.004837   \n",
       "6        16.356821       17.320501       18.186972       19.015043   \n",
       "39       16.620220       17.434778       18.139967       18.733814   \n",
       "4        18.089382       18.867624       19.602230       20.291792   \n",
       "2        18.719252       18.987040       19.240505       19.481171   \n",
       "19       16.493097       17.157476       17.849754       18.602829   \n",
       "35       15.687491       16.687002       17.704224       18.527758   \n",
       "0        16.229065       17.169615       18.072643       18.977373   \n",
       "44       17.334753       18.208447       19.097219       19.994881   \n",
       "27       15.170038       15.976849       16.770655       17.561710   \n",
       "32       17.551104       18.112757       18.542997       19.155266   \n",
       "40       20.619600       20.445692       20.330851       20.287035   \n",
       "43       16.977177       17.879543       18.826962       19.783237   \n",
       "13       19.336975       19.878744       20.487968       21.144838   \n",
       "17       22.880596       22.608377       22.377552       22.217377   \n",
       "28       18.654621       19.892683       21.163805       22.344378   \n",
       "25       20.954788       22.192665       23.455677       24.692188   \n",
       "14       27.228987       27.370951       27.466963       27.530399   \n",
       "37       24.917761       25.307981       25.669456       26.092098   \n",
       "10       26.683458       26.434025       26.301178       26.210037   \n",
       "26       28.791918       29.283121       29.739248       30.144047   \n",
       "20       26.026449       26.307207       26.589748       26.957422   \n",
       "42       21.887777       22.764872       23.488838       24.036427   \n",
       "9        27.602985       28.264090       28.891703       29.477776   \n",
       "11       30.842653       30.878279       30.959229       30.943300   \n",
       "\n",
       "    test_mae(t+16)  test_mae(t+17)  test_mae(t+18)  test_mae(t+19)  \\\n",
       "29       15.531412       16.099224       16.683821       17.256342   \n",
       "12       15.053366       15.370562       15.586323       15.798634   \n",
       "38       15.063616       15.360583       15.663295       16.004938   \n",
       "5        17.372627       18.027727       18.646788       19.265472   \n",
       "30       15.207379       15.532366       15.824340       16.075926   \n",
       "33       15.375210       15.975498       16.462032       17.175732   \n",
       "15       17.515383       18.128284       18.703037       19.297628   \n",
       "21       14.907431       15.129011       15.319042       15.537729   \n",
       "23       18.127089       19.030231       19.875698       20.663685   \n",
       "36       15.057336       15.980198       16.984249       18.057579   \n",
       "34       17.319576       17.739185       18.148319       18.582518   \n",
       "31       16.754742       17.105921       17.433689       17.764273   \n",
       "16       19.725729       20.088127       20.413914       20.740185   \n",
       "22       17.505888       18.054600       18.589483       19.133724   \n",
       "3        18.741405       19.450014       20.108582       20.715981   \n",
       "24       16.060770       16.303602       16.555473       16.807163   \n",
       "7        17.732309       18.581205       19.293995       19.980360   \n",
       "18       16.785013       16.864689       16.939903       16.998920   \n",
       "1        18.481647       18.921314       19.276642       19.614832   \n",
       "45       18.283808       18.794672       19.302101       19.873262   \n",
       "8        19.436762       19.973253       20.482304       20.987247   \n",
       "41       18.956707       19.820560       20.570244       21.282341   \n",
       "6        19.818275       20.570786       21.317150       22.038813   \n",
       "39       19.302925       19.821541       20.240486       20.523680   \n",
       "4        20.900875       21.416956       21.869158       22.305794   \n",
       "2        19.684546       19.844309       19.968094       20.086826   \n",
       "19       19.323193       20.026464       20.644396       21.308001   \n",
       "35       19.242878       20.142241       20.870043       21.810358   \n",
       "0        19.857183       20.748529       21.677843       22.650225   \n",
       "44       20.745625       21.429533       21.995888       22.477907   \n",
       "27       18.364143       19.110636       19.784697       20.445417   \n",
       "32       19.640114       20.183678       20.766685       21.438229   \n",
       "40       20.256460       20.234434       20.233940       20.255970   \n",
       "43       20.725595       21.666378       22.582603       23.486774   \n",
       "13       21.847029       22.528599       23.188278       23.889082   \n",
       "17       22.071198       21.957775       21.863983       21.785938   \n",
       "28       23.448616       24.658430       25.654512       26.854719   \n",
       "25       25.870413       26.976841       28.005070       28.940859   \n",
       "14       27.583254       27.614235       27.625099       27.700760   \n",
       "37       26.500811       26.909277       27.304714       27.690290   \n",
       "10       26.082321       26.049553       26.092022       26.229645   \n",
       "26       30.493069       30.819393       31.146364       31.456766   \n",
       "20       27.380726       27.819260       28.299473       28.744661   \n",
       "42       24.459564       24.788841       25.059458       25.333035   \n",
       "9        30.045944       30.613148       31.176956       31.773968   \n",
       "11       31.013866       30.951878       31.119537       31.071012   \n",
       "\n",
       "    test_mae(t+20)  \n",
       "29       17.777834  \n",
       "12       15.899250  \n",
       "38       16.381424  \n",
       "5        19.899242  \n",
       "30       16.363590  \n",
       "33       17.527330  \n",
       "15       19.869844  \n",
       "21       15.761189  \n",
       "23       21.355709  \n",
       "36       19.119024  \n",
       "34       19.032888  \n",
       "31       18.080326  \n",
       "16       21.033175  \n",
       "22       19.610550  \n",
       "3        21.310787  \n",
       "24       17.055710  \n",
       "7        20.639826  \n",
       "18       17.026264  \n",
       "1        19.877411  \n",
       "45       20.499794  \n",
       "8        21.464321  \n",
       "41       21.971457  \n",
       "6        22.742344  \n",
       "39       20.653320  \n",
       "4        22.722769  \n",
       "2        20.199846  \n",
       "19       21.939484  \n",
       "35       22.743580  \n",
       "0        23.624945  \n",
       "44       22.848272  \n",
       "27       21.034103  \n",
       "32       22.122795  \n",
       "40       20.274055  \n",
       "43       24.339378  \n",
       "13       24.600050  \n",
       "17       21.703148  \n",
       "28       27.796484  \n",
       "25       29.753147  \n",
       "14       27.750038  \n",
       "37       28.070812  \n",
       "10       26.407499  \n",
       "26       31.712200  \n",
       "20       29.232882  \n",
       "42       25.586086  \n",
       "9        32.308380  \n",
       "11       31.414185  \n",
       "\n",
       "[46 rows x 27 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ca marcheanalyze_object = talos.Analyze(scan_object)\n",
    "print(\"MAE\", analyze_object.low('mae'))\n",
    "print(\"MSE\", analyze_object.low('mse'))\n",
    "print(\"RMSE\", analyze_object.low('root_mean_squared_error'))\n",
    "print(\"TEST MAE\", analyze_object.low('test_mae'))\n",
    "print(\"TEST MSE\", analyze_object.low('test_mse'))\n",
    "print(\"TEST RMSE\", analyze_object.low('test_root_mean_squared_error'))\n",
    "df = analyze_object.table('test_root_mean_squared_error', ascending=True)\n",
    "print(df.columns)\n",
    "columns = [f\"test_mae(t+{i})\" for i in range(1, n_forecast+1)]\n",
    "df[['hidden_1', 'hidden_2', 'lr', 'batch_size_div', 'scaling', 'test_mse', 'test_mae'] + columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 estimators combined\n",
    "The first estimator should be simple and output an estimate $ \\hat{y}_{(1)}(t) $ of $ y(t) $\n",
    "The correction factor ($ C(t) $) between the estimation and the real value is then computed:\n",
    "\n",
    "$$ y(t) = \\hat{y}_{(1)}(t) * ( 1 + C(t)) $$\n",
    "\n",
    "$$ C(t) = \\frac{y(t) - \\hat{y}_{(1)}(t)}{\\hat{y}_{(1)}(t)} $$\n",
    "\n",
    "This formula is used such that $C(t)$ should be bounded in [-1, 1] if the target is a positive value and the first estimator is relatively closed to the expected value.\n",
    "\n",
    "Anoter estimator is then trained based on the first one. The goal is to be able to predict the correction $ C(t) $ that should be done on the first estimator, in order to reduce the estimation error and get a better prediction. The prediction of $ C(t) $ is written $ \\hat{C}(t) $. Once this estimator is computed, the final prediction can be constructed:\n",
    "\n",
    "$$ \\hat{y}(t) = \\hat{y}_{(1)}(t) * ( 1 + \\hat{C}(t)) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First estimator\n",
    "Construct the predition $\\hat{y}_{(1)}(t)$ and compute $C(t)$ based on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 484ms/step - loss: 0.4527 - mse: 0.4527 - mae: 0.4760 - root_mean_squared_error: 0.6728\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.4113 - mse: 0.4113 - mae: 0.4485 - root_mean_squared_error: 0.6413\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3840 - mse: 0.3840 - mae: 0.4300 - root_mean_squared_error: 0.6197\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3626 - mse: 0.3626 - mae: 0.4154 - root_mean_squared_error: 0.6022\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3447 - mse: 0.3447 - mae: 0.4029 - root_mean_squared_error: 0.5871\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3291 - mse: 0.3291 - mae: 0.3919 - root_mean_squared_error: 0.5736\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3151 - mse: 0.3151 - mae: 0.3819 - root_mean_squared_error: 0.5613\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3024 - mse: 0.3024 - mae: 0.3727 - root_mean_squared_error: 0.5499\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2907 - mse: 0.2907 - mae: 0.3641 - root_mean_squared_error: 0.5391\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2798 - mse: 0.2798 - mae: 0.3561 - root_mean_squared_error: 0.5290\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2696 - mse: 0.2696 - mae: 0.3485 - root_mean_squared_error: 0.5193\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2601 - mse: 0.2601 - mae: 0.3413 - root_mean_squared_error: 0.5100\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2511 - mse: 0.2511 - mae: 0.3344 - root_mean_squared_error: 0.5011\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2425 - mse: 0.2425 - mae: 0.3278 - root_mean_squared_error: 0.4925\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2344 - mse: 0.2344 - mae: 0.3214 - root_mean_squared_error: 0.4841\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2266 - mse: 0.2266 - mae: 0.3152 - root_mean_squared_error: 0.4761\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2192 - mse: 0.2192 - mae: 0.3091 - root_mean_squared_error: 0.4682\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2121 - mse: 0.2121 - mae: 0.3033 - root_mean_squared_error: 0.4606\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2053 - mse: 0.2053 - mae: 0.2976 - root_mean_squared_error: 0.4531\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1988 - mse: 0.1988 - mae: 0.2921 - root_mean_squared_error: 0.4459\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1925 - mse: 0.1925 - mae: 0.2867 - root_mean_squared_error: 0.4388\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1865 - mse: 0.1865 - mae: 0.2815 - root_mean_squared_error: 0.4318\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1807 - mse: 0.1807 - mae: 0.2764 - root_mean_squared_error: 0.4251\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1751 - mse: 0.1751 - mae: 0.2714 - root_mean_squared_error: 0.4184\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1697 - mse: 0.1697 - mae: 0.2666 - root_mean_squared_error: 0.4119\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1645 - mse: 0.1645 - mae: 0.2619 - root_mean_squared_error: 0.4056\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1595 - mse: 0.1595 - mae: 0.2573 - root_mean_squared_error: 0.3993\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1546 - mse: 0.1546 - mae: 0.2529 - root_mean_squared_error: 0.3932\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1499 - mse: 0.1499 - mae: 0.2486 - root_mean_squared_error: 0.3872\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1454 - mse: 0.1454 - mae: 0.2444 - root_mean_squared_error: 0.3813\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1411 - mse: 0.1411 - mae: 0.2404 - root_mean_squared_error: 0.3756\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1369 - mse: 0.1369 - mae: 0.2365 - root_mean_squared_error: 0.3700\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1328 - mse: 0.1328 - mae: 0.2328 - root_mean_squared_error: 0.3644\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1289 - mse: 0.1289 - mae: 0.2292 - root_mean_squared_error: 0.3590\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1251 - mse: 0.1251 - mae: 0.2256 - root_mean_squared_error: 0.3537\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1215 - mse: 0.1215 - mae: 0.2222 - root_mean_squared_error: 0.3485\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1179 - mse: 0.1179 - mae: 0.2188 - root_mean_squared_error: 0.3434\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1145 - mse: 0.1145 - mae: 0.2156 - root_mean_squared_error: 0.3384\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1113 - mse: 0.1113 - mae: 0.2124 - root_mean_squared_error: 0.3336\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1081 - mse: 0.1081 - mae: 0.2093 - root_mean_squared_error: 0.3288\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1051 - mse: 0.1051 - mae: 0.2063 - root_mean_squared_error: 0.3241\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1021 - mse: 0.1021 - mae: 0.2033 - root_mean_squared_error: 0.3196\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0993 - mse: 0.0993 - mae: 0.2005 - root_mean_squared_error: 0.3151\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0966 - mse: 0.0966 - mae: 0.1977 - root_mean_squared_error: 0.3108\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0939 - mse: 0.0939 - mae: 0.1950 - root_mean_squared_error: 0.3065\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0914 - mse: 0.0914 - mae: 0.1924 - root_mean_squared_error: 0.3023\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0890 - mse: 0.0890 - mae: 0.1898 - root_mean_squared_error: 0.2983\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0866 - mse: 0.0866 - mae: 0.1873 - root_mean_squared_error: 0.2943\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0844 - mse: 0.0844 - mae: 0.1849 - root_mean_squared_error: 0.2905\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0822 - mse: 0.0822 - mae: 0.1825 - root_mean_squared_error: 0.2867\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0801 - mse: 0.0801 - mae: 0.1803 - root_mean_squared_error: 0.2830\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0781 - mse: 0.0781 - mae: 0.1781 - root_mean_squared_error: 0.2794\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0761 - mse: 0.0761 - mae: 0.1759 - root_mean_squared_error: 0.2759\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0743 - mse: 0.0743 - mae: 0.1738 - root_mean_squared_error: 0.2725\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0725 - mse: 0.0725 - mae: 0.1718 - root_mean_squared_error: 0.2692\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0707 - mse: 0.0707 - mae: 0.1699 - root_mean_squared_error: 0.2660\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0691 - mse: 0.0691 - mae: 0.1680 - root_mean_squared_error: 0.2628\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0675 - mse: 0.0675 - mae: 0.1661 - root_mean_squared_error: 0.2598\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0659 - mse: 0.0659 - mae: 0.1643 - root_mean_squared_error: 0.2568\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0645 - mse: 0.0645 - mae: 0.1625 - root_mean_squared_error: 0.2539\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0630 - mse: 0.0630 - mae: 0.1608 - root_mean_squared_error: 0.2511\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0617 - mse: 0.0617 - mae: 0.1591 - root_mean_squared_error: 0.2483\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0603 - mse: 0.0603 - mae: 0.1575 - root_mean_squared_error: 0.2456\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0591 - mse: 0.0591 - mae: 0.1559 - root_mean_squared_error: 0.2430\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0578 - mse: 0.0578 - mae: 0.1543 - root_mean_squared_error: 0.2405\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0567 - mse: 0.0567 - mae: 0.1528 - root_mean_squared_error: 0.2381\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0555 - mse: 0.0555 - mae: 0.1513 - root_mean_squared_error: 0.2357\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0545 - mse: 0.0545 - mae: 0.1498 - root_mean_squared_error: 0.2334\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0534 - mse: 0.0534 - mae: 0.1484 - root_mean_squared_error: 0.2311\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0524 - mse: 0.0524 - mae: 0.1470 - root_mean_squared_error: 0.2289\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0514 - mse: 0.0514 - mae: 0.1457 - root_mean_squared_error: 0.2268\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0505 - mse: 0.0505 - mae: 0.1444 - root_mean_squared_error: 0.2248\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0496 - mse: 0.0496 - mae: 0.1431 - root_mean_squared_error: 0.2228\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0488 - mse: 0.0488 - mae: 0.1419 - root_mean_squared_error: 0.2208\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0479 - mse: 0.0479 - mae: 0.1407 - root_mean_squared_error: 0.2190\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0472 - mse: 0.0472 - mae: 0.1396 - root_mean_squared_error: 0.2171\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0464 - mse: 0.0464 - mae: 0.1385 - root_mean_squared_error: 0.2154\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0457 - mse: 0.0457 - mae: 0.1374 - root_mean_squared_error: 0.2137\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0450 - mse: 0.0450 - mae: 0.1364 - root_mean_squared_error: 0.2120\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0443 - mse: 0.0443 - mae: 0.1354 - root_mean_squared_error: 0.2104\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0436 - mse: 0.0436 - mae: 0.1345 - root_mean_squared_error: 0.2089\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0430 - mse: 0.0430 - mae: 0.1335 - root_mean_squared_error: 0.2074\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0424 - mse: 0.0424 - mae: 0.1327 - root_mean_squared_error: 0.2059\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0418 - mse: 0.0418 - mae: 0.1318 - root_mean_squared_error: 0.2046\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0413 - mse: 0.0413 - mae: 0.1311 - root_mean_squared_error: 0.2032\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0408 - mse: 0.0408 - mae: 0.1303 - root_mean_squared_error: 0.2019\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0402 - mse: 0.0402 - mae: 0.1295 - root_mean_squared_error: 0.2006\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0397 - mse: 0.0397 - mae: 0.1288 - root_mean_squared_error: 0.1994\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0393 - mse: 0.0393 - mae: 0.1281 - root_mean_squared_error: 0.1982\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0388 - mse: 0.0388 - mae: 0.1274 - root_mean_squared_error: 0.1970\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0384 - mse: 0.0384 - mae: 0.1267 - root_mean_squared_error: 0.1959\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0379 - mse: 0.0379 - mae: 0.1261 - root_mean_squared_error: 0.1948\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0375 - mse: 0.0375 - mae: 0.1255 - root_mean_squared_error: 0.1937\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0371 - mse: 0.0371 - mae: 0.1250 - root_mean_squared_error: 0.1926\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0367 - mse: 0.0367 - mae: 0.1243 - root_mean_squared_error: 0.1916\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0363 - mse: 0.0363 - mae: 0.1239 - root_mean_squared_error: 0.1906\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0360 - mse: 0.0360 - mae: 0.1231 - root_mean_squared_error: 0.1896\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0356 - mse: 0.0356 - mae: 0.1226 - root_mean_squared_error: 0.1887\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0352 - mse: 0.0352 - mae: 0.1220 - root_mean_squared_error: 0.1877\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0349 - mse: 0.0349 - mae: 0.1214 - root_mean_squared_error: 0.1868\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0346 - mse: 0.0346 - mae: 0.1209 - root_mean_squared_error: 0.1859\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0342 - mse: 0.0342 - mae: 0.1203 - root_mean_squared_error: 0.1850\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0339 - mse: 0.0339 - mae: 0.1198 - root_mean_squared_error: 0.1841\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0336 - mse: 0.0336 - mae: 0.1192 - root_mean_squared_error: 0.1832\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0332 - mse: 0.0332 - mae: 0.1186 - root_mean_squared_error: 0.1823\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0329 - mse: 0.0329 - mae: 0.1181 - root_mean_squared_error: 0.1814\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0326 - mse: 0.0326 - mae: 0.1175 - root_mean_squared_error: 0.1806\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0323 - mse: 0.0323 - mae: 0.1169 - root_mean_squared_error: 0.1797\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0320 - mse: 0.0320 - mae: 0.1164 - root_mean_squared_error: 0.1789\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0317 - mse: 0.0317 - mae: 0.1158 - root_mean_squared_error: 0.1781\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0314 - mse: 0.0314 - mae: 0.1153 - root_mean_squared_error: 0.1772\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0311 - mse: 0.0311 - mae: 0.1148 - root_mean_squared_error: 0.1764\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0308 - mse: 0.0308 - mae: 0.1142 - root_mean_squared_error: 0.1756\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0306 - mse: 0.0306 - mae: 0.1137 - root_mean_squared_error: 0.1748\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0303 - mse: 0.0303 - mae: 0.1134 - root_mean_squared_error: 0.1740\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0300 - mse: 0.0300 - mae: 0.1127 - root_mean_squared_error: 0.1732\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0297 - mse: 0.0297 - mae: 0.1122 - root_mean_squared_error: 0.1724\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0295 - mse: 0.0295 - mae: 0.1116 - root_mean_squared_error: 0.1716\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0292 - mse: 0.0292 - mae: 0.1112 - root_mean_squared_error: 0.1709\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0289 - mse: 0.0289 - mae: 0.1107 - root_mean_squared_error: 0.1701\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0287 - mse: 0.0287 - mae: 0.1102 - root_mean_squared_error: 0.1694\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0284 - mse: 0.0284 - mae: 0.1097 - root_mean_squared_error: 0.1686\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0282 - mse: 0.0282 - mae: 0.1093 - root_mean_squared_error: 0.1679\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0279 - mse: 0.0279 - mae: 0.1088 - root_mean_squared_error: 0.1672\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0277 - mse: 0.0277 - mae: 0.1084 - root_mean_squared_error: 0.1665\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0275 - mse: 0.0275 - mae: 0.1080 - root_mean_squared_error: 0.1658\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0273 - mse: 0.0273 - mae: 0.1075 - root_mean_squared_error: 0.1651\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0270 - mse: 0.0270 - mae: 0.1071 - root_mean_squared_error: 0.1644\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0268 - mse: 0.0268 - mae: 0.1067 - root_mean_squared_error: 0.1637\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0266 - mse: 0.0266 - mae: 0.1063 - root_mean_squared_error: 0.1631\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0264 - mse: 0.0264 - mae: 0.1061 - root_mean_squared_error: 0.1624\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0262 - mse: 0.0262 - mae: 0.1055 - root_mean_squared_error: 0.1618\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0260 - mse: 0.0260 - mae: 0.1053 - root_mean_squared_error: 0.1612\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0258 - mse: 0.0258 - mae: 0.1049 - root_mean_squared_error: 0.1606\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0256 - mse: 0.0256 - mae: 0.1046 - root_mean_squared_error: 0.1600\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0254 - mse: 0.0254 - mae: 0.1042 - root_mean_squared_error: 0.1593\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0252 - mse: 0.0252 - mae: 0.1038 - root_mean_squared_error: 0.1587\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0250 - mse: 0.0250 - mae: 0.1036 - root_mean_squared_error: 0.1582\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0248 - mse: 0.0248 - mae: 0.1032 - root_mean_squared_error: 0.1576\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0247 - mse: 0.0247 - mae: 0.1030 - root_mean_squared_error: 0.1571\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0245 - mse: 0.0245 - mae: 0.1026 - root_mean_squared_error: 0.1565\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0243 - mse: 0.0243 - mae: 0.1023 - root_mean_squared_error: 0.1560\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0242 - mse: 0.0242 - mae: 0.1020 - root_mean_squared_error: 0.1554\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0240 - mse: 0.0240 - mae: 0.1018 - root_mean_squared_error: 0.1549\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0238 - mse: 0.0238 - mae: 0.1015 - root_mean_squared_error: 0.1544\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0237 - mse: 0.0237 - mae: 0.1013 - root_mean_squared_error: 0.1539\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0236 - mse: 0.0236 - mae: 0.1010 - root_mean_squared_error: 0.1535\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0234 - mse: 0.0234 - mae: 0.1009 - root_mean_squared_error: 0.1530\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0233 - mse: 0.0233 - mae: 0.1005 - root_mean_squared_error: 0.1525\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0231 - mse: 0.0231 - mae: 0.1003 - root_mean_squared_error: 0.1521\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0230 - mse: 0.0230 - mae: 0.1001 - root_mean_squared_error: 0.1516\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0229 - mse: 0.0229 - mae: 0.0998 - root_mean_squared_error: 0.1512\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0227 - mse: 0.0227 - mae: 0.0998 - root_mean_squared_error: 0.1508\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0226 - mse: 0.0226 - mae: 0.0993 - root_mean_squared_error: 0.1503\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0225 - mse: 0.0225 - mae: 0.0994 - root_mean_squared_error: 0.1499\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0223 - mse: 0.0223 - mae: 0.0990 - root_mean_squared_error: 0.1495\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0222 - mse: 0.0222 - mae: 0.0988 - root_mean_squared_error: 0.1491\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0221 - mse: 0.0221 - mae: 0.0986 - root_mean_squared_error: 0.1487\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0220 - mse: 0.0220 - mae: 0.0984 - root_mean_squared_error: 0.1483\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0219 - mse: 0.0219 - mae: 0.0983 - root_mean_squared_error: 0.1479\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0218 - mse: 0.0218 - mae: 0.0980 - root_mean_squared_error: 0.1476\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0217 - mse: 0.0217 - mae: 0.0980 - root_mean_squared_error: 0.1472\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0216 - mse: 0.0216 - mae: 0.0976 - root_mean_squared_error: 0.1469\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0215 - mse: 0.0215 - mae: 0.0978 - root_mean_squared_error: 0.1465\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0214 - mse: 0.0214 - mae: 0.0972 - root_mean_squared_error: 0.1462\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0213 - mse: 0.0213 - mae: 0.0974 - root_mean_squared_error: 0.1458\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0212 - mse: 0.0212 - mae: 0.0970 - root_mean_squared_error: 0.1455\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0211 - mse: 0.0211 - mae: 0.0969 - root_mean_squared_error: 0.1452\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0210 - mse: 0.0210 - mae: 0.0968 - root_mean_squared_error: 0.1449\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0209 - mse: 0.0209 - mae: 0.0966 - root_mean_squared_error: 0.1446\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0208 - mse: 0.0208 - mae: 0.0965 - root_mean_squared_error: 0.1442\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0207 - mse: 0.0207 - mae: 0.0962 - root_mean_squared_error: 0.1439\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0206 - mse: 0.0206 - mae: 0.0962 - root_mean_squared_error: 0.1436\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0206 - mse: 0.0206 - mae: 0.0959 - root_mean_squared_error: 0.1434\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0205 - mse: 0.0205 - mae: 0.0959 - root_mean_squared_error: 0.1431\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0204 - mse: 0.0204 - mae: 0.0957 - root_mean_squared_error: 0.1428\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0203 - mse: 0.0203 - mae: 0.0956 - root_mean_squared_error: 0.1425\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.0955 - root_mean_squared_error: 0.1423\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0202 - mse: 0.0202 - mae: 0.0955 - root_mean_squared_error: 0.1420\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.0952 - root_mean_squared_error: 0.1418\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0200 - mse: 0.0200 - mae: 0.0950 - root_mean_squared_error: 0.1415\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.0950 - root_mean_squared_error: 0.1412\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.0948 - root_mean_squared_error: 0.1410\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.0946 - root_mean_squared_error: 0.1408\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.0947 - root_mean_squared_error: 0.1405\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0944 - root_mean_squared_error: 0.1403\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0944 - root_mean_squared_error: 0.1401\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0942 - root_mean_squared_error: 0.1398\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0941 - root_mean_squared_error: 0.1396\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0194 - mse: 0.0194 - mae: 0.0940 - root_mean_squared_error: 0.1394\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0194 - mse: 0.0194 - mae: 0.0938 - root_mean_squared_error: 0.1392\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193 - mse: 0.0193 - mae: 0.0939 - root_mean_squared_error: 0.1390\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193 - mse: 0.0193 - mae: 0.0936 - root_mean_squared_error: 0.1388\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0937 - root_mean_squared_error: 0.1386\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0934 - root_mean_squared_error: 0.1384\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0935 - root_mean_squared_error: 0.1382\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0933 - root_mean_squared_error: 0.1380\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0932 - root_mean_squared_error: 0.1378\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0931 - root_mean_squared_error: 0.1377\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0931 - root_mean_squared_error: 0.1375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndf_predicted_1 = dg.inverse_transform_y(Y_train_val_pred, idx=train_val_idx, return_type='dict_df')\\ndf_predicted_1\\n\\n# predictions on the training set\\nmodel_prediction = model_generator(batch_input_shape=(batch_size_train, n_samples, dg.n_features))\\nmodel_prediction.set_weights(model.get_weights())\\nY_train_pred = model_prediction.predict(X_train, batch_size=batch_size_train)\\n\\n# predictions on the validation without augmented regions\\nmodel_prediction = model_generator(batch_input_shape=(batch_size_val, n_samples, dg.n_features))\\nmodel_prediction.set_weights(model.get_weights())\\nY_val_pred = model_prediction.predict(X_val, batch_size=batch_size_val)\\ndf_Y_real = {loc: dg.df[loc][dg.target_columns].iloc[train_val_idx] for loc in dg.df}\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X% for training, remaining for test\n",
    "ratio_training = 0.8\n",
    "epochs = 200\n",
    "\n",
    "nb_datapoints = dg.batch_size\n",
    "max_train = int(ratio_training * nb_datapoints)\n",
    "train_idx = np.array(range(max_train))\n",
    "valid_idx = np.array([])\n",
    "test_idx = np.array(range(max_train, nb_datapoints))\n",
    "\n",
    "X_train_1 = dg.get_x(train_idx, scaled=True)\n",
    "Y_train = dg.get_y(train_idx, scaled=True)\n",
    "\n",
    "if len(valid_idx) > 0:\n",
    "    X_val_1 = dg.get_x(val_idx, geo=dg.loc_init, scaled=True, use_previous_scaler=True)\n",
    "    Y_val_unscaled = dg.get_y(val_idx, geo=dg.loc_init, scaled=False)\n",
    "    Y_val_real = dg.remove_padded_y(Y_val_unscaled, idx=val_idx, geo=dg.loc_init)\n",
    "\n",
    "X_test_1 = dg.get_x(test_idx, scaled=True, geo=dg.loc_init, use_previous_scaler=True)\n",
    "Y_test_unscaled = dg.get_y(test_idx, scaled=False, geo=dg.loc_init)\n",
    "Y_test_real = dg.remove_padded_y(Y_test_unscaled, idx=test_idx, geo=dg.loc_init)\n",
    "    \n",
    "\"\"\"\n",
    "X_train_val = dg.get_x(train_val_idx, scaled=True)\n",
    "df_Y_real = {loc: dg.df[loc][dg.target_columns].iloc[train_val_idx] for loc in dg.df}\n",
    "Y_real_train_val = dg.get_y(train_val_idx, scaled=True, use_previous_scaler=True)\n",
    "\"\"\"\n",
    "\n",
    "model_generator = get_dense_model\n",
    "\n",
    "batch_size_train = len(X_train_1)\n",
    "batch_size_test = len(X_test_1)\n",
    "\n",
    "if len(valid_idx) > 0:  # use validation set for an early stop\n",
    "    batch_size_val = len(X_val_1)\n",
    "    model_validation = model_generator(batch_input_shape=(batch_size_val, n_samples, dg.n_features))\n",
    "    val_log = ValidationLogger(model_validation, len(X_val), X_val, Y_val_real, valid_idx, dg.loc_init)\n",
    "    callbacks = [val_log, EarlyStopping(monitor='val_root_mean_squared_error', mode='min', verbose=1, patience=20)]\n",
    "else:\n",
    "    callbacks = None\n",
    "\n",
    "model = model_generator(batch_input_shape=(batch_size_train, n_samples, dg.n_features))\n",
    "history = model.fit(X_train_1, Y_train, batch_size=batch_size_train, epochs=epochs, callbacks=callbacks)\n",
    "# compute the predictions on both the training and the validation\n",
    "Y_train_pred_1 = model.predict(X_train_1, batch_size=batch_size_train)\n",
    "\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_test, n_samples, dg.n_features))\n",
    "model_prediction.set_weights(model.get_weights())\n",
    "Y_test_pred_1 = model_prediction.predict(X_test_1, batch_size=batch_size_test)\n",
    "# unscale and unpad the data\n",
    "Y_test_pred_unscaled_1 = dg.inverse_transform_y(Y_test_pred_1, idx=test_idx, geo=dg.loc_init)\n",
    "Y_test_pred_real_1 = dg.remove_padded_y(Y_test_pred_unscaled_1, idx=test_idx, geo=dg.loc_init)\n",
    "\n",
    "\"\"\"\n",
    "df_predicted_1 = dg.inverse_transform_y(Y_train_val_pred, idx=train_val_idx, return_type='dict_df')\n",
    "df_predicted_1\n",
    "\n",
    "# predictions on the training set\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_train, n_samples, dg.n_features))\n",
    "model_prediction.set_weights(model.get_weights())\n",
    "Y_train_pred = model_prediction.predict(X_train, batch_size=batch_size_train)\n",
    "\n",
    "# predictions on the validation without augmented regions\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_val, n_samples, dg.n_features))\n",
    "model_prediction.set_weights(model.get_weights())\n",
    "Y_val_pred = model_prediction.predict(X_val, batch_size=batch_size_val)\n",
    "df_Y_real = {loc: dg.df[loc][dg.target_columns].iloc[train_val_idx] for loc in dg.df}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor k in dg.df:\\n    # add the data columns to the dataframe\\n    df_c[k] = dg.df[k][data_dg_2]\\n    for t in range(1, n_forecast+1):  # add the target columns\\n        target_t = f'{target}(t+{t})'\\n        df_c[k][f'C(t+{t})'] = (df_Y_real[k][target_t] - df_predicted_1[k][target_t]) / df_predicted_1[k][target_t]\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the corrections and add them to a dataframe\n",
    "data_dg_2 = [f'{topic}(t{i})' for i in range(-n_samples+1, 0, 1) for topic in list_topics] + [topic for topic in list_topics]\n",
    "target_df_2 = [f'{target}(t+{i})' for i in range(1, n_forecast+1)]\n",
    "df_c = {loc : dg.df[loc][data_dg_2 + target_df_2] for loc in dg.df}\n",
    "\"\"\"\n",
    "for k in dg.df:\n",
    "    # add the data columns to the dataframe\n",
    "    df_c[k] = dg.df[k][data_dg_2]\n",
    "    for t in range(1, n_forecast+1):  # add the target columns\n",
    "        target_t = f'{target}(t+{t})'\n",
    "        df_c[k][f'C(t+{t})'] = (df_Y_real[k][target_t] - df_predicted_1[k][target_t]) / df_predicted_1[k][target_t]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Fièvre(t-29)</th>\n",
       "      <th>Mal de gorge(t-29)</th>\n",
       "      <th>Symptôme(t-29)</th>\n",
       "      <th>Fièvre(t-28)</th>\n",
       "      <th>Mal de gorge(t-28)</th>\n",
       "      <th>Symptôme(t-28)</th>\n",
       "      <th>Fièvre(t-27)</th>\n",
       "      <th>Mal de gorge(t-27)</th>\n",
       "      <th>Symptôme(t-27)</th>\n",
       "      <th>Fièvre(t-26)</th>\n",
       "      <th>...</th>\n",
       "      <th>TOT_HOSP(t+11)</th>\n",
       "      <th>TOT_HOSP(t+12)</th>\n",
       "      <th>TOT_HOSP(t+13)</th>\n",
       "      <th>TOT_HOSP(t+14)</th>\n",
       "      <th>TOT_HOSP(t+15)</th>\n",
       "      <th>TOT_HOSP(t+16)</th>\n",
       "      <th>TOT_HOSP(t+17)</th>\n",
       "      <th>TOT_HOSP(t+18)</th>\n",
       "      <th>TOT_HOSP(t+19)</th>\n",
       "      <th>TOT_HOSP(t+20)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">BE</th>\n",
       "      <th>2020-03-04</th>\n",
       "      <td>22.444890</td>\n",
       "      <td>18.401937</td>\n",
       "      <td>19.524793</td>\n",
       "      <td>20.813055</td>\n",
       "      <td>17.578692</td>\n",
       "      <td>19.672373</td>\n",
       "      <td>18.436874</td>\n",
       "      <td>16.416465</td>\n",
       "      <td>18.816411</td>\n",
       "      <td>17.492127</td>\n",
       "      <td>...</td>\n",
       "      <td>254.142857</td>\n",
       "      <td>374.714286</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>729.714286</td>\n",
       "      <td>926.857143</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1377.714286</td>\n",
       "      <td>1674.142857</td>\n",
       "      <td>1993.142857</td>\n",
       "      <td>2357.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>20.813055</td>\n",
       "      <td>17.578692</td>\n",
       "      <td>19.672373</td>\n",
       "      <td>18.436874</td>\n",
       "      <td>16.416465</td>\n",
       "      <td>18.816411</td>\n",
       "      <td>17.492127</td>\n",
       "      <td>16.900726</td>\n",
       "      <td>18.373672</td>\n",
       "      <td>15.803035</td>\n",
       "      <td>...</td>\n",
       "      <td>374.714286</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>729.714286</td>\n",
       "      <td>926.857143</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1377.714286</td>\n",
       "      <td>1674.142857</td>\n",
       "      <td>1993.142857</td>\n",
       "      <td>2357.285714</td>\n",
       "      <td>2743.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-06</th>\n",
       "      <td>18.436874</td>\n",
       "      <td>16.416465</td>\n",
       "      <td>18.816411</td>\n",
       "      <td>17.492127</td>\n",
       "      <td>16.900726</td>\n",
       "      <td>18.373672</td>\n",
       "      <td>15.803035</td>\n",
       "      <td>19.305892</td>\n",
       "      <td>17.591499</td>\n",
       "      <td>14.614944</td>\n",
       "      <td>...</td>\n",
       "      <td>532.000000</td>\n",
       "      <td>729.714286</td>\n",
       "      <td>926.857143</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1377.714286</td>\n",
       "      <td>1674.142857</td>\n",
       "      <td>1993.142857</td>\n",
       "      <td>2357.285714</td>\n",
       "      <td>2743.714286</td>\n",
       "      <td>3148.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-07</th>\n",
       "      <td>17.492127</td>\n",
       "      <td>16.900726</td>\n",
       "      <td>18.373672</td>\n",
       "      <td>15.803035</td>\n",
       "      <td>19.305892</td>\n",
       "      <td>17.591499</td>\n",
       "      <td>14.614944</td>\n",
       "      <td>17.401130</td>\n",
       "      <td>17.753837</td>\n",
       "      <td>14.185514</td>\n",
       "      <td>...</td>\n",
       "      <td>729.714286</td>\n",
       "      <td>926.857143</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1377.714286</td>\n",
       "      <td>1674.142857</td>\n",
       "      <td>1993.142857</td>\n",
       "      <td>2357.285714</td>\n",
       "      <td>2743.714286</td>\n",
       "      <td>3148.571429</td>\n",
       "      <td>3579.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-08</th>\n",
       "      <td>15.803035</td>\n",
       "      <td>19.305892</td>\n",
       "      <td>17.591499</td>\n",
       "      <td>14.614944</td>\n",
       "      <td>17.401130</td>\n",
       "      <td>17.753837</td>\n",
       "      <td>14.185514</td>\n",
       "      <td>17.223567</td>\n",
       "      <td>17.163518</td>\n",
       "      <td>14.142571</td>\n",
       "      <td>...</td>\n",
       "      <td>926.857143</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1377.714286</td>\n",
       "      <td>1674.142857</td>\n",
       "      <td>1993.142857</td>\n",
       "      <td>2357.285714</td>\n",
       "      <td>2743.714286</td>\n",
       "      <td>3148.571429</td>\n",
       "      <td>3579.142857</td>\n",
       "      <td>3986.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-15</th>\n",
       "      <td>13.199598</td>\n",
       "      <td>18.310267</td>\n",
       "      <td>20.119596</td>\n",
       "      <td>14.563217</td>\n",
       "      <td>17.469796</td>\n",
       "      <td>20.679904</td>\n",
       "      <td>14.186428</td>\n",
       "      <td>17.433776</td>\n",
       "      <td>21.832724</td>\n",
       "      <td>14.892161</td>\n",
       "      <td>...</td>\n",
       "      <td>1817.857143</td>\n",
       "      <td>1845.142857</td>\n",
       "      <td>1866.714286</td>\n",
       "      <td>1887.571429</td>\n",
       "      <td>1898.571429</td>\n",
       "      <td>1905.000000</td>\n",
       "      <td>1911.000000</td>\n",
       "      <td>1919.142857</td>\n",
       "      <td>1927.857143</td>\n",
       "      <td>1934.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-16</th>\n",
       "      <td>14.563217</td>\n",
       "      <td>17.469796</td>\n",
       "      <td>20.679904</td>\n",
       "      <td>14.186428</td>\n",
       "      <td>17.433776</td>\n",
       "      <td>21.832724</td>\n",
       "      <td>14.892161</td>\n",
       "      <td>19.366859</td>\n",
       "      <td>22.174061</td>\n",
       "      <td>15.849087</td>\n",
       "      <td>...</td>\n",
       "      <td>1845.142857</td>\n",
       "      <td>1866.714286</td>\n",
       "      <td>1887.571429</td>\n",
       "      <td>1898.571429</td>\n",
       "      <td>1905.000000</td>\n",
       "      <td>1911.000000</td>\n",
       "      <td>1919.142857</td>\n",
       "      <td>1927.857143</td>\n",
       "      <td>1934.000000</td>\n",
       "      <td>1934.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-17</th>\n",
       "      <td>14.186428</td>\n",
       "      <td>17.433776</td>\n",
       "      <td>21.832724</td>\n",
       "      <td>14.892161</td>\n",
       "      <td>19.366859</td>\n",
       "      <td>22.174061</td>\n",
       "      <td>15.849087</td>\n",
       "      <td>19.306826</td>\n",
       "      <td>22.032374</td>\n",
       "      <td>15.759375</td>\n",
       "      <td>...</td>\n",
       "      <td>1866.714286</td>\n",
       "      <td>1887.571429</td>\n",
       "      <td>1898.571429</td>\n",
       "      <td>1905.000000</td>\n",
       "      <td>1911.000000</td>\n",
       "      <td>1919.142857</td>\n",
       "      <td>1927.857143</td>\n",
       "      <td>1934.000000</td>\n",
       "      <td>1934.571429</td>\n",
       "      <td>1934.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-18</th>\n",
       "      <td>14.892161</td>\n",
       "      <td>19.366859</td>\n",
       "      <td>22.174061</td>\n",
       "      <td>15.849087</td>\n",
       "      <td>19.306826</td>\n",
       "      <td>22.032374</td>\n",
       "      <td>15.759375</td>\n",
       "      <td>20.531513</td>\n",
       "      <td>21.581551</td>\n",
       "      <td>15.185219</td>\n",
       "      <td>...</td>\n",
       "      <td>1887.571429</td>\n",
       "      <td>1898.571429</td>\n",
       "      <td>1905.000000</td>\n",
       "      <td>1911.000000</td>\n",
       "      <td>1919.142857</td>\n",
       "      <td>1927.857143</td>\n",
       "      <td>1934.000000</td>\n",
       "      <td>1934.571429</td>\n",
       "      <td>1934.571429</td>\n",
       "      <td>1938.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-19</th>\n",
       "      <td>15.849087</td>\n",
       "      <td>19.306826</td>\n",
       "      <td>22.032374</td>\n",
       "      <td>15.759375</td>\n",
       "      <td>20.531513</td>\n",
       "      <td>21.581551</td>\n",
       "      <td>15.185219</td>\n",
       "      <td>19.582981</td>\n",
       "      <td>21.633073</td>\n",
       "      <td>15.209142</td>\n",
       "      <td>...</td>\n",
       "      <td>1898.571429</td>\n",
       "      <td>1905.000000</td>\n",
       "      <td>1911.000000</td>\n",
       "      <td>1919.142857</td>\n",
       "      <td>1927.857143</td>\n",
       "      <td>1934.000000</td>\n",
       "      <td>1934.571429</td>\n",
       "      <td>1934.571429</td>\n",
       "      <td>1938.142857</td>\n",
       "      <td>1948.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Fièvre(t-29)  Mal de gorge(t-29)  Symptôme(t-29)  \\\n",
       "LOC DATE                                                           \n",
       "BE  2020-03-04     22.444890           18.401937       19.524793   \n",
       "    2020-03-05     20.813055           17.578692       19.672373   \n",
       "    2020-03-06     18.436874           16.416465       18.816411   \n",
       "    2020-03-07     17.492127           16.900726       18.373672   \n",
       "    2020-03-08     15.803035           19.305892       17.591499   \n",
       "...                      ...                 ...             ...   \n",
       "    2021-02-15     13.199598           18.310267       20.119596   \n",
       "    2021-02-16     14.563217           17.469796       20.679904   \n",
       "    2021-02-17     14.186428           17.433776       21.832724   \n",
       "    2021-02-18     14.892161           19.366859       22.174061   \n",
       "    2021-02-19     15.849087           19.306826       22.032374   \n",
       "\n",
       "                Fièvre(t-28)  Mal de gorge(t-28)  Symptôme(t-28)  \\\n",
       "LOC DATE                                                           \n",
       "BE  2020-03-04     20.813055           17.578692       19.672373   \n",
       "    2020-03-05     18.436874           16.416465       18.816411   \n",
       "    2020-03-06     17.492127           16.900726       18.373672   \n",
       "    2020-03-07     15.803035           19.305892       17.591499   \n",
       "    2020-03-08     14.614944           17.401130       17.753837   \n",
       "...                      ...                 ...             ...   \n",
       "    2021-02-15     14.563217           17.469796       20.679904   \n",
       "    2021-02-16     14.186428           17.433776       21.832724   \n",
       "    2021-02-17     14.892161           19.366859       22.174061   \n",
       "    2021-02-18     15.849087           19.306826       22.032374   \n",
       "    2021-02-19     15.759375           20.531513       21.581551   \n",
       "\n",
       "                Fièvre(t-27)  Mal de gorge(t-27)  Symptôme(t-27)  \\\n",
       "LOC DATE                                                           \n",
       "BE  2020-03-04     18.436874           16.416465       18.816411   \n",
       "    2020-03-05     17.492127           16.900726       18.373672   \n",
       "    2020-03-06     15.803035           19.305892       17.591499   \n",
       "    2020-03-07     14.614944           17.401130       17.753837   \n",
       "    2020-03-08     14.185514           17.223567       17.163518   \n",
       "...                      ...                 ...             ...   \n",
       "    2021-02-15     14.186428           17.433776       21.832724   \n",
       "    2021-02-16     14.892161           19.366859       22.174061   \n",
       "    2021-02-17     15.849087           19.306826       22.032374   \n",
       "    2021-02-18     15.759375           20.531513       21.581551   \n",
       "    2021-02-19     15.185219           19.582981       21.633073   \n",
       "\n",
       "                Fièvre(t-26)  ...  TOT_HOSP(t+11)  TOT_HOSP(t+12)  \\\n",
       "LOC DATE                      ...                                   \n",
       "BE  2020-03-04     17.492127  ...      254.142857      374.714286   \n",
       "    2020-03-05     15.803035  ...      374.714286      532.000000   \n",
       "    2020-03-06     14.614944  ...      532.000000      729.714286   \n",
       "    2020-03-07     14.185514  ...      729.714286      926.857143   \n",
       "    2020-03-08     14.142571  ...      926.857143     1143.000000   \n",
       "...                      ...  ...             ...             ...   \n",
       "    2021-02-15     14.892161  ...     1817.857143     1845.142857   \n",
       "    2021-02-16     15.849087  ...     1845.142857     1866.714286   \n",
       "    2021-02-17     15.759375  ...     1866.714286     1887.571429   \n",
       "    2021-02-18     15.185219  ...     1887.571429     1898.571429   \n",
       "    2021-02-19     15.209142  ...     1898.571429     1905.000000   \n",
       "\n",
       "                TOT_HOSP(t+13)  TOT_HOSP(t+14)  TOT_HOSP(t+15)  \\\n",
       "LOC DATE                                                         \n",
       "BE  2020-03-04      532.000000      729.714286      926.857143   \n",
       "    2020-03-05      729.714286      926.857143     1143.000000   \n",
       "    2020-03-06      926.857143     1143.000000     1377.714286   \n",
       "    2020-03-07     1143.000000     1377.714286     1674.142857   \n",
       "    2020-03-08     1377.714286     1674.142857     1993.142857   \n",
       "...                        ...             ...             ...   \n",
       "    2021-02-15     1866.714286     1887.571429     1898.571429   \n",
       "    2021-02-16     1887.571429     1898.571429     1905.000000   \n",
       "    2021-02-17     1898.571429     1905.000000     1911.000000   \n",
       "    2021-02-18     1905.000000     1911.000000     1919.142857   \n",
       "    2021-02-19     1911.000000     1919.142857     1927.857143   \n",
       "\n",
       "                TOT_HOSP(t+16)  TOT_HOSP(t+17)  TOT_HOSP(t+18)  \\\n",
       "LOC DATE                                                         \n",
       "BE  2020-03-04     1143.000000     1377.714286     1674.142857   \n",
       "    2020-03-05     1377.714286     1674.142857     1993.142857   \n",
       "    2020-03-06     1674.142857     1993.142857     2357.285714   \n",
       "    2020-03-07     1993.142857     2357.285714     2743.714286   \n",
       "    2020-03-08     2357.285714     2743.714286     3148.571429   \n",
       "...                        ...             ...             ...   \n",
       "    2021-02-15     1905.000000     1911.000000     1919.142857   \n",
       "    2021-02-16     1911.000000     1919.142857     1927.857143   \n",
       "    2021-02-17     1919.142857     1927.857143     1934.000000   \n",
       "    2021-02-18     1927.857143     1934.000000     1934.571429   \n",
       "    2021-02-19     1934.000000     1934.571429     1934.571429   \n",
       "\n",
       "                TOT_HOSP(t+19)  TOT_HOSP(t+20)  \n",
       "LOC DATE                                        \n",
       "BE  2020-03-04     1993.142857     2357.285714  \n",
       "    2020-03-05     2357.285714     2743.714286  \n",
       "    2020-03-06     2743.714286     3148.571429  \n",
       "    2020-03-07     3148.571429     3579.142857  \n",
       "    2020-03-08     3579.142857     3986.142857  \n",
       "...                        ...             ...  \n",
       "    2021-02-15     1927.857143     1934.000000  \n",
       "    2021-02-16     1934.000000     1934.571429  \n",
       "    2021-02-17     1934.571429     1934.571429  \n",
       "    2021-02-18     1934.571429     1938.142857  \n",
       "    2021-02-19     1938.142857     1948.857143  \n",
       "\n",
       "[353 rows x 110 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c['BE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_generator = MinMaxScaler\n",
    "dg_2 = util.DataGenerator(df_c, n_samples, n_forecast, target=target, scaler_generator=scaler_generator, \n",
    "                          scaler_type='batch', augment_merge=0, predict_one=False, cumsum=False,\n",
    "                          data_columns=[k for k in list_topics], no_lag=True)\n",
    "dg_2.set_loc_init(dg.loc_init)  # consider the other localisations as being augmented\n",
    "n_features = dg_2.n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the second estimator to compute $Y_2(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.3553 - mse: 0.3553 - mae: 0.4494 - root_mean_squared_error: 0.5961\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3276 - mse: 0.3276 - mae: 0.4300 - root_mean_squared_error: 0.5723\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3094 - mse: 0.3094 - mae: 0.4171 - root_mean_squared_error: 0.5562\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2952 - mse: 0.2952 - mae: 0.4068 - root_mean_squared_error: 0.5433\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2833 - mse: 0.2833 - mae: 0.3981 - root_mean_squared_error: 0.5323\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2729 - mse: 0.2729 - mae: 0.3904 - root_mean_squared_error: 0.5224\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2636 - mse: 0.2636 - mae: 0.3835 - root_mean_squared_error: 0.5134\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2552 - mse: 0.2552 - mae: 0.3771 - root_mean_squared_error: 0.5052\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2474 - mse: 0.2474 - mae: 0.3712 - root_mean_squared_error: 0.4974\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2402 - mse: 0.2402 - mae: 0.3656 - root_mean_squared_error: 0.4901\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2335 - mse: 0.2335 - mae: 0.3603 - root_mean_squared_error: 0.4832\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2271 - mse: 0.2271 - mae: 0.3554 - root_mean_squared_error: 0.4766\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2212 - mse: 0.2212 - mae: 0.3506 - root_mean_squared_error: 0.4703\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2155 - mse: 0.2155 - mae: 0.3461 - root_mean_squared_error: 0.4642\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2101 - mse: 0.2101 - mae: 0.3418 - root_mean_squared_error: 0.4584\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2049 - mse: 0.2049 - mae: 0.3376 - root_mean_squared_error: 0.4527\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2000 - mse: 0.2000 - mae: 0.3336 - root_mean_squared_error: 0.4472\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1953 - mse: 0.1953 - mae: 0.3297 - root_mean_squared_error: 0.4420\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1908 - mse: 0.1908 - mae: 0.3260 - root_mean_squared_error: 0.4368\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1865 - mse: 0.1865 - mae: 0.3225 - root_mean_squared_error: 0.4319\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1824 - mse: 0.1824 - mae: 0.3190 - root_mean_squared_error: 0.4270\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1784 - mse: 0.1784 - mae: 0.3157 - root_mean_squared_error: 0.4223\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1745 - mse: 0.1745 - mae: 0.3125 - root_mean_squared_error: 0.4178\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1708 - mse: 0.1708 - mae: 0.3094 - root_mean_squared_error: 0.4133\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1673 - mse: 0.1673 - mae: 0.3064 - root_mean_squared_error: 0.4090\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1639 - mse: 0.1639 - mae: 0.3035 - root_mean_squared_error: 0.4048\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1606 - mse: 0.1606 - mae: 0.3006 - root_mean_squared_error: 0.4007\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1574 - mse: 0.1574 - mae: 0.2979 - root_mean_squared_error: 0.3967\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1543 - mse: 0.1543 - mae: 0.2953 - root_mean_squared_error: 0.3928\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1514 - mse: 0.1514 - mae: 0.2928 - root_mean_squared_error: 0.3891\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1485 - mse: 0.1485 - mae: 0.2903 - root_mean_squared_error: 0.3854\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1458 - mse: 0.1458 - mae: 0.2879 - root_mean_squared_error: 0.3818\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1432 - mse: 0.1432 - mae: 0.2856 - root_mean_squared_error: 0.3784\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1406 - mse: 0.1406 - mae: 0.2834 - root_mean_squared_error: 0.3750\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1382 - mse: 0.1382 - mae: 0.2812 - root_mean_squared_error: 0.3717\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1358 - mse: 0.1358 - mae: 0.2791 - root_mean_squared_error: 0.3685\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1335 - mse: 0.1335 - mae: 0.2771 - root_mean_squared_error: 0.3654\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1314 - mse: 0.1314 - mae: 0.2752 - root_mean_squared_error: 0.3624\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1293 - mse: 0.1293 - mae: 0.2733 - root_mean_squared_error: 0.3595\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1272 - mse: 0.1272 - mae: 0.2715 - root_mean_squared_error: 0.3567\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1253 - mse: 0.1253 - mae: 0.2697 - root_mean_squared_error: 0.3539\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1234 - mse: 0.1234 - mae: 0.2681 - root_mean_squared_error: 0.3513\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1216 - mse: 0.1216 - mae: 0.2664 - root_mean_squared_error: 0.3487\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1198 - mse: 0.1198 - mae: 0.2649 - root_mean_squared_error: 0.3462\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1182 - mse: 0.1182 - mae: 0.2634 - root_mean_squared_error: 0.3438\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1166 - mse: 0.1166 - mae: 0.2619 - root_mean_squared_error: 0.3414\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1150 - mse: 0.1150 - mae: 0.2605 - root_mean_squared_error: 0.3391\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1135 - mse: 0.1135 - mae: 0.2592 - root_mean_squared_error: 0.3369\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1121 - mse: 0.1121 - mae: 0.2579 - root_mean_squared_error: 0.3348\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1107 - mse: 0.1107 - mae: 0.2566 - root_mean_squared_error: 0.3327\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1094 - mse: 0.1094 - mae: 0.2554 - root_mean_squared_error: 0.3307\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1081 - mse: 0.1081 - mae: 0.2542 - root_mean_squared_error: 0.3288\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1069 - mse: 0.1069 - mae: 0.2531 - root_mean_squared_error: 0.3269\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1057 - mse: 0.1057 - mae: 0.2520 - root_mean_squared_error: 0.3251\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1045 - mse: 0.1045 - mae: 0.2510 - root_mean_squared_error: 0.3233\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1034 - mse: 0.1034 - mae: 0.2500 - root_mean_squared_error: 0.3216\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1024 - mse: 0.1024 - mae: 0.2491 - root_mean_squared_error: 0.3200\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1014 - mse: 0.1014 - mae: 0.2480 - root_mean_squared_error: 0.3184\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1004 - mse: 0.1004 - mae: 0.2472 - root_mean_squared_error: 0.3168\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0994 - mse: 0.0994 - mae: 0.2463 - root_mean_squared_error: 0.3154\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0985 - mse: 0.0985 - mae: 0.2455 - root_mean_squared_error: 0.3139\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0977 - mse: 0.0977 - mae: 0.2446 - root_mean_squared_error: 0.3125\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0968 - mse: 0.0968 - mae: 0.2439 - root_mean_squared_error: 0.3112\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0960 - mse: 0.0960 - mae: 0.2431 - root_mean_squared_error: 0.3098\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0952 - mse: 0.0952 - mae: 0.2424 - root_mean_squared_error: 0.3086\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0945 - mse: 0.0945 - mae: 0.2417 - root_mean_squared_error: 0.3073\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0937 - mse: 0.0937 - mae: 0.2410 - root_mean_squared_error: 0.3062\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0930 - mse: 0.0930 - mae: 0.2404 - root_mean_squared_error: 0.3050\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0923 - mse: 0.0923 - mae: 0.2398 - root_mean_squared_error: 0.3039\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0917 - mse: 0.0917 - mae: 0.2391 - root_mean_squared_error: 0.3028\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0911 - mse: 0.0911 - mae: 0.2386 - root_mean_squared_error: 0.3018\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0905 - mse: 0.0905 - mae: 0.2381 - root_mean_squared_error: 0.3008\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0899 - mse: 0.0899 - mae: 0.2375 - root_mean_squared_error: 0.2998\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0893 - mse: 0.0893 - mae: 0.2370 - root_mean_squared_error: 0.2988\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0888 - mse: 0.0888 - mae: 0.2365 - root_mean_squared_error: 0.2979\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0882 - mse: 0.0882 - mae: 0.2360 - root_mean_squared_error: 0.2970\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0877 - mse: 0.0877 - mae: 0.2356 - root_mean_squared_error: 0.2962\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0872 - mse: 0.0872 - mae: 0.2352 - root_mean_squared_error: 0.2953\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0868 - mse: 0.0868 - mae: 0.2347 - root_mean_squared_error: 0.2946\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0863 - mse: 0.0863 - mae: 0.2343 - root_mean_squared_error: 0.2938\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0859 - mse: 0.0859 - mae: 0.2339 - root_mean_squared_error: 0.2930\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0854 - mse: 0.0854 - mae: 0.2335 - root_mean_squared_error: 0.2923\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0850 - mse: 0.0850 - mae: 0.2331 - root_mean_squared_error: 0.2916\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0846 - mse: 0.0846 - mae: 0.2327 - root_mean_squared_error: 0.2909\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0843 - mse: 0.0843 - mae: 0.2324 - root_mean_squared_error: 0.2903\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0839 - mse: 0.0839 - mae: 0.2320 - root_mean_squared_error: 0.2896\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0835 - mse: 0.0835 - mae: 0.2316 - root_mean_squared_error: 0.2890\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0832 - mse: 0.0832 - mae: 0.2313 - root_mean_squared_error: 0.2884\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0829 - mse: 0.0829 - mae: 0.2309 - root_mean_squared_error: 0.2879\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0825 - mse: 0.0825 - mae: 0.2307 - root_mean_squared_error: 0.2873\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0822 - mse: 0.0822 - mae: 0.2303 - root_mean_squared_error: 0.2868\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0819 - mse: 0.0819 - mae: 0.2300 - root_mean_squared_error: 0.2862\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0816 - mse: 0.0816 - mae: 0.2297 - root_mean_squared_error: 0.2857\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0814 - mse: 0.0814 - mae: 0.2294 - root_mean_squared_error: 0.2852\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0811 - mse: 0.0811 - mae: 0.2290 - root_mean_squared_error: 0.2848\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0808 - mse: 0.0808 - mae: 0.2288 - root_mean_squared_error: 0.2843\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0806 - mse: 0.0806 - mae: 0.2285 - root_mean_squared_error: 0.2838\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0803 - mse: 0.0803 - mae: 0.2282 - root_mean_squared_error: 0.2834\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0801 - mse: 0.0801 - mae: 0.2279 - root_mean_squared_error: 0.2830\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0798 - mse: 0.0798 - mae: 0.2276 - root_mean_squared_error: 0.2825\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0796 - mse: 0.0796 - mae: 0.2273 - root_mean_squared_error: 0.2821\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0794 - mse: 0.0794 - mae: 0.2271 - root_mean_squared_error: 0.2817\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0792 - mse: 0.0792 - mae: 0.2268 - root_mean_squared_error: 0.2813\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0789 - mse: 0.0789 - mae: 0.2266 - root_mean_squared_error: 0.2810\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0787 - mse: 0.0787 - mae: 0.2262 - root_mean_squared_error: 0.2806\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0785 - mse: 0.0785 - mae: 0.2261 - root_mean_squared_error: 0.2802\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0783 - mse: 0.0783 - mae: 0.2257 - root_mean_squared_error: 0.2799\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0781 - mse: 0.0781 - mae: 0.2256 - root_mean_squared_error: 0.2795\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0779 - mse: 0.0779 - mae: 0.2252 - root_mean_squared_error: 0.2792\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0777 - mse: 0.0777 - mae: 0.2251 - root_mean_squared_error: 0.2788\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0776 - mse: 0.0776 - mae: 0.2248 - root_mean_squared_error: 0.2785\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0774 - mse: 0.0774 - mae: 0.2246 - root_mean_squared_error: 0.2782\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0772 - mse: 0.0772 - mae: 0.2243 - root_mean_squared_error: 0.2778\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0770 - mse: 0.0770 - mae: 0.2242 - root_mean_squared_error: 0.2775\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0769 - mse: 0.0769 - mae: 0.2239 - root_mean_squared_error: 0.2772\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0767 - mse: 0.0767 - mae: 0.2237 - root_mean_squared_error: 0.2769\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0765 - mse: 0.0765 - mae: 0.2235 - root_mean_squared_error: 0.2766\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0764 - mse: 0.0764 - mae: 0.2232 - root_mean_squared_error: 0.2764\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0762 - mse: 0.0762 - mae: 0.2232 - root_mean_squared_error: 0.2761\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0761 - mse: 0.0761 - mae: 0.2229 - root_mean_squared_error: 0.2758\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0759 - mse: 0.0759 - mae: 0.2227 - root_mean_squared_error: 0.2755\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0758 - mse: 0.0758 - mae: 0.2225 - root_mean_squared_error: 0.2753\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0756 - mse: 0.0756 - mae: 0.2223 - root_mean_squared_error: 0.2750\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0755 - mse: 0.0755 - mae: 0.2222 - root_mean_squared_error: 0.2747\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0753 - mse: 0.0753 - mae: 0.2220 - root_mean_squared_error: 0.2745\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0752 - mse: 0.0752 - mae: 0.2218 - root_mean_squared_error: 0.2742\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0751 - mse: 0.0751 - mae: 0.2216 - root_mean_squared_error: 0.2740\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0749 - mse: 0.0749 - mae: 0.2215 - root_mean_squared_error: 0.2737\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0748 - mse: 0.0748 - mae: 0.2213 - root_mean_squared_error: 0.2735\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0747 - mse: 0.0747 - mae: 0.2212 - root_mean_squared_error: 0.2733\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0746 - mse: 0.0746 - mae: 0.2209 - root_mean_squared_error: 0.2731\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0744 - mse: 0.0744 - mae: 0.2210 - root_mean_squared_error: 0.2728\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0743 - mse: 0.0743 - mae: 0.2206 - root_mean_squared_error: 0.2726\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0742 - mse: 0.0742 - mae: 0.2206 - root_mean_squared_error: 0.2724\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0741 - mse: 0.0741 - mae: 0.2204 - root_mean_squared_error: 0.2722\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0740 - mse: 0.0740 - mae: 0.2203 - root_mean_squared_error: 0.2720\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0739 - mse: 0.0739 - mae: 0.2202 - root_mean_squared_error: 0.2718\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0738 - mse: 0.0738 - mae: 0.2200 - root_mean_squared_error: 0.2716\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0736 - mse: 0.0736 - mae: 0.2199 - root_mean_squared_error: 0.2714\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0735 - mse: 0.0735 - mae: 0.2198 - root_mean_squared_error: 0.2712\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0734 - mse: 0.0734 - mae: 0.2196 - root_mean_squared_error: 0.2710\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0733 - mse: 0.0733 - mae: 0.2195 - root_mean_squared_error: 0.2708\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0732 - mse: 0.0732 - mae: 0.2193 - root_mean_squared_error: 0.2706\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0731 - mse: 0.0731 - mae: 0.2193 - root_mean_squared_error: 0.2704\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0730 - mse: 0.0730 - mae: 0.2191 - root_mean_squared_error: 0.2703\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0729 - mse: 0.0729 - mae: 0.2191 - root_mean_squared_error: 0.2701\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0729 - mse: 0.0729 - mae: 0.2189 - root_mean_squared_error: 0.2699\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0728 - mse: 0.0728 - mae: 0.2189 - root_mean_squared_error: 0.2698\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0727 - mse: 0.0727 - mae: 0.2186 - root_mean_squared_error: 0.2696\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0726 - mse: 0.0726 - mae: 0.2187 - root_mean_squared_error: 0.2694\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0725 - mse: 0.0725 - mae: 0.2185 - root_mean_squared_error: 0.2693\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0724 - mse: 0.0724 - mae: 0.2185 - root_mean_squared_error: 0.2691\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0723 - mse: 0.0723 - mae: 0.2183 - root_mean_squared_error: 0.2689\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0722 - mse: 0.0722 - mae: 0.2183 - root_mean_squared_error: 0.2688\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0722 - mse: 0.0722 - mae: 0.2181 - root_mean_squared_error: 0.2686\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0721 - mse: 0.0721 - mae: 0.2181 - root_mean_squared_error: 0.2685\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0720 - mse: 0.0720 - mae: 0.2179 - root_mean_squared_error: 0.2683\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0719 - mse: 0.0719 - mae: 0.2179 - root_mean_squared_error: 0.2682\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0718 - mse: 0.0718 - mae: 0.2177 - root_mean_squared_error: 0.2680\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0718 - mse: 0.0718 - mae: 0.2178 - root_mean_squared_error: 0.2679\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0717 - mse: 0.0717 - mae: 0.2175 - root_mean_squared_error: 0.2678\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0716 - mse: 0.0716 - mae: 0.2177 - root_mean_squared_error: 0.2676\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0716 - mse: 0.0716 - mae: 0.2172 - root_mean_squared_error: 0.2675\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0715 - mse: 0.0715 - mae: 0.2176 - root_mean_squared_error: 0.2674\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0714 - mse: 0.0714 - mae: 0.2171 - root_mean_squared_error: 0.2672\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0713 - mse: 0.0713 - mae: 0.2174 - root_mean_squared_error: 0.2671\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0713 - mse: 0.0713 - mae: 0.2170 - root_mean_squared_error: 0.2670\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0712 - mse: 0.0712 - mae: 0.2172 - root_mean_squared_error: 0.2668\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0711 - mse: 0.0711 - mae: 0.2169 - root_mean_squared_error: 0.2667\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0711 - mse: 0.0711 - mae: 0.2170 - root_mean_squared_error: 0.2666\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0710 - mse: 0.0710 - mae: 0.2167 - root_mean_squared_error: 0.2665\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0709 - mse: 0.0709 - mae: 0.2169 - root_mean_squared_error: 0.2664\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0709 - mse: 0.0709 - mae: 0.2166 - root_mean_squared_error: 0.2662\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0708 - mse: 0.0708 - mae: 0.2168 - root_mean_squared_error: 0.2661\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0708 - mse: 0.0708 - mae: 0.2165 - root_mean_squared_error: 0.2660\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0707 - mse: 0.0707 - mae: 0.2167 - root_mean_squared_error: 0.2659\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0706 - mse: 0.0706 - mae: 0.2164 - root_mean_squared_error: 0.2658\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0706 - mse: 0.0706 - mae: 0.2166 - root_mean_squared_error: 0.2657\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0705 - mse: 0.0705 - mae: 0.2162 - root_mean_squared_error: 0.2656\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0705 - mse: 0.0705 - mae: 0.2165 - root_mean_squared_error: 0.2655\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0704 - mse: 0.0704 - mae: 0.2161 - root_mean_squared_error: 0.2654\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0704 - mse: 0.0704 - mae: 0.2164 - root_mean_squared_error: 0.2652\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0703 - mse: 0.0703 - mae: 0.2160 - root_mean_squared_error: 0.2651\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0702 - mse: 0.0702 - mae: 0.2162 - root_mean_squared_error: 0.2650\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0702 - mse: 0.0702 - mae: 0.2159 - root_mean_squared_error: 0.2649\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0701 - mse: 0.0701 - mae: 0.2162 - root_mean_squared_error: 0.2648\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0701 - mse: 0.0701 - mae: 0.2158 - root_mean_squared_error: 0.2647\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0700 - mse: 0.0700 - mae: 0.2161 - root_mean_squared_error: 0.2646\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0700 - mse: 0.0700 - mae: 0.2157 - root_mean_squared_error: 0.2645\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0699 - mse: 0.0699 - mae: 0.2160 - root_mean_squared_error: 0.2645\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0699 - mse: 0.0699 - mae: 0.2156 - root_mean_squared_error: 0.2644\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0698 - mse: 0.0698 - mae: 0.2159 - root_mean_squared_error: 0.2643\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0698 - mse: 0.0698 - mae: 0.2155 - root_mean_squared_error: 0.2642\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0697 - mse: 0.0697 - mae: 0.2158 - root_mean_squared_error: 0.2641\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0697 - mse: 0.0697 - mae: 0.2155 - root_mean_squared_error: 0.2640\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0697 - mse: 0.0697 - mae: 0.2157 - root_mean_squared_error: 0.2639\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0696 - mse: 0.0696 - mae: 0.2154 - root_mean_squared_error: 0.2638\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0696 - mse: 0.0696 - mae: 0.2156 - root_mean_squared_error: 0.2637\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0695 - mse: 0.0695 - mae: 0.2153 - root_mean_squared_error: 0.2637\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0695 - mse: 0.0695 - mae: 0.2155 - root_mean_squared_error: 0.2636\n"
     ]
    }
   ],
   "source": [
    "X_train_2 = dg_2.get_x(train_idx, scaled=True)\n",
    "Y_train_2 = dg_2.get_y(train_idx, scaled=True)\n",
    "\n",
    "if len(valid_idx) > 0:\n",
    "    X_val_2 = dg_2.get_x(valid_idx, scaled=True, use_previous_scaler=True, geo=dg_2.loc_init)\n",
    "    Y_val_unscaled_2 = dg.get_y(valid_idx, geo=dg.loc_init, scaled=False)\n",
    "    Y_val_real_2 = dg.remove_padded_y(Y_val_unscaled, idx=val_idx, geo=dg.loc_init)\n",
    "    batch_size_val = len(X_val_2)\n",
    "    \n",
    "X_test_2 = dg_2.get_x(test_idx, scaled=True, use_previous_scaler=True, geo=dg_2.loc_init)\n",
    "Y_test_unscaled_2 = dg.get_y(test_idx, scaled=False, geo=dg_2.loc_init)\n",
    "Y_test_real_2 = dg.remove_padded_y(Y_test_unscaled_2, idx=test_idx, geo=dg_2.loc_init)\n",
    "\n",
    "model_generator = get_dense_model\n",
    "\n",
    "batch_size_train = len(X_train_2)\n",
    "batch_size_test = len(X_test_2)\n",
    "model = model_generator(batch_input_shape=(batch_size_train, n_samples, dg_2.n_features))\n",
    "    \n",
    "history = model.fit(X_train_2, Y_train_2, batch_size=batch_size_train, epochs=epochs, callbacks=None)\n",
    "# compute the predictions on both the training and the validation\n",
    "Y_train_pred_2 = model.predict(X_train_2, batch_size=batch_size_train)\n",
    "\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_test, n_samples, dg_2.n_features))\n",
    "model_prediction.set_weights(model.get_weights())\n",
    "Y_test_pred_2 = model_prediction.predict(X_test_2, batch_size=batch_size_test)\n",
    "# unscale and unpad the data\n",
    "Y_test_pred_unscaled_2 = dg_2.inverse_transform_y(Y_test_pred_2, idx=test_idx, geo=dg_2.loc_init)\n",
    "Y_test_pred_real_2 = dg.remove_padded_y(Y_test_pred_unscaled_2, idx=test_idx, geo=dg_2.loc_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the final prediction by combining the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssembleLayerTimeDist(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    assemble the prediction from the trends and the predictions from another model\n",
    "    the same weight is used at every timestep (1 trainable parameter)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape):\n",
    "        super(AssembleLayer, self).__init__(name='')\n",
    "        self.dense_trends = tf.keras.layers.Dense(1, use_bias=False, \n",
    "                                                 kernel_constraint=tf.keras.constraints.MinMaxNorm(0.001, 0.2))\n",
    "        batch_input_shape = (input_shape[0], input_shape[1], 1)\n",
    "        self.time_dist = TimeDistributed(self.dense_trends, batch_input_shape=batch_input_shape)\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x_trends = input_tensor[:, :, 1:]\n",
    "        x_hosp = input_tensor[:, :, :1]\n",
    "        x_trends = tf.keras.layers.Subtract()([x_trends, x_hosp])  # x_trends - x_hosp\n",
    "        x_trends = self.time_dist(x_trends, training=training)  # apply simple weight\n",
    "        return tf.keras.layers.Add()([x_trends, x_hosp])  # final prediction = x_hosp + (x_trends - x_hosp) * c\n",
    "\n",
    "class AssembleLayer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    assemble the prediction from the trends and the predictions from another model\n",
    "    different weight are used at every timestep (n_forecast trainable parameter)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_input_shape):\n",
    "        super(AssembleLayer, self).__init__(name='')\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[1,batch_input_shape[1]], \n",
    "                                      constraint=tf.keras.constraints.MinMaxNorm(0.001, 0.2))\n",
    "\n",
    "    \"\"\"\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                              shape=[1,input_shape[1]], constraint=tf.keras.constraints.MinMaxNorm(0.001, 0.2))\n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x_trends = input_tensor[:, :, 1]\n",
    "        x_hosp = input_tensor[:, :, 0]\n",
    "        x_trends = tf.keras.layers.Subtract()([x_trends, x_hosp])  # x_trends - x_hosp\n",
    "        x_trends = tf.multiply(x_trends, self.kernel)  # apply simple weight: (x_trends - x_hosp) * c\n",
    "        # x_trends = tf.reshape(x_trends, x_hosp.shape)\n",
    "        return tf.keras.layers.Add()([x_hosp, x_trends])  # final prediction = x_hosp + (x_trends - x_hosp) * c\n",
    "\n",
    "    \n",
    "def get_assemble(batch_input_shape):\n",
    "    model = AssembleLayer(batch_input_shape)\n",
    "    model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0228 - mse: 0.0228 - mae: 0.1083 - root_mean_squared_error: 0.1511\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.0979 - root_mean_squared_error: 0.1411\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.0978 - root_mean_squared_error: 0.1410\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0199 - mse: 0.0199 - mae: 0.0977 - root_mean_squared_error: 0.1409\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.0976 - root_mean_squared_error: 0.1408\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.0975 - root_mean_squared_error: 0.1408\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.0975 - root_mean_squared_error: 0.1407\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.0974 - root_mean_squared_error: 0.1406\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0198 - mse: 0.0198 - mae: 0.0973 - root_mean_squared_error: 0.1406\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0973 - root_mean_squared_error: 0.1405\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0972 - root_mean_squared_error: 0.1405\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0972 - root_mean_squared_error: 0.1404\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0971 - root_mean_squared_error: 0.1404\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0971 - root_mean_squared_error: 0.1403\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0970 - root_mean_squared_error: 0.1403\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0197 - mse: 0.0197 - mae: 0.0970 - root_mean_squared_error: 0.1402\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0969 - root_mean_squared_error: 0.1402\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0968 - root_mean_squared_error: 0.1401\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0968 - root_mean_squared_error: 0.1401\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0967 - root_mean_squared_error: 0.1400\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0967 - root_mean_squared_error: 0.1400\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0966 - root_mean_squared_error: 0.1399\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0966 - root_mean_squared_error: 0.1399\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0196 - mse: 0.0196 - mae: 0.0965 - root_mean_squared_error: 0.1398\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0965 - root_mean_squared_error: 0.1398\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0965 - root_mean_squared_error: 0.1398\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0964 - root_mean_squared_error: 0.1397\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0964 - root_mean_squared_error: 0.1397\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0963 - root_mean_squared_error: 0.1396\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0963 - root_mean_squared_error: 0.1396\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0962 - root_mean_squared_error: 0.1396\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0962 - root_mean_squared_error: 0.1395\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195 - mae: 0.0961 - root_mean_squared_error: 0.1395\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0194 - mse: 0.0194 - mae: 0.0961 - root_mean_squared_error: 0.1394\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0194 - mse: 0.0194 - mae: 0.0961 - root_mean_squared_error: 0.1394\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0194 - mse: 0.0194 - mae: 0.0960 - root_mean_squared_error: 0.1394\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0194 - mse: 0.0194 - mae: 0.0960 - root_mean_squared_error: 0.1393\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0194 - mse: 0.0194 - mae: 0.0959 - root_mean_squared_error: 0.1393\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0194 - mse: 0.0194 - mae: 0.0959 - root_mean_squared_error: 0.1393\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0194 - mse: 0.0194 - mae: 0.0958 - root_mean_squared_error: 0.1392\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0194 - mse: 0.0194 - mae: 0.0958 - root_mean_squared_error: 0.1392\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0194 - mse: 0.0194 - mae: 0.0958 - root_mean_squared_error: 0.1391\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0194 - mse: 0.0194 - mae: 0.0957 - root_mean_squared_error: 0.1391\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0193 - mse: 0.0193 - mae: 0.0957 - root_mean_squared_error: 0.1391\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193 - mse: 0.0193 - mae: 0.0956 - root_mean_squared_error: 0.1390\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193 - mse: 0.0193 - mae: 0.0956 - root_mean_squared_error: 0.1390\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0193 - mse: 0.0193 - mae: 0.0956 - root_mean_squared_error: 0.1390\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0193 - mse: 0.0193 - mae: 0.0955 - root_mean_squared_error: 0.1389\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0193 - mse: 0.0193 - mae: 0.0955 - root_mean_squared_error: 0.1389\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193 - mse: 0.0193 - mae: 0.0954 - root_mean_squared_error: 0.1389\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193 - mse: 0.0193 - mae: 0.0954 - root_mean_squared_error: 0.1388\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0193 - mse: 0.0193 - mae: 0.0954 - root_mean_squared_error: 0.1388\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0193 - mse: 0.0193 - mae: 0.0953 - root_mean_squared_error: 0.1388\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0953 - root_mean_squared_error: 0.1387\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0953 - root_mean_squared_error: 0.1387\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0952 - root_mean_squared_error: 0.1387\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0952 - root_mean_squared_error: 0.1386\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0952 - root_mean_squared_error: 0.1386\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0951 - root_mean_squared_error: 0.1386\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0951 - root_mean_squared_error: 0.1385\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0951 - root_mean_squared_error: 0.1385\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0950 - root_mean_squared_error: 0.1385\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0950 - root_mean_squared_error: 0.1385\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0950 - root_mean_squared_error: 0.1384\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0192 - mse: 0.0192 - mae: 0.0949 - root_mean_squared_error: 0.1384\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0949 - root_mean_squared_error: 0.1384\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0949 - root_mean_squared_error: 0.1383\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0948 - root_mean_squared_error: 0.1383\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0948 - root_mean_squared_error: 0.1383\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0948 - root_mean_squared_error: 0.1383\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0947 - root_mean_squared_error: 0.1382\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0947 - root_mean_squared_error: 0.1382\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0947 - root_mean_squared_error: 0.1382\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0946 - root_mean_squared_error: 0.1381\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0946 - root_mean_squared_error: 0.1381\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0946 - root_mean_squared_error: 0.1381\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0946 - root_mean_squared_error: 0.1381\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0191 - mse: 0.0191 - mae: 0.0945 - root_mean_squared_error: 0.1380\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0945 - root_mean_squared_error: 0.1380\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0945 - root_mean_squared_error: 0.1380\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0944 - root_mean_squared_error: 0.1380\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0944 - root_mean_squared_error: 0.1379\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0944 - root_mean_squared_error: 0.1379\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0944 - root_mean_squared_error: 0.1379\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0943 - root_mean_squared_error: 0.1379\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0943 - root_mean_squared_error: 0.1378\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0943 - root_mean_squared_error: 0.1378\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0943 - root_mean_squared_error: 0.1378\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0942 - root_mean_squared_error: 0.1378\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0942 - root_mean_squared_error: 0.1378\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0942 - root_mean_squared_error: 0.1377\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0942 - root_mean_squared_error: 0.1377\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0941 - root_mean_squared_error: 0.1377\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0190 - mse: 0.0190 - mae: 0.0941 - root_mean_squared_error: 0.1377\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0941 - root_mean_squared_error: 0.1376\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0941 - root_mean_squared_error: 0.1376\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0940 - root_mean_squared_error: 0.1376\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0940 - root_mean_squared_error: 0.1376\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0940 - root_mean_squared_error: 0.1376\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0940 - root_mean_squared_error: 0.1375\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0939 - root_mean_squared_error: 0.1375\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0939 - root_mean_squared_error: 0.1375\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0939 - root_mean_squared_error: 0.1375\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0939 - root_mean_squared_error: 0.1375\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0939 - root_mean_squared_error: 0.1374\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0938 - root_mean_squared_error: 0.1374\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0938 - root_mean_squared_error: 0.1374\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0938 - root_mean_squared_error: 0.1374\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0938 - root_mean_squared_error: 0.1374\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0938 - root_mean_squared_error: 0.1373\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0937 - root_mean_squared_error: 0.1373\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0189 - mse: 0.0189 - mae: 0.0937 - root_mean_squared_error: 0.1373\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0937 - root_mean_squared_error: 0.1373\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0937 - root_mean_squared_error: 0.1373\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0937 - root_mean_squared_error: 0.1373\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0937 - root_mean_squared_error: 0.1372\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0936 - root_mean_squared_error: 0.1372\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0936 - root_mean_squared_error: 0.1372\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0936 - root_mean_squared_error: 0.1372\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0936 - root_mean_squared_error: 0.1372\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0936 - root_mean_squared_error: 0.1372\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0936 - root_mean_squared_error: 0.1372\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0935 - root_mean_squared_error: 0.1371\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0935 - root_mean_squared_error: 0.1371\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0935 - root_mean_squared_error: 0.1371\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0935 - root_mean_squared_error: 0.1371\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0935 - root_mean_squared_error: 0.1371\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0935 - root_mean_squared_error: 0.1371\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0935 - root_mean_squared_error: 0.1371\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0934 - root_mean_squared_error: 0.1371\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0934 - root_mean_squared_error: 0.1370\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0934 - root_mean_squared_error: 0.1370\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0934 - root_mean_squared_error: 0.1370\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0934 - root_mean_squared_error: 0.1370\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0934 - root_mean_squared_error: 0.1370\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0934 - root_mean_squared_error: 0.1370\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0933 - root_mean_squared_error: 0.1370\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0933 - root_mean_squared_error: 0.1369\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0188 - mse: 0.0188 - mae: 0.0933 - root_mean_squared_error: 0.1369\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0933 - root_mean_squared_error: 0.1369\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0933 - root_mean_squared_error: 0.1369\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0933 - root_mean_squared_error: 0.1369\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0933 - root_mean_squared_error: 0.1369\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0933 - root_mean_squared_error: 0.1369\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0932 - root_mean_squared_error: 0.1369\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0932 - root_mean_squared_error: 0.1369\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0932 - root_mean_squared_error: 0.1368\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0932 - root_mean_squared_error: 0.1368\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0932 - root_mean_squared_error: 0.1368\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0932 - root_mean_squared_error: 0.1368\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0932 - root_mean_squared_error: 0.1368\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0932 - root_mean_squared_error: 0.1368\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0932 - root_mean_squared_error: 0.1368\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0931 - root_mean_squared_error: 0.1368\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0931 - root_mean_squared_error: 0.1368\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0931 - root_mean_squared_error: 0.1368\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0931 - root_mean_squared_error: 0.1367\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0931 - root_mean_squared_error: 0.1367\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0931 - root_mean_squared_error: 0.1367\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0931 - root_mean_squared_error: 0.1367\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0931 - root_mean_squared_error: 0.1367\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0931 - root_mean_squared_error: 0.1367\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0931 - root_mean_squared_error: 0.1367\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0931 - root_mean_squared_error: 0.1367\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1367\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1367\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1367\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1367\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1366\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1366\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1366\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1366\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1366\n",
      "Epoch 174/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1366\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1366\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1366\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1366\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1366\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1366\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0930 - root_mean_squared_error: 0.1366\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0929 - root_mean_squared_error: 0.1366\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0187 - mse: 0.0187 - mae: 0.0929 - root_mean_squared_error: 0.1366\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1366\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1366\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1366\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0929 - root_mean_squared_error: 0.1365\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fca3c781a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fca3c6a8280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "X_train_assembled = np.stack([Y_train_pred_1, Y_train_pred_2], axis=2)\n",
    "Y_train_assembled = Y_train  # output the target of the first model\n",
    "X_test_assembled = np.stack([Y_test_pred_1, Y_test_pred_2], axis=2)\n",
    "Y_test_assembled_real = Y_test_real\n",
    "\n",
    "batch_size_train = len(X_train_assembled)\n",
    "batch_size_test = len(X_test_assembled)\n",
    "\n",
    "model_generator = get_assemble\n",
    "\n",
    "model_train = model_generator(batch_input_shape=(batch_size_train, n_forecast, 2))\n",
    "model_train.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                      metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "history = model_train.fit(X_train_assembled, Y_train_assembled, batch_size=batch_size_train, epochs=epochs)\n",
    "Y_train_assembled_pred = model_train.predict(X_train_assembled, batch_size=batch_size_train)\n",
    "\n",
    "model_prediction = model_generator(batch_input_shape=(batch_size_test, n_forecast, 2))\n",
    "model_prediction.set_weights(model_train.get_weights())\n",
    "Y_test_assembled_pred = model_prediction.predict(X_test_assembled, batch_size=batch_size_test)\n",
    "# unscale and unpad the data\n",
    "Y_test_assembled_pred_unscaled = dg_2.inverse_transform_y(Y_test_assembled_pred, idx=test_idx, geo=dg.loc_init)\n",
    "Y_test_assembled_pred_real = dg.remove_padded_y(Y_test_assembled_pred_unscaled, idx=test_idx, geo=dg.loc_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factor t+1 = -0.0408\n",
      "factor t+2 = -0.0125\n",
      "factor t+3 = 0.0185\n",
      "factor t+4 = -0.0063\n",
      "factor t+5 = -0.0030\n",
      "factor t+6 = -0.0240\n",
      "factor t+7 = 0.0158\n",
      "factor t+8 = 0.0023\n",
      "factor t+9 = -0.0264\n",
      "factor t+10 = -0.0402\n",
      "factor t+11 = 0.0159\n",
      "factor t+12 = -0.0229\n",
      "factor t+13 = 0.0010\n",
      "factor t+14 = 0.0068\n",
      "factor t+15 = 0.1051\n",
      "factor t+16 = 0.1561\n",
      "factor t+17 = 0.1105\n",
      "factor t+18 = 0.0947\n",
      "factor t+19 = 0.0675\n",
      "factor t+20 = 0.1453\n"
     ]
    }
   ],
   "source": [
    "factors = model_prediction.weights[0].numpy().reshape(n_forecast)\n",
    "for t in range(n_forecast):\n",
    "    print(f'factor t+{t+1} = {factors[t]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE(t+1)</th>\n",
       "      <th>MSE(t+1)</th>\n",
       "      <th>MAE(t+2)</th>\n",
       "      <th>MSE(t+2)</th>\n",
       "      <th>MAE(t+3)</th>\n",
       "      <th>MSE(t+3)</th>\n",
       "      <th>MAE(t+4)</th>\n",
       "      <th>MSE(t+4)</th>\n",
       "      <th>MAE(t+5)</th>\n",
       "      <th>MSE(t+5)</th>\n",
       "      <th>...</th>\n",
       "      <th>MAE(t+16)</th>\n",
       "      <th>MSE(t+16)</th>\n",
       "      <th>MAE(t+17)</th>\n",
       "      <th>MSE(t+17)</th>\n",
       "      <th>MAE(t+18)</th>\n",
       "      <th>MSE(t+18)</th>\n",
       "      <th>MAE(t+19)</th>\n",
       "      <th>MSE(t+19)</th>\n",
       "      <th>MAE(t+20)</th>\n",
       "      <th>MSE(t+20)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model 1</th>\n",
       "      <td>74.246497</td>\n",
       "      <td>10661.181702</td>\n",
       "      <td>64.859612</td>\n",
       "      <td>8287.868036</td>\n",
       "      <td>54.875115</td>\n",
       "      <td>5.992706e+03</td>\n",
       "      <td>48.304868</td>\n",
       "      <td>5066.573934</td>\n",
       "      <td>69.233385</td>\n",
       "      <td>9537.989886</td>\n",
       "      <td>...</td>\n",
       "      <td>291.965565</td>\n",
       "      <td>141194.272971</td>\n",
       "      <td>147.297524</td>\n",
       "      <td>40577.414599</td>\n",
       "      <td>211.333142</td>\n",
       "      <td>72714.931442</td>\n",
       "      <td>237.669351</td>\n",
       "      <td>92290.728277</td>\n",
       "      <td>279.445759</td>\n",
       "      <td>128032.712031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model trends</th>\n",
       "      <td>615.283346</td>\n",
       "      <td>619318.162119</td>\n",
       "      <td>597.649313</td>\n",
       "      <td>606784.083552</td>\n",
       "      <td>708.906178</td>\n",
       "      <td>1.034760e+06</td>\n",
       "      <td>600.412266</td>\n",
       "      <td>601334.015551</td>\n",
       "      <td>641.838813</td>\n",
       "      <td>755432.050846</td>\n",
       "      <td>...</td>\n",
       "      <td>494.508663</td>\n",
       "      <td>435262.153752</td>\n",
       "      <td>494.572441</td>\n",
       "      <td>438693.819976</td>\n",
       "      <td>475.924753</td>\n",
       "      <td>404937.398773</td>\n",
       "      <td>489.817600</td>\n",
       "      <td>433661.971431</td>\n",
       "      <td>480.883558</td>\n",
       "      <td>415089.513832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assembled</th>\n",
       "      <td>54.494289</td>\n",
       "      <td>6331.683331</td>\n",
       "      <td>60.420886</td>\n",
       "      <td>7426.896690</td>\n",
       "      <td>62.566905</td>\n",
       "      <td>7.438153e+03</td>\n",
       "      <td>46.831250</td>\n",
       "      <td>4913.244302</td>\n",
       "      <td>68.233721</td>\n",
       "      <td>9358.484411</td>\n",
       "      <td>...</td>\n",
       "      <td>320.721673</td>\n",
       "      <td>170132.963148</td>\n",
       "      <td>171.359895</td>\n",
       "      <td>50217.614753</td>\n",
       "      <td>231.425384</td>\n",
       "      <td>86758.686110</td>\n",
       "      <td>251.248726</td>\n",
       "      <td>103355.435355</td>\n",
       "      <td>305.335133</td>\n",
       "      <td>153550.177272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                MAE(t+1)       MSE(t+1)    MAE(t+2)       MSE(t+2)  \\\n",
       "model                                                                \n",
       "model 1        74.246497   10661.181702   64.859612    8287.868036   \n",
       "model trends  615.283346  619318.162119  597.649313  606784.083552   \n",
       "assembled      54.494289    6331.683331   60.420886    7426.896690   \n",
       "\n",
       "                MAE(t+3)      MSE(t+3)    MAE(t+4)       MSE(t+4)    MAE(t+5)  \\\n",
       "model                                                                           \n",
       "model 1        54.875115  5.992706e+03   48.304868    5066.573934   69.233385   \n",
       "model trends  708.906178  1.034760e+06  600.412266  601334.015551  641.838813   \n",
       "assembled      62.566905  7.438153e+03   46.831250    4913.244302   68.233721   \n",
       "\n",
       "                   MSE(t+5)  ...   MAE(t+16)      MSE(t+16)   MAE(t+17)  \\\n",
       "model                        ...                                          \n",
       "model 1         9537.989886  ...  291.965565  141194.272971  147.297524   \n",
       "model trends  755432.050846  ...  494.508663  435262.153752  494.572441   \n",
       "assembled       9358.484411  ...  320.721673  170132.963148  171.359895   \n",
       "\n",
       "                  MSE(t+17)   MAE(t+18)      MSE(t+18)   MAE(t+19)  \\\n",
       "model                                                                \n",
       "model 1        40577.414599  211.333142   72714.931442  237.669351   \n",
       "model trends  438693.819976  475.924753  404937.398773  489.817600   \n",
       "assembled      50217.614753  231.425384   86758.686110  251.248726   \n",
       "\n",
       "                  MSE(t+19)   MAE(t+20)      MSE(t+20)  \n",
       "model                                                   \n",
       "model 1        92290.728277  279.445759  128032.712031  \n",
       "model trends  433661.971431  480.883558  415089.513832  \n",
       "assembled     103355.435355  305.335133  153550.177272  \n",
       "\n",
       "[3 rows x 40 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_1 = compute_error(Y_test_pred_real_1, Y_test_real)\n",
    "error_1['model'] = 'model 1'\n",
    "error_2 = compute_error(Y_test_pred_real_2, Y_test_real_2)\n",
    "error_2['model'] = 'model trends'\n",
    "error_assembled = compute_error(Y_test_assembled_pred_real, Y_test_assembled_real)\n",
    "error_assembled['model'] = 'assembled'\n",
    "pd.concat([error_1, error_2, error_assembled]).set_index('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
